+ set -e
+ export PYTHONUNBUFFERED=True
+ PYTHONUNBUFFERED=True
+ GPU_ID=0
+ NET=ZF
+ NET_lc=zf
+ array=($@)
+ len=2
+ EXTRA_ARGS=
+ EXTRA_ARGS_SLUG=
++ date +%Y-%m-%d_%H-%M-%S
+ LOG=experiments/logs/faster_rcnn_alt_opt_ZF_.txt.2016-05-12_06-26-34
+ exec
++ tee -a experiments/logs/faster_rcnn_alt_opt_ZF_.txt.2016-05-12_06-26-34
+ echo Logging output to experiments/logs/faster_rcnn_alt_opt_ZF_.txt.2016-05-12_06-26-34
Logging output to experiments/logs/faster_rcnn_alt_opt_ZF_.txt.2016-05-12_06-26-34
+ ./tools/train_faster_rcnn_alt_opt.py --gpu 0 --net_name ZF --weights data/imagenet_models/ZF.v2.caffemodel --imdb voc_2007_trainval --cfg experiments/cfgs/faster_rcnn_alt_opt.yml
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='voc_2007_trainval', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.3,
           'BG_THRESH_LO': 0.01,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `voc_2007_trainval` for training
Set proposal method: gt
Appending horizontally-flipped training examples...
done
Preparing training data...
wrote gt roidb to /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/data/cache/voc_2007_trainval_gt_roidb.pkl
done
roidb len: 1000
Output will be saved to `/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval`
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0512 06:26:37.438225  9591 solver.cpp:54] Initializing solver from parameters: 
train_net: "models/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt"
base_lr: 0.001
display: 20
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 60000
snapshot: 0
snapshot_prefix: "zf_rpn"
average_loss: 100
I0512 06:26:37.438280  9591 solver.cpp:86] Creating training net from train_net file: models/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt
I0512 06:26:37.438972  9591 net.cpp:50] Initializing net from parameters: 
name: "ZF"
state {
  phase: TRAIN
}
layer {
  name: "input-data"
  type: "Python"
  top: "data"
  top: "im_info"
  top: "gt_boxes"
  python_param {
    module: "roi_data_layer.layer"
    layer: "RoIDataLayer"
    param_str: "\'num_classes\': 21"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_cls_score"
  top: "rpn_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn-data"
  type: "Python"
  bottom: "rpn_cls_score"
  bottom: "gt_boxes"
  bottom: "im_info"
  bottom: "data"
  top: "rpn_labels"
  top: "rpn_bbox_targets"
  top: "rpn_bbox_inside_weights"
  top: "rpn_bbox_outside_weights"
  python_param {
    module: "rpn.anchor_target_layer"
    layer: "AnchorTargetLayer"
    param_str: "\'feat_stride\': 16"
  }
}
layer {
  name: "rpn_loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "rpn_cls_score_reshape"
  bottom: "rpn_labels"
  top: "rpn_cls_loss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalize: true
  }
}
layer {
  name: "rpn_loss_bbox"
  type: "SmoothL1Loss"
  bottom: "rpn_bbox_pred"
  bottom: "rpn_bbox_targets"
  bottom: "rpn_bbox_inside_weights"
  bottom: "rpn_bbox_outside_weights"
  top: "rpn_loss_bbox"
  loss_weight: 1
  smooth_l1_loss_param {
    sigma: 3
  }
}
layer {
  name: "dummy_roi_pool_conv5"
  type: "DummyData"
  top: "dummy_roi_pool_conv5"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 0.01
    }
    shape {
      dim: 1
      dim: 9216
    }
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "dummy_roi_pool_conv5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "silence_fc7"
  type: "Silence"
  bottom: "fc7"
}
I0512 06:26:37.439106  9591 layer_factory.hpp:76] Creating layer input-data
I0512 06:26:37.439553  9591 net.cpp:110] Creating Layer input-data
I0512 06:26:37.439566  9591 net.cpp:433] input-data -> data
I0512 06:26:37.439581  9591 net.cpp:433] input-data -> im_info
I0512 06:26:37.439587  9591 net.cpp:433] input-data -> gt_boxes
RoiDataLayer: name_to_top: {'gt_boxes': 2, 'data': 0, 'im_info': 1}
I0512 06:26:37.440119  9591 net.cpp:155] Setting up input-data
I0512 06:26:37.440130  9591 net.cpp:163] Top shape: 1 3 600 1000 (1800000)
I0512 06:26:37.440135  9591 net.cpp:163] Top shape: 1 3 (3)
I0512 06:26:37.440137  9591 net.cpp:163] Top shape: 1 4 (4)
I0512 06:26:37.440141  9591 layer_factory.hpp:76] Creating layer data_input-data_0_split
I0512 06:26:37.440148  9591 net.cpp:110] Creating Layer data_input-data_0_split
I0512 06:26:37.440153  9591 net.cpp:477] data_input-data_0_split <- data
I0512 06:26:37.440158  9591 net.cpp:433] data_input-data_0_split -> data_input-data_0_split_0
I0512 06:26:37.440165  9591 net.cpp:433] data_input-data_0_split -> data_input-data_0_split_1
I0512 06:26:37.440176  9591 net.cpp:155] Setting up data_input-data_0_split
I0512 06:26:37.440181  9591 net.cpp:163] Top shape: 1 3 600 1000 (1800000)
I0512 06:26:37.440183  9591 net.cpp:163] Top shape: 1 3 600 1000 (1800000)
I0512 06:26:37.440186  9591 layer_factory.hpp:76] Creating layer conv1
I0512 06:26:37.440193  9591 net.cpp:110] Creating Layer conv1
I0512 06:26:37.440197  9591 net.cpp:477] conv1 <- data_input-data_0_split_0
I0512 06:26:37.440201  9591 net.cpp:433] conv1 -> conv1
I0512 06:26:37.455348  9591 net.cpp:155] Setting up conv1
I0512 06:26:37.455368  9591 net.cpp:163] Top shape: 1 96 300 500 (14400000)
I0512 06:26:37.455382  9591 layer_factory.hpp:76] Creating layer relu1
I0512 06:26:37.455389  9591 net.cpp:110] Creating Layer relu1
I0512 06:26:37.455392  9591 net.cpp:477] relu1 <- conv1
I0512 06:26:37.455399  9591 net.cpp:419] relu1 -> conv1 (in-place)
I0512 06:26:37.455405  9591 net.cpp:155] Setting up relu1
I0512 06:26:37.455410  9591 net.cpp:163] Top shape: 1 96 300 500 (14400000)
I0512 06:26:37.455412  9591 layer_factory.hpp:76] Creating layer norm1
I0512 06:26:37.455420  9591 net.cpp:110] Creating Layer norm1
I0512 06:26:37.455426  9591 net.cpp:477] norm1 <- conv1
I0512 06:26:37.455432  9591 net.cpp:433] norm1 -> norm1
I0512 06:26:37.455464  9591 net.cpp:155] Setting up norm1
I0512 06:26:37.455469  9591 net.cpp:163] Top shape: 1 96 300 500 (14400000)
I0512 06:26:37.455472  9591 layer_factory.hpp:76] Creating layer pool1
I0512 06:26:37.455479  9591 net.cpp:110] Creating Layer pool1
I0512 06:26:37.455482  9591 net.cpp:477] pool1 <- norm1
I0512 06:26:37.455485  9591 net.cpp:433] pool1 -> pool1
I0512 06:26:37.455493  9591 net.cpp:155] Setting up pool1
I0512 06:26:37.455497  9591 net.cpp:163] Top shape: 1 96 151 251 (3638496)
I0512 06:26:37.455500  9591 layer_factory.hpp:76] Creating layer conv2
I0512 06:26:37.455505  9591 net.cpp:110] Creating Layer conv2
I0512 06:26:37.455507  9591 net.cpp:477] conv2 <- pool1
I0512 06:26:37.455513  9591 net.cpp:433] conv2 -> conv2
I0512 06:26:37.457043  9591 net.cpp:155] Setting up conv2
I0512 06:26:37.457054  9591 net.cpp:163] Top shape: 1 256 76 126 (2451456)
I0512 06:26:37.457062  9591 layer_factory.hpp:76] Creating layer relu2
I0512 06:26:37.457067  9591 net.cpp:110] Creating Layer relu2
I0512 06:26:37.457070  9591 net.cpp:477] relu2 <- conv2
I0512 06:26:37.457075  9591 net.cpp:419] relu2 -> conv2 (in-place)
I0512 06:26:37.457080  9591 net.cpp:155] Setting up relu2
I0512 06:26:37.457084  9591 net.cpp:163] Top shape: 1 256 76 126 (2451456)
I0512 06:26:37.457087  9591 layer_factory.hpp:76] Creating layer norm2
I0512 06:26:37.457093  9591 net.cpp:110] Creating Layer norm2
I0512 06:26:37.457095  9591 net.cpp:477] norm2 <- conv2
I0512 06:26:37.457099  9591 net.cpp:433] norm2 -> norm2
I0512 06:26:37.457114  9591 net.cpp:155] Setting up norm2
I0512 06:26:37.457120  9591 net.cpp:163] Top shape: 1 256 76 126 (2451456)
I0512 06:26:37.457124  9591 layer_factory.hpp:76] Creating layer pool2
I0512 06:26:37.457129  9591 net.cpp:110] Creating Layer pool2
I0512 06:26:37.457130  9591 net.cpp:477] pool2 <- norm2
I0512 06:26:37.457134  9591 net.cpp:433] pool2 -> pool2
I0512 06:26:37.457139  9591 net.cpp:155] Setting up pool2
I0512 06:26:37.457142  9591 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:26:37.457145  9591 layer_factory.hpp:76] Creating layer conv3
I0512 06:26:37.457149  9591 net.cpp:110] Creating Layer conv3
I0512 06:26:37.457151  9591 net.cpp:477] conv3 <- pool2
I0512 06:26:37.457156  9591 net.cpp:433] conv3 -> conv3
I0512 06:26:37.460248  9591 net.cpp:155] Setting up conv3
I0512 06:26:37.460259  9591 net.cpp:163] Top shape: 1 384 39 64 (958464)
I0512 06:26:37.460268  9591 layer_factory.hpp:76] Creating layer relu3
I0512 06:26:37.460273  9591 net.cpp:110] Creating Layer relu3
I0512 06:26:37.460275  9591 net.cpp:477] relu3 <- conv3
I0512 06:26:37.460279  9591 net.cpp:419] relu3 -> conv3 (in-place)
I0512 06:26:37.460284  9591 net.cpp:155] Setting up relu3
I0512 06:26:37.460288  9591 net.cpp:163] Top shape: 1 384 39 64 (958464)
I0512 06:26:37.460290  9591 layer_factory.hpp:76] Creating layer conv4
I0512 06:26:37.460294  9591 net.cpp:110] Creating Layer conv4
I0512 06:26:37.460297  9591 net.cpp:477] conv4 <- conv3
I0512 06:26:37.460301  9591 net.cpp:433] conv4 -> conv4
I0512 06:26:37.463397  9591 net.cpp:155] Setting up conv4
I0512 06:26:37.463407  9591 net.cpp:163] Top shape: 1 384 39 64 (958464)
I0512 06:26:37.463413  9591 layer_factory.hpp:76] Creating layer relu4
I0512 06:26:37.463418  9591 net.cpp:110] Creating Layer relu4
I0512 06:26:37.463421  9591 net.cpp:477] relu4 <- conv4
I0512 06:26:37.463426  9591 net.cpp:419] relu4 -> conv4 (in-place)
I0512 06:26:37.463431  9591 net.cpp:155] Setting up relu4
I0512 06:26:37.463435  9591 net.cpp:163] Top shape: 1 384 39 64 (958464)
I0512 06:26:37.463438  9591 layer_factory.hpp:76] Creating layer conv5
I0512 06:26:37.463443  9591 net.cpp:110] Creating Layer conv5
I0512 06:26:37.463445  9591 net.cpp:477] conv5 <- conv4
I0512 06:26:37.463449  9591 net.cpp:433] conv5 -> conv5
I0512 06:26:37.465571  9591 net.cpp:155] Setting up conv5
I0512 06:26:37.465582  9591 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:26:37.465590  9591 layer_factory.hpp:76] Creating layer relu5
I0512 06:26:37.465595  9591 net.cpp:110] Creating Layer relu5
I0512 06:26:37.465598  9591 net.cpp:477] relu5 <- conv5
I0512 06:26:37.465602  9591 net.cpp:419] relu5 -> conv5 (in-place)
I0512 06:26:37.465607  9591 net.cpp:155] Setting up relu5
I0512 06:26:37.465610  9591 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:26:37.465613  9591 layer_factory.hpp:76] Creating layer rpn_conv1
I0512 06:26:37.465623  9591 net.cpp:110] Creating Layer rpn_conv1
I0512 06:26:37.465626  9591 net.cpp:477] rpn_conv1 <- conv5
I0512 06:26:37.465631  9591 net.cpp:433] rpn_conv1 -> rpn_conv1
I0512 06:26:37.482316  9591 net.cpp:155] Setting up rpn_conv1
I0512 06:26:37.482326  9591 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:26:37.482332  9591 layer_factory.hpp:76] Creating layer rpn_relu1
I0512 06:26:37.482337  9591 net.cpp:110] Creating Layer rpn_relu1
I0512 06:26:37.482341  9591 net.cpp:477] rpn_relu1 <- rpn_conv1
I0512 06:26:37.482347  9591 net.cpp:419] rpn_relu1 -> rpn_conv1 (in-place)
I0512 06:26:37.482352  9591 net.cpp:155] Setting up rpn_relu1
I0512 06:26:37.482355  9591 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:26:37.482357  9591 layer_factory.hpp:76] Creating layer rpn_conv1_rpn_relu1_0_split
I0512 06:26:37.482362  9591 net.cpp:110] Creating Layer rpn_conv1_rpn_relu1_0_split
I0512 06:26:37.482365  9591 net.cpp:477] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I0512 06:26:37.482368  9591 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I0512 06:26:37.482373  9591 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I0512 06:26:37.482379  9591 net.cpp:155] Setting up rpn_conv1_rpn_relu1_0_split
I0512 06:26:37.482383  9591 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:26:37.482386  9591 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:26:37.482389  9591 layer_factory.hpp:76] Creating layer rpn_cls_score
I0512 06:26:37.482395  9591 net.cpp:110] Creating Layer rpn_cls_score
I0512 06:26:37.482400  9591 net.cpp:477] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I0512 06:26:37.482406  9591 net.cpp:433] rpn_cls_score -> rpn_cls_score
I0512 06:26:37.482575  9591 net.cpp:155] Setting up rpn_cls_score
I0512 06:26:37.482583  9591 net.cpp:163] Top shape: 1 18 39 64 (44928)
I0512 06:26:37.482588  9591 layer_factory.hpp:76] Creating layer rpn_cls_score_rpn_cls_score_0_split
I0512 06:26:37.482591  9591 net.cpp:110] Creating Layer rpn_cls_score_rpn_cls_score_0_split
I0512 06:26:37.482594  9591 net.cpp:477] rpn_cls_score_rpn_cls_score_0_split <- rpn_cls_score
I0512 06:26:37.482597  9591 net.cpp:433] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_0
I0512 06:26:37.482602  9591 net.cpp:433] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_1
I0512 06:26:37.482609  9591 net.cpp:155] Setting up rpn_cls_score_rpn_cls_score_0_split
I0512 06:26:37.482614  9591 net.cpp:163] Top shape: 1 18 39 64 (44928)
I0512 06:26:37.482616  9591 net.cpp:163] Top shape: 1 18 39 64 (44928)
I0512 06:26:37.482620  9591 layer_factory.hpp:76] Creating layer rpn_bbox_pred
I0512 06:26:37.482626  9591 net.cpp:110] Creating Layer rpn_bbox_pred
I0512 06:26:37.482630  9591 net.cpp:477] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I0512 06:26:37.482636  9591 net.cpp:433] rpn_bbox_pred -> rpn_bbox_pred
I0512 06:26:37.482924  9591 net.cpp:155] Setting up rpn_bbox_pred
I0512 06:26:37.482930  9591 net.cpp:163] Top shape: 1 36 39 64 (89856)
I0512 06:26:37.482935  9591 layer_factory.hpp:76] Creating layer rpn_cls_score_reshape
I0512 06:26:37.482946  9591 net.cpp:110] Creating Layer rpn_cls_score_reshape
I0512 06:26:37.482949  9591 net.cpp:477] rpn_cls_score_reshape <- rpn_cls_score_rpn_cls_score_0_split_0
I0512 06:26:37.482954  9591 net.cpp:433] rpn_cls_score_reshape -> rpn_cls_score_reshape
I0512 06:26:37.482967  9591 net.cpp:155] Setting up rpn_cls_score_reshape
I0512 06:26:37.482971  9591 net.cpp:163] Top shape: 1 2 351 64 (44928)
I0512 06:26:37.482975  9591 layer_factory.hpp:76] Creating layer rpn-data
I0512 06:26:37.483490  9591 net.cpp:110] Creating Layer rpn-data
I0512 06:26:37.483500  9591 net.cpp:477] rpn-data <- rpn_cls_score_rpn_cls_score_0_split_1
I0512 06:26:37.483505  9591 net.cpp:477] rpn-data <- gt_boxes
I0512 06:26:37.483508  9591 net.cpp:477] rpn-data <- im_info
I0512 06:26:37.483511  9591 net.cpp:477] rpn-data <- data_input-data_0_split_1
I0512 06:26:37.483516  9591 net.cpp:433] rpn-data -> rpn_labels
I0512 06:26:37.483523  9591 net.cpp:433] rpn-data -> rpn_bbox_targets
I0512 06:26:37.483528  9591 net.cpp:433] rpn-data -> rpn_bbox_inside_weights
I0512 06:26:37.483533  9591 net.cpp:433] rpn-data -> rpn_bbox_outside_weights
I0512 06:26:37.484722  9591 net.cpp:155] Setting up rpn-data
I0512 06:26:37.484735  9591 net.cpp:163] Top shape: 1 1 351 64 (22464)
I0512 06:26:37.484740  9591 net.cpp:163] Top shape: 1 36 39 64 (89856)
I0512 06:26:37.484742  9591 net.cpp:163] Top shape: 1 36 39 64 (89856)
I0512 06:26:37.484745  9591 net.cpp:163] Top shape: 1 36 39 64 (89856)
I0512 06:26:37.484748  9591 layer_factory.hpp:76] Creating layer rpn_loss_cls
I0512 06:26:37.484757  9591 net.cpp:110] Creating Layer rpn_loss_cls
I0512 06:26:37.484761  9591 net.cpp:477] rpn_loss_cls <- rpn_cls_score_reshape
I0512 06:26:37.484766  9591 net.cpp:477] rpn_loss_cls <- rpn_labels
I0512 06:26:37.484771  9591 net.cpp:433] rpn_loss_cls -> rpn_cls_loss
I0512 06:26:37.484781  9591 layer_factory.hpp:76] Creating layer rpn_loss_cls
I0512 06:26:37.484848  9591 net.cpp:155] Setting up rpn_loss_cls
I0512 06:26:37.484854  9591 net.cpp:163] Top shape: (1)
I0512 06:26:37.484858  9591 net.cpp:168]     with loss weight 1
I0512 06:26:37.484870  9591 layer_factory.hpp:76] Creating layer rpn_loss_bbox
I0512 06:26:37.484879  9591 net.cpp:110] Creating Layer rpn_loss_bbox
I0512 06:26:37.484882  9591 net.cpp:477] rpn_loss_bbox <- rpn_bbox_pred
I0512 06:26:37.484886  9591 net.cpp:477] rpn_loss_bbox <- rpn_bbox_targets
I0512 06:26:37.484889  9591 net.cpp:477] rpn_loss_bbox <- rpn_bbox_inside_weights
I0512 06:26:37.484892  9591 net.cpp:477] rpn_loss_bbox <- rpn_bbox_outside_weights
I0512 06:26:37.484896  9591 net.cpp:433] rpn_loss_bbox -> rpn_loss_bbox
I0512 06:26:37.485345  9591 net.cpp:155] Setting up rpn_loss_bbox
I0512 06:26:37.485352  9591 net.cpp:163] Top shape: (1)
I0512 06:26:37.485354  9591 net.cpp:168]     with loss weight 1
I0512 06:26:37.485359  9591 layer_factory.hpp:76] Creating layer dummy_roi_pool_conv5
I0512 06:26:37.485368  9591 net.cpp:110] Creating Layer dummy_roi_pool_conv5
I0512 06:26:37.485371  9591 net.cpp:433] dummy_roi_pool_conv5 -> dummy_roi_pool_conv5
I0512 06:26:37.485383  9591 net.cpp:155] Setting up dummy_roi_pool_conv5
I0512 06:26:37.485388  9591 net.cpp:163] Top shape: 1 9216 (9216)
I0512 06:26:37.485390  9591 layer_factory.hpp:76] Creating layer fc6
I0512 06:26:37.485397  9591 net.cpp:110] Creating Layer fc6
I0512 06:26:37.485400  9591 net.cpp:477] fc6 <- dummy_roi_pool_conv5
I0512 06:26:37.485404  9591 net.cpp:433] fc6 -> fc6
I0512 06:26:37.567881  9591 net.cpp:155] Setting up fc6
I0512 06:26:37.567917  9591 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:26:37.567935  9591 layer_factory.hpp:76] Creating layer relu6
I0512 06:26:37.567945  9591 net.cpp:110] Creating Layer relu6
I0512 06:26:37.567950  9591 net.cpp:477] relu6 <- fc6
I0512 06:26:37.567956  9591 net.cpp:419] relu6 -> fc6 (in-place)
I0512 06:26:37.567965  9591 net.cpp:155] Setting up relu6
I0512 06:26:37.567968  9591 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:26:37.567971  9591 layer_factory.hpp:76] Creating layer fc7
I0512 06:26:37.567978  9591 net.cpp:110] Creating Layer fc7
I0512 06:26:37.567981  9591 net.cpp:477] fc7 <- fc6
I0512 06:26:37.567991  9591 net.cpp:433] fc7 -> fc7
I0512 06:26:37.606763  9591 net.cpp:155] Setting up fc7
I0512 06:26:37.606801  9591 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:26:37.606817  9591 layer_factory.hpp:76] Creating layer silence_fc7
I0512 06:26:37.606834  9591 net.cpp:110] Creating Layer silence_fc7
I0512 06:26:37.606845  9591 net.cpp:477] silence_fc7 <- fc7
I0512 06:26:37.606854  9591 net.cpp:155] Setting up silence_fc7
I0512 06:26:37.606858  9591 net.cpp:240] silence_fc7 does not need backward computation.
I0512 06:26:37.606861  9591 net.cpp:240] fc7 does not need backward computation.
I0512 06:26:37.606863  9591 net.cpp:240] relu6 does not need backward computation.
I0512 06:26:37.606866  9591 net.cpp:240] fc6 does not need backward computation.
I0512 06:26:37.606869  9591 net.cpp:240] dummy_roi_pool_conv5 does not need backward computation.
I0512 06:26:37.606873  9591 net.cpp:236] rpn_loss_bbox needs backward computation.
I0512 06:26:37.606878  9591 net.cpp:236] rpn_loss_cls needs backward computation.
I0512 06:26:37.606889  9591 net.cpp:236] rpn-data needs backward computation.
I0512 06:26:37.606897  9591 net.cpp:236] rpn_cls_score_reshape needs backward computation.
I0512 06:26:37.606901  9591 net.cpp:236] rpn_bbox_pred needs backward computation.
I0512 06:26:37.606906  9591 net.cpp:236] rpn_cls_score_rpn_cls_score_0_split needs backward computation.
I0512 06:26:37.606909  9591 net.cpp:236] rpn_cls_score needs backward computation.
I0512 06:26:37.606914  9591 net.cpp:236] rpn_conv1_rpn_relu1_0_split needs backward computation.
I0512 06:26:37.606917  9591 net.cpp:236] rpn_relu1 needs backward computation.
I0512 06:26:37.606921  9591 net.cpp:236] rpn_conv1 needs backward computation.
I0512 06:26:37.606925  9591 net.cpp:236] relu5 needs backward computation.
I0512 06:26:37.606931  9591 net.cpp:236] conv5 needs backward computation.
I0512 06:26:37.606935  9591 net.cpp:236] relu4 needs backward computation.
I0512 06:26:37.606937  9591 net.cpp:236] conv4 needs backward computation.
I0512 06:26:37.606942  9591 net.cpp:236] relu3 needs backward computation.
I0512 06:26:37.606945  9591 net.cpp:236] conv3 needs backward computation.
I0512 06:26:37.606950  9591 net.cpp:236] pool2 needs backward computation.
I0512 06:26:37.606956  9591 net.cpp:236] norm2 needs backward computation.
I0512 06:26:37.606961  9591 net.cpp:236] relu2 needs backward computation.
I0512 06:26:37.606966  9591 net.cpp:236] conv2 needs backward computation.
I0512 06:26:37.606968  9591 net.cpp:236] pool1 needs backward computation.
I0512 06:26:37.606973  9591 net.cpp:236] norm1 needs backward computation.
I0512 06:26:37.606977  9591 net.cpp:236] relu1 needs backward computation.
I0512 06:26:37.606982  9591 net.cpp:236] conv1 needs backward computation.
I0512 06:26:37.606993  9591 net.cpp:240] data_input-data_0_split does not need backward computation.
I0512 06:26:37.606998  9591 net.cpp:240] input-data does not need backward computation.
I0512 06:26:37.607002  9591 net.cpp:283] This network produces output rpn_cls_loss
I0512 06:26:37.607004  9591 net.cpp:283] This network produces output rpn_loss_bbox
I0512 06:26:37.607028  9591 net.cpp:297] Network initialization done.
I0512 06:26:37.607031  9591 net.cpp:298] Memory required for data: 273930660
I0512 06:26:37.607257  9591 solver.cpp:65] Solver scaffolding done.
Loading pretrained model weights from data/imagenet_models/ZF.v2.caffemodel
Solving...
I0512 06:26:38.131260  9591 solver.cpp:242] Iteration 0, loss = 1.3772
I0512 06:26:38.131304  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.625964 (* 1 = 0.625964 loss)
I0512 06:26:38.131311  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.751232 (* 1 = 0.751232 loss)
I0512 06:26:38.131320  9591 solver.cpp:571] Iteration 0, lr = 0.001
I0512 06:26:39.438261  9591 solver.cpp:242] Iteration 20, loss = 0.709157
I0512 06:26:39.438302  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.493089 (* 1 = 0.493089 loss)
I0512 06:26:39.438309  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.216068 (* 1 = 0.216068 loss)
I0512 06:26:39.438314  9591 solver.cpp:571] Iteration 20, lr = 0.001
I0512 06:26:40.735929  9591 solver.cpp:242] Iteration 40, loss = 0.419617
I0512 06:26:40.735970  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.359898 (* 1 = 0.359898 loss)
I0512 06:26:40.735976  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.059719 (* 1 = 0.059719 loss)
I0512 06:26:40.735982  9591 solver.cpp:571] Iteration 40, lr = 0.001
I0512 06:26:42.061015  9591 solver.cpp:242] Iteration 60, loss = 0.288162
I0512 06:26:42.061072  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.229282 (* 1 = 0.229282 loss)
I0512 06:26:42.061081  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0588803 (* 1 = 0.0588803 loss)
I0512 06:26:42.061087  9591 solver.cpp:571] Iteration 60, lr = 0.001
I0512 06:26:43.357722  9591 solver.cpp:242] Iteration 80, loss = 0.453263
I0512 06:26:43.357761  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.405176 (* 1 = 0.405176 loss)
I0512 06:26:43.357769  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0480865 (* 1 = 0.0480865 loss)
I0512 06:26:43.357774  9591 solver.cpp:571] Iteration 80, lr = 0.001
I0512 06:26:44.639951  9591 solver.cpp:242] Iteration 100, loss = 0.258631
I0512 06:26:44.639999  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0714378 (* 1 = 0.0714378 loss)
I0512 06:26:44.640008  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.187193 (* 1 = 0.187193 loss)
I0512 06:26:44.640015  9591 solver.cpp:571] Iteration 100, lr = 0.001
I0512 06:26:45.927907  9591 solver.cpp:242] Iteration 120, loss = 0.149905
I0512 06:26:45.927948  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.139456 (* 1 = 0.139456 loss)
I0512 06:26:45.927955  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0104489 (* 1 = 0.0104489 loss)
I0512 06:26:45.927960  9591 solver.cpp:571] Iteration 120, lr = 0.001
I0512 06:26:47.266894  9591 solver.cpp:242] Iteration 140, loss = 0.343865
I0512 06:26:47.266938  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.262708 (* 1 = 0.262708 loss)
I0512 06:26:47.266947  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0811571 (* 1 = 0.0811571 loss)
I0512 06:26:47.266952  9591 solver.cpp:571] Iteration 140, lr = 0.001
I0512 06:26:48.586418  9591 solver.cpp:242] Iteration 160, loss = 0.671457
I0512 06:26:48.586460  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.559472 (* 1 = 0.559472 loss)
I0512 06:26:48.586468  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.111984 (* 1 = 0.111984 loss)
I0512 06:26:48.586473  9591 solver.cpp:571] Iteration 160, lr = 0.001
I0512 06:26:49.927528  9591 solver.cpp:242] Iteration 180, loss = 0.157119
I0512 06:26:49.927568  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.154153 (* 1 = 0.154153 loss)
I0512 06:26:49.927575  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.00296657 (* 1 = 0.00296657 loss)
I0512 06:26:49.927580  9591 solver.cpp:571] Iteration 180, lr = 0.001
speed: 0.066s / iter
I0512 06:26:51.241345  9591 solver.cpp:242] Iteration 200, loss = 0.50364
I0512 06:26:51.241385  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.410059 (* 1 = 0.410059 loss)
I0512 06:26:51.241391  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0935805 (* 1 = 0.0935805 loss)
I0512 06:26:51.241396  9591 solver.cpp:571] Iteration 200, lr = 0.001
I0512 06:26:52.569571  9591 solver.cpp:242] Iteration 220, loss = 0.816115
I0512 06:26:52.569612  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.291657 (* 1 = 0.291657 loss)
I0512 06:26:52.569619  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.524458 (* 1 = 0.524458 loss)
I0512 06:26:52.569624  9591 solver.cpp:571] Iteration 220, lr = 0.001
I0512 06:26:53.849797  9591 solver.cpp:242] Iteration 240, loss = 0.665926
I0512 06:26:53.849840  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.523806 (* 1 = 0.523806 loss)
I0512 06:26:53.849849  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.14212 (* 1 = 0.14212 loss)
I0512 06:26:53.849855  9591 solver.cpp:571] Iteration 240, lr = 0.001
I0512 06:26:55.160581  9591 solver.cpp:242] Iteration 260, loss = 0.362277
I0512 06:26:55.160621  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.1162 (* 1 = 0.1162 loss)
I0512 06:26:55.160627  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.246077 (* 1 = 0.246077 loss)
I0512 06:26:55.160634  9591 solver.cpp:571] Iteration 260, lr = 0.001
I0512 06:26:56.474340  9591 solver.cpp:242] Iteration 280, loss = 0.316822
I0512 06:26:56.474380  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.252593 (* 1 = 0.252593 loss)
I0512 06:26:56.474387  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0642293 (* 1 = 0.0642293 loss)
I0512 06:26:56.474392  9591 solver.cpp:571] Iteration 280, lr = 0.001
I0512 06:26:57.780918  9591 solver.cpp:242] Iteration 300, loss = 0.522834
I0512 06:26:57.780959  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.354216 (* 1 = 0.354216 loss)
I0512 06:26:57.780966  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.168617 (* 1 = 0.168617 loss)
I0512 06:26:57.780972  9591 solver.cpp:571] Iteration 300, lr = 0.001
I0512 06:26:59.074250  9591 solver.cpp:242] Iteration 320, loss = 0.318275
I0512 06:26:59.074290  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.300422 (* 1 = 0.300422 loss)
I0512 06:26:59.074297  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0178528 (* 1 = 0.0178528 loss)
I0512 06:26:59.074303  9591 solver.cpp:571] Iteration 320, lr = 0.001
I0512 06:27:00.388540  9591 solver.cpp:242] Iteration 340, loss = 0.311951
I0512 06:27:00.388581  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.300861 (* 1 = 0.300861 loss)
I0512 06:27:00.388589  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0110905 (* 1 = 0.0110905 loss)
I0512 06:27:00.388594  9591 solver.cpp:571] Iteration 340, lr = 0.001
I0512 06:27:01.666854  9591 solver.cpp:242] Iteration 360, loss = 0.420957
I0512 06:27:01.666896  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.368105 (* 1 = 0.368105 loss)
I0512 06:27:01.666903  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0528524 (* 1 = 0.0528524 loss)
I0512 06:27:01.666908  9591 solver.cpp:571] Iteration 360, lr = 0.001
I0512 06:27:02.981461  9591 solver.cpp:242] Iteration 380, loss = 0.166912
I0512 06:27:02.981506  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.155595 (* 1 = 0.155595 loss)
I0512 06:27:02.981515  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0113171 (* 1 = 0.0113171 loss)
I0512 06:27:02.981523  9591 solver.cpp:571] Iteration 380, lr = 0.001
speed: 0.065s / iter
I0512 06:27:04.294392  9591 solver.cpp:242] Iteration 400, loss = 0.412653
I0512 06:27:04.294432  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.188331 (* 1 = 0.188331 loss)
I0512 06:27:04.294440  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.224322 (* 1 = 0.224322 loss)
I0512 06:27:04.294445  9591 solver.cpp:571] Iteration 400, lr = 0.001
I0512 06:27:05.590152  9591 solver.cpp:242] Iteration 420, loss = 0.124761
I0512 06:27:05.590194  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0495098 (* 1 = 0.0495098 loss)
I0512 06:27:05.590201  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0752516 (* 1 = 0.0752516 loss)
I0512 06:27:05.590206  9591 solver.cpp:571] Iteration 420, lr = 0.001
I0512 06:27:06.881237  9591 solver.cpp:242] Iteration 440, loss = 0.330478
I0512 06:27:06.881279  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.286365 (* 1 = 0.286365 loss)
I0512 06:27:06.881286  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0441124 (* 1 = 0.0441124 loss)
I0512 06:27:06.881291  9591 solver.cpp:571] Iteration 440, lr = 0.001
I0512 06:27:08.125675  9591 solver.cpp:242] Iteration 460, loss = 0.180189
I0512 06:27:08.125717  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0722174 (* 1 = 0.0722174 loss)
I0512 06:27:08.125725  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.107972 (* 1 = 0.107972 loss)
I0512 06:27:08.125730  9591 solver.cpp:571] Iteration 460, lr = 0.001
I0512 06:27:09.421555  9591 solver.cpp:242] Iteration 480, loss = 0.388094
I0512 06:27:09.421604  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.291147 (* 1 = 0.291147 loss)
I0512 06:27:09.421614  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0969474 (* 1 = 0.0969474 loss)
I0512 06:27:09.421622  9591 solver.cpp:571] Iteration 480, lr = 0.001
I0512 06:27:10.751870  9591 solver.cpp:242] Iteration 500, loss = 0.106553
I0512 06:27:10.751911  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0616309 (* 1 = 0.0616309 loss)
I0512 06:27:10.751919  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0449224 (* 1 = 0.0449224 loss)
I0512 06:27:10.751924  9591 solver.cpp:571] Iteration 500, lr = 0.001
I0512 06:27:12.065956  9591 solver.cpp:242] Iteration 520, loss = 0.571791
I0512 06:27:12.066000  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.453655 (* 1 = 0.453655 loss)
I0512 06:27:12.066009  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.118136 (* 1 = 0.118136 loss)
I0512 06:27:12.066014  9591 solver.cpp:571] Iteration 520, lr = 0.001
I0512 06:27:13.370136  9591 solver.cpp:242] Iteration 540, loss = 0.325598
I0512 06:27:13.370177  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.204116 (* 1 = 0.204116 loss)
I0512 06:27:13.370184  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.121482 (* 1 = 0.121482 loss)
I0512 06:27:13.370189  9591 solver.cpp:571] Iteration 540, lr = 0.001
I0512 06:27:14.696833  9591 solver.cpp:242] Iteration 560, loss = 0.325723
I0512 06:27:14.696874  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.268966 (* 1 = 0.268966 loss)
I0512 06:27:14.696882  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0567569 (* 1 = 0.0567569 loss)
I0512 06:27:14.696887  9591 solver.cpp:571] Iteration 560, lr = 0.001
I0512 06:27:16.022099  9591 solver.cpp:242] Iteration 580, loss = 0.302104
I0512 06:27:16.022140  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.27655 (* 1 = 0.27655 loss)
I0512 06:27:16.022147  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0255538 (* 1 = 0.0255538 loss)
I0512 06:27:16.022152  9591 solver.cpp:571] Iteration 580, lr = 0.001
speed: 0.065s / iter
I0512 06:27:17.356282  9591 solver.cpp:242] Iteration 600, loss = 0.130028
I0512 06:27:17.356323  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.118295 (* 1 = 0.118295 loss)
I0512 06:27:17.356330  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0117325 (* 1 = 0.0117325 loss)
I0512 06:27:17.356335  9591 solver.cpp:571] Iteration 600, lr = 0.001
I0512 06:27:18.664862  9591 solver.cpp:242] Iteration 620, loss = 0.804321
I0512 06:27:18.664904  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.527985 (* 1 = 0.527985 loss)
I0512 06:27:18.664911  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.276335 (* 1 = 0.276335 loss)
I0512 06:27:18.664916  9591 solver.cpp:571] Iteration 620, lr = 0.001
I0512 06:27:19.980633  9591 solver.cpp:242] Iteration 640, loss = 0.211716
I0512 06:27:19.980679  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.072789 (* 1 = 0.072789 loss)
I0512 06:27:19.980686  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.138927 (* 1 = 0.138927 loss)
I0512 06:27:19.980692  9591 solver.cpp:571] Iteration 640, lr = 0.001
I0512 06:27:21.325970  9591 solver.cpp:242] Iteration 660, loss = 0.141788
I0512 06:27:21.326015  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.120962 (* 1 = 0.120962 loss)
I0512 06:27:21.326022  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.020826 (* 1 = 0.020826 loss)
I0512 06:27:21.326028  9591 solver.cpp:571] Iteration 660, lr = 0.001
I0512 06:27:22.657516  9591 solver.cpp:242] Iteration 680, loss = 0.138887
I0512 06:27:22.657557  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.126563 (* 1 = 0.126563 loss)
I0512 06:27:22.657564  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0123248 (* 1 = 0.0123248 loss)
I0512 06:27:22.657569  9591 solver.cpp:571] Iteration 680, lr = 0.001
I0512 06:27:23.955114  9591 solver.cpp:242] Iteration 700, loss = 0.094403
I0512 06:27:23.955157  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0869949 (* 1 = 0.0869949 loss)
I0512 06:27:23.955164  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.00740808 (* 1 = 0.00740808 loss)
I0512 06:27:23.955169  9591 solver.cpp:571] Iteration 700, lr = 0.001
I0512 06:27:25.251356  9591 solver.cpp:242] Iteration 720, loss = 0.51497
I0512 06:27:25.251397  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.438182 (* 1 = 0.438182 loss)
I0512 06:27:25.251405  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.076789 (* 1 = 0.076789 loss)
I0512 06:27:25.251410  9591 solver.cpp:571] Iteration 720, lr = 0.001
I0512 06:27:26.567914  9591 solver.cpp:242] Iteration 740, loss = 0.199555
I0512 06:27:26.567956  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.117369 (* 1 = 0.117369 loss)
I0512 06:27:26.567963  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.082186 (* 1 = 0.082186 loss)
I0512 06:27:26.567968  9591 solver.cpp:571] Iteration 740, lr = 0.001
I0512 06:27:27.857019  9591 solver.cpp:242] Iteration 760, loss = 0.141427
I0512 06:27:27.857061  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.132875 (* 1 = 0.132875 loss)
I0512 06:27:27.857069  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.00855203 (* 1 = 0.00855203 loss)
I0512 06:27:27.857074  9591 solver.cpp:571] Iteration 760, lr = 0.001
I0512 06:27:29.196322  9591 solver.cpp:242] Iteration 780, loss = 0.230898
I0512 06:27:29.196363  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.196278 (* 1 = 0.196278 loss)
I0512 06:27:29.196370  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0346198 (* 1 = 0.0346198 loss)
I0512 06:27:29.196375  9591 solver.cpp:571] Iteration 780, lr = 0.001
speed: 0.065s / iter
I0512 06:27:30.532866  9591 solver.cpp:242] Iteration 800, loss = 0.82452
I0512 06:27:30.532905  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.547923 (* 1 = 0.547923 loss)
I0512 06:27:30.532912  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.276597 (* 1 = 0.276597 loss)
I0512 06:27:30.532917  9591 solver.cpp:571] Iteration 800, lr = 0.001
I0512 06:27:31.874989  9591 solver.cpp:242] Iteration 820, loss = 0.221144
I0512 06:27:31.875030  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.136086 (* 1 = 0.136086 loss)
I0512 06:27:31.875037  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0850585 (* 1 = 0.0850585 loss)
I0512 06:27:31.875042  9591 solver.cpp:571] Iteration 820, lr = 0.001
I0512 06:27:33.159050  9591 solver.cpp:242] Iteration 840, loss = 0.367883
I0512 06:27:33.159090  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.228923 (* 1 = 0.228923 loss)
I0512 06:27:33.159097  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.138961 (* 1 = 0.138961 loss)
I0512 06:27:33.159101  9591 solver.cpp:571] Iteration 840, lr = 0.001
I0512 06:27:34.475713  9591 solver.cpp:242] Iteration 860, loss = 0.485199
I0512 06:27:34.475756  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.395989 (* 1 = 0.395989 loss)
I0512 06:27:34.475764  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0892104 (* 1 = 0.0892104 loss)
I0512 06:27:34.475769  9591 solver.cpp:571] Iteration 860, lr = 0.001
I0512 06:27:35.812170  9591 solver.cpp:242] Iteration 880, loss = 0.25613
I0512 06:27:35.812209  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.167739 (* 1 = 0.167739 loss)
I0512 06:27:35.812217  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0883909 (* 1 = 0.0883909 loss)
I0512 06:27:35.812222  9591 solver.cpp:571] Iteration 880, lr = 0.001
I0512 06:27:37.109872  9591 solver.cpp:242] Iteration 900, loss = 0.22861
I0512 06:27:37.109912  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.201323 (* 1 = 0.201323 loss)
I0512 06:27:37.109920  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0272871 (* 1 = 0.0272871 loss)
I0512 06:27:37.109925  9591 solver.cpp:571] Iteration 900, lr = 0.001
I0512 06:27:38.450564  9591 solver.cpp:242] Iteration 920, loss = 0.687236
I0512 06:27:38.450606  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.585429 (* 1 = 0.585429 loss)
I0512 06:27:38.450613  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.101807 (* 1 = 0.101807 loss)
I0512 06:27:38.450618  9591 solver.cpp:571] Iteration 920, lr = 0.001
I0512 06:27:39.747875  9591 solver.cpp:242] Iteration 940, loss = 0.0875858
I0512 06:27:39.747921  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0536795 (* 1 = 0.0536795 loss)
I0512 06:27:39.747930  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0339063 (* 1 = 0.0339063 loss)
I0512 06:27:39.747936  9591 solver.cpp:571] Iteration 940, lr = 0.001
I0512 06:27:41.043855  9591 solver.cpp:242] Iteration 960, loss = 0.214889
I0512 06:27:41.043896  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.1508 (* 1 = 0.1508 loss)
I0512 06:27:41.043903  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0640883 (* 1 = 0.0640883 loss)
I0512 06:27:41.043908  9591 solver.cpp:571] Iteration 960, lr = 0.001
I0512 06:27:42.374508  9591 solver.cpp:242] Iteration 980, loss = 0.254439
I0512 06:27:42.374549  9591 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.241786 (* 1 = 0.241786 loss)
I0512 06:27:42.374557  9591 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0126537 (* 1 = 0.0126537 loss)
I0512 06:27:42.374562  9591 solver.cpp:571] Iteration 980, lr = 0.001
speed: 0.066s / iter
Wrote snapshot to: /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage1_iter_1000.caffemodel
done solving
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, generate proposals
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
RPN model: /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage1_iter_1000.caffemodel
Using config:
{'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 2000,
          'RPN_PRE_NMS_TOP_N': -1,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.3,
           'BG_THRESH_LO': 0.01,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': False,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'selective_search',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `voc_2007_trainval` for proposal generation
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0512 06:27:44.695611  9597 net.cpp:50] Initializing net from parameters: 
name: "ZF"
input: "data"
input: "im_info"
state {
  phase: TEST
}
input_shape {
  dim: 1
  dim: 3
  dim: 224
  dim: 224
}
input_shape {
  dim: 1
  dim: 3
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "rpn_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_cls_score"
  top: "rpn_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn_cls_prob"
  type: "Softmax"
  bottom: "rpn_cls_score_reshape"
  top: "rpn_cls_prob"
}
layer {
  name: "rpn_cls_prob_reshape"
  type: "Reshape"
  bottom: "rpn_cls_prob"
  top: "rpn_cls_prob_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 18
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "proposal"
  type: "Python"
  bottom: "rpn_cls_prob_reshape"
  bottom: "rpn_bbox_pred"
  bottom: "im_info"
  top: "rois"
  top: "scores"
  python_param {
    module: "rpn.proposal_layer"
    layer: "ProposalLayer"
    param_str: "\'feat_stride\': 16"
  }
}
I0512 06:27:44.695746  9597 net.cpp:435] Input 0 -> data
I0512 06:27:44.695775  9597 net.cpp:435] Input 1 -> im_info
I0512 06:27:44.695791  9597 layer_factory.hpp:76] Creating layer conv1
I0512 06:27:44.695809  9597 net.cpp:110] Creating Layer conv1
I0512 06:27:44.695816  9597 net.cpp:477] conv1 <- data
I0512 06:27:44.695822  9597 net.cpp:433] conv1 -> conv1
I0512 06:27:44.715157  9597 net.cpp:155] Setting up conv1
I0512 06:27:44.715183  9597 net.cpp:163] Top shape: 1 96 112 112 (1204224)
I0512 06:27:44.715201  9597 layer_factory.hpp:76] Creating layer relu1
I0512 06:27:44.715210  9597 net.cpp:110] Creating Layer relu1
I0512 06:27:44.715214  9597 net.cpp:477] relu1 <- conv1
I0512 06:27:44.715220  9597 net.cpp:419] relu1 -> conv1 (in-place)
I0512 06:27:44.715229  9597 net.cpp:155] Setting up relu1
I0512 06:27:44.715234  9597 net.cpp:163] Top shape: 1 96 112 112 (1204224)
I0512 06:27:44.715239  9597 layer_factory.hpp:76] Creating layer norm1
I0512 06:27:44.715250  9597 net.cpp:110] Creating Layer norm1
I0512 06:27:44.715256  9597 net.cpp:477] norm1 <- conv1
I0512 06:27:44.715262  9597 net.cpp:433] norm1 -> norm1
I0512 06:27:44.715304  9597 net.cpp:155] Setting up norm1
I0512 06:27:44.715325  9597 net.cpp:163] Top shape: 1 96 112 112 (1204224)
I0512 06:27:44.715329  9597 layer_factory.hpp:76] Creating layer pool1
I0512 06:27:44.715337  9597 net.cpp:110] Creating Layer pool1
I0512 06:27:44.715339  9597 net.cpp:477] pool1 <- norm1
I0512 06:27:44.715344  9597 net.cpp:433] pool1 -> pool1
I0512 06:27:44.715353  9597 net.cpp:155] Setting up pool1
I0512 06:27:44.715358  9597 net.cpp:163] Top shape: 1 96 57 57 (311904)
I0512 06:27:44.715373  9597 layer_factory.hpp:76] Creating layer conv2
I0512 06:27:44.715379  9597 net.cpp:110] Creating Layer conv2
I0512 06:27:44.715383  9597 net.cpp:477] conv2 <- pool1
I0512 06:27:44.715387  9597 net.cpp:433] conv2 -> conv2
I0512 06:27:44.717900  9597 net.cpp:155] Setting up conv2
I0512 06:27:44.717916  9597 net.cpp:163] Top shape: 1 256 29 29 (215296)
I0512 06:27:44.717926  9597 layer_factory.hpp:76] Creating layer relu2
I0512 06:27:44.717936  9597 net.cpp:110] Creating Layer relu2
I0512 06:27:44.717941  9597 net.cpp:477] relu2 <- conv2
I0512 06:27:44.717946  9597 net.cpp:419] relu2 -> conv2 (in-place)
I0512 06:27:44.717952  9597 net.cpp:155] Setting up relu2
I0512 06:27:44.717957  9597 net.cpp:163] Top shape: 1 256 29 29 (215296)
I0512 06:27:44.717959  9597 layer_factory.hpp:76] Creating layer norm2
I0512 06:27:44.717967  9597 net.cpp:110] Creating Layer norm2
I0512 06:27:44.717970  9597 net.cpp:477] norm2 <- conv2
I0512 06:27:44.717974  9597 net.cpp:433] norm2 -> norm2
I0512 06:27:44.717999  9597 net.cpp:155] Setting up norm2
I0512 06:27:44.718008  9597 net.cpp:163] Top shape: 1 256 29 29 (215296)
I0512 06:27:44.718011  9597 layer_factory.hpp:76] Creating layer pool2
I0512 06:27:44.718016  9597 net.cpp:110] Creating Layer pool2
I0512 06:27:44.718020  9597 net.cpp:477] pool2 <- norm2
I0512 06:27:44.718024  9597 net.cpp:433] pool2 -> pool2
I0512 06:27:44.718030  9597 net.cpp:155] Setting up pool2
I0512 06:27:44.718035  9597 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:27:44.718039  9597 layer_factory.hpp:76] Creating layer conv3
I0512 06:27:44.718045  9597 net.cpp:110] Creating Layer conv3
I0512 06:27:44.718057  9597 net.cpp:477] conv3 <- pool2
I0512 06:27:44.718065  9597 net.cpp:433] conv3 -> conv3
I0512 06:27:44.720751  9597 net.cpp:155] Setting up conv3
I0512 06:27:44.720765  9597 net.cpp:163] Top shape: 1 384 15 15 (86400)
I0512 06:27:44.720777  9597 layer_factory.hpp:76] Creating layer relu3
I0512 06:27:44.720787  9597 net.cpp:110] Creating Layer relu3
I0512 06:27:44.720793  9597 net.cpp:477] relu3 <- conv3
I0512 06:27:44.720798  9597 net.cpp:419] relu3 -> conv3 (in-place)
I0512 06:27:44.720805  9597 net.cpp:155] Setting up relu3
I0512 06:27:44.720810  9597 net.cpp:163] Top shape: 1 384 15 15 (86400)
I0512 06:27:44.720814  9597 layer_factory.hpp:76] Creating layer conv4
I0512 06:27:44.720820  9597 net.cpp:110] Creating Layer conv4
I0512 06:27:44.720824  9597 net.cpp:477] conv4 <- conv3
I0512 06:27:44.720830  9597 net.cpp:433] conv4 -> conv4
I0512 06:27:44.724727  9597 net.cpp:155] Setting up conv4
I0512 06:27:44.724742  9597 net.cpp:163] Top shape: 1 384 15 15 (86400)
I0512 06:27:44.724750  9597 layer_factory.hpp:76] Creating layer relu4
I0512 06:27:44.724756  9597 net.cpp:110] Creating Layer relu4
I0512 06:27:44.724761  9597 net.cpp:477] relu4 <- conv4
I0512 06:27:44.724766  9597 net.cpp:419] relu4 -> conv4 (in-place)
I0512 06:27:44.724772  9597 net.cpp:155] Setting up relu4
I0512 06:27:44.724778  9597 net.cpp:163] Top shape: 1 384 15 15 (86400)
I0512 06:27:44.724782  9597 layer_factory.hpp:76] Creating layer conv5
I0512 06:27:44.724788  9597 net.cpp:110] Creating Layer conv5
I0512 06:27:44.724792  9597 net.cpp:477] conv5 <- conv4
I0512 06:27:44.724797  9597 net.cpp:433] conv5 -> conv5
I0512 06:27:44.727457  9597 net.cpp:155] Setting up conv5
I0512 06:27:44.727471  9597 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:27:44.727483  9597 layer_factory.hpp:76] Creating layer relu5
I0512 06:27:44.727490  9597 net.cpp:110] Creating Layer relu5
I0512 06:27:44.727494  9597 net.cpp:477] relu5 <- conv5
I0512 06:27:44.727499  9597 net.cpp:419] relu5 -> conv5 (in-place)
I0512 06:27:44.727509  9597 net.cpp:155] Setting up relu5
I0512 06:27:44.727514  9597 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:27:44.727516  9597 layer_factory.hpp:76] Creating layer rpn_conv1
I0512 06:27:44.727524  9597 net.cpp:110] Creating Layer rpn_conv1
I0512 06:27:44.727529  9597 net.cpp:477] rpn_conv1 <- conv5
I0512 06:27:44.727533  9597 net.cpp:433] rpn_conv1 -> rpn_conv1
I0512 06:27:44.729358  9597 net.cpp:155] Setting up rpn_conv1
I0512 06:27:44.729372  9597 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:27:44.729380  9597 layer_factory.hpp:76] Creating layer rpn_relu1
I0512 06:27:44.729387  9597 net.cpp:110] Creating Layer rpn_relu1
I0512 06:27:44.729390  9597 net.cpp:477] rpn_relu1 <- rpn_conv1
I0512 06:27:44.729398  9597 net.cpp:419] rpn_relu1 -> rpn_conv1 (in-place)
I0512 06:27:44.729404  9597 net.cpp:155] Setting up rpn_relu1
I0512 06:27:44.729409  9597 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:27:44.729413  9597 layer_factory.hpp:76] Creating layer rpn_conv1_rpn_relu1_0_split
I0512 06:27:44.729423  9597 net.cpp:110] Creating Layer rpn_conv1_rpn_relu1_0_split
I0512 06:27:44.729428  9597 net.cpp:477] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I0512 06:27:44.729431  9597 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I0512 06:27:44.729437  9597 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I0512 06:27:44.729444  9597 net.cpp:155] Setting up rpn_conv1_rpn_relu1_0_split
I0512 06:27:44.729449  9597 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:27:44.729452  9597 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:27:44.729456  9597 layer_factory.hpp:76] Creating layer rpn_cls_score
I0512 06:27:44.729462  9597 net.cpp:110] Creating Layer rpn_cls_score
I0512 06:27:44.729465  9597 net.cpp:477] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I0512 06:27:44.729472  9597 net.cpp:433] rpn_cls_score -> rpn_cls_score
I0512 06:27:44.729529  9597 net.cpp:155] Setting up rpn_cls_score
I0512 06:27:44.729537  9597 net.cpp:163] Top shape: 1 18 15 15 (4050)
I0512 06:27:44.729544  9597 layer_factory.hpp:76] Creating layer rpn_bbox_pred
I0512 06:27:44.729549  9597 net.cpp:110] Creating Layer rpn_bbox_pred
I0512 06:27:44.729568  9597 net.cpp:477] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I0512 06:27:44.729573  9597 net.cpp:433] rpn_bbox_pred -> rpn_bbox_pred
I0512 06:27:44.729630  9597 net.cpp:155] Setting up rpn_bbox_pred
I0512 06:27:44.729638  9597 net.cpp:163] Top shape: 1 36 15 15 (8100)
I0512 06:27:44.729645  9597 layer_factory.hpp:76] Creating layer rpn_cls_score_reshape
I0512 06:27:44.729655  9597 net.cpp:110] Creating Layer rpn_cls_score_reshape
I0512 06:27:44.729660  9597 net.cpp:477] rpn_cls_score_reshape <- rpn_cls_score
I0512 06:27:44.729665  9597 net.cpp:433] rpn_cls_score_reshape -> rpn_cls_score_reshape
I0512 06:27:44.729682  9597 net.cpp:155] Setting up rpn_cls_score_reshape
I0512 06:27:44.729689  9597 net.cpp:163] Top shape: 1 2 135 15 (4050)
I0512 06:27:44.729693  9597 layer_factory.hpp:76] Creating layer rpn_cls_prob
I0512 06:27:44.729701  9597 net.cpp:110] Creating Layer rpn_cls_prob
I0512 06:27:44.729706  9597 net.cpp:477] rpn_cls_prob <- rpn_cls_score_reshape
I0512 06:27:44.729718  9597 net.cpp:433] rpn_cls_prob -> rpn_cls_prob
I0512 06:27:44.729745  9597 net.cpp:155] Setting up rpn_cls_prob
I0512 06:27:44.729753  9597 net.cpp:163] Top shape: 1 2 135 15 (4050)
I0512 06:27:44.729756  9597 layer_factory.hpp:76] Creating layer rpn_cls_prob_reshape
I0512 06:27:44.729763  9597 net.cpp:110] Creating Layer rpn_cls_prob_reshape
I0512 06:27:44.729766  9597 net.cpp:477] rpn_cls_prob_reshape <- rpn_cls_prob
I0512 06:27:44.729771  9597 net.cpp:433] rpn_cls_prob_reshape -> rpn_cls_prob_reshape
I0512 06:27:44.729779  9597 net.cpp:155] Setting up rpn_cls_prob_reshape
I0512 06:27:44.729784  9597 net.cpp:163] Top shape: 1 18 15 15 (4050)
I0512 06:27:44.729789  9597 layer_factory.hpp:76] Creating layer proposal
I0512 06:27:44.731681  9597 net.cpp:110] Creating Layer proposal
I0512 06:27:44.731698  9597 net.cpp:477] proposal <- rpn_cls_prob_reshape
I0512 06:27:44.731704  9597 net.cpp:477] proposal <- rpn_bbox_pred
I0512 06:27:44.731709  9597 net.cpp:477] proposal <- im_info
I0512 06:27:44.731715  9597 net.cpp:433] proposal -> rois
I0512 06:27:44.731724  9597 net.cpp:433] proposal -> scores
I0512 06:27:44.732956  9597 net.cpp:155] Setting up proposal
I0512 06:27:44.732975  9597 net.cpp:163] Top shape: 1 5 (5)
I0512 06:27:44.732980  9597 net.cpp:163] Top shape: 1 1 1 1 (1)
I0512 06:27:44.732985  9597 net.cpp:240] proposal does not need backward computation.
I0512 06:27:44.732996  9597 net.cpp:240] rpn_cls_prob_reshape does not need backward computation.
I0512 06:27:44.733000  9597 net.cpp:240] rpn_cls_prob does not need backward computation.
I0512 06:27:44.733003  9597 net.cpp:240] rpn_cls_score_reshape does not need backward computation.
I0512 06:27:44.733007  9597 net.cpp:240] rpn_bbox_pred does not need backward computation.
I0512 06:27:44.733011  9597 net.cpp:240] rpn_cls_score does not need backward computation.
I0512 06:27:44.733014  9597 net.cpp:240] rpn_conv1_rpn_relu1_0_split does not need backward computation.
I0512 06:27:44.733018  9597 net.cpp:240] rpn_relu1 does not need backward computation.
I0512 06:27:44.733021  9597 net.cpp:240] rpn_conv1 does not need backward computation.
I0512 06:27:44.733026  9597 net.cpp:240] relu5 does not need backward computation.
I0512 06:27:44.733027  9597 net.cpp:240] conv5 does not need backward computation.
I0512 06:27:44.733031  9597 net.cpp:240] relu4 does not need backward computation.
I0512 06:27:44.733033  9597 net.cpp:240] conv4 does not need backward computation.
I0512 06:27:44.733037  9597 net.cpp:240] relu3 does not need backward computation.
I0512 06:27:44.733039  9597 net.cpp:240] conv3 does not need backward computation.
I0512 06:27:44.733043  9597 net.cpp:240] pool2 does not need backward computation.
I0512 06:27:44.733047  9597 net.cpp:240] norm2 does not need backward computation.
I0512 06:27:44.733049  9597 net.cpp:240] relu2 does not need backward computation.
I0512 06:27:44.733052  9597 net.cpp:240] conv2 does not need backward computation.
I0512 06:27:44.733057  9597 net.cpp:240] pool1 does not need backward computation.
I0512 06:27:44.733060  9597 net.cpp:240] norm1 does not need backward computation.
I0512 06:27:44.733063  9597 net.cpp:240] relu1 does not need backward computation.
I0512 06:27:44.733067  9597 net.cpp:240] conv1 does not need backward computation.
I0512 06:27:44.733070  9597 net.cpp:283] This network produces output rois
I0512 06:27:44.733074  9597 net.cpp:283] This network produces output scores
I0512 06:27:44.733093  9597 net.cpp:297] Network initialization done.
I0512 06:27:44.733098  9597 net.cpp:298] Memory required for data: 21374280
Output will be saved to `/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval`
im_proposals: 1/1000 0.068s
im_proposals: 2/1000 0.070s
im_proposals: 3/1000 0.064s
im_proposals: 4/1000 0.062s
im_proposals: 5/1000 0.062s
im_proposals: 6/1000 0.060s
im_proposals: 7/1000 0.059s
im_proposals: 8/1000 0.058s
im_proposals: 9/1000 0.059s
im_proposals: 10/1000 0.059s
im_proposals: 11/1000 0.059s
im_proposals: 12/1000 0.059s
im_proposals: 13/1000 0.059s
im_proposals: 14/1000 0.060s
im_proposals: 15/1000 0.059s
im_proposals: 16/1000 0.059s
im_proposals: 17/1000 0.059s
im_proposals: 18/1000 0.059s
im_proposals: 19/1000 0.058s
im_proposals: 20/1000 0.058s
im_proposals: 21/1000 0.059s
im_proposals: 22/1000 0.059s
im_proposals: 23/1000 0.059s
im_proposals: 24/1000 0.059s
im_proposals: 25/1000 0.059s
im_proposals: 26/1000 0.058s
im_proposals: 27/1000 0.058s
im_proposals: 28/1000 0.058s
im_proposals: 29/1000 0.058s
im_proposals: 30/1000 0.058s
im_proposals: 31/1000 0.058s
im_proposals: 32/1000 0.058s
im_proposals: 33/1000 0.058s
im_proposals: 34/1000 0.058s
im_proposals: 35/1000 0.058s
im_proposals: 36/1000 0.057s
im_proposals: 37/1000 0.058s
im_proposals: 38/1000 0.057s
im_proposals: 39/1000 0.057s
im_proposals: 40/1000 0.057s
im_proposals: 41/1000 0.057s
im_proposals: 42/1000 0.057s
im_proposals: 43/1000 0.057s
im_proposals: 44/1000 0.057s
im_proposals: 45/1000 0.057s
im_proposals: 46/1000 0.057s
im_proposals: 47/1000 0.057s
im_proposals: 48/1000 0.057s
im_proposals: 49/1000 0.057s
im_proposals: 50/1000 0.057s
im_proposals: 51/1000 0.057s
im_proposals: 52/1000 0.057s
im_proposals: 53/1000 0.057s
im_proposals: 54/1000 0.057s
im_proposals: 55/1000 0.057s
im_proposals: 56/1000 0.057s
im_proposals: 57/1000 0.057s
im_proposals: 58/1000 0.056s
im_proposals: 59/1000 0.056s
im_proposals: 60/1000 0.056s
im_proposals: 61/1000 0.056s
im_proposals: 62/1000 0.056s
im_proposals: 63/1000 0.056s
im_proposals: 64/1000 0.056s
im_proposals: 65/1000 0.056s
im_proposals: 66/1000 0.056s
im_proposals: 67/1000 0.056s
im_proposals: 68/1000 0.056s
im_proposals: 69/1000 0.056s
im_proposals: 70/1000 0.056s
im_proposals: 71/1000 0.056s
im_proposals: 72/1000 0.056s
im_proposals: 73/1000 0.056s
im_proposals: 74/1000 0.056s
im_proposals: 75/1000 0.056s
im_proposals: 76/1000 0.056s
im_proposals: 77/1000 0.056s
im_proposals: 78/1000 0.056s
im_proposals: 79/1000 0.056s
im_proposals: 80/1000 0.056s
im_proposals: 81/1000 0.056s
im_proposals: 82/1000 0.055s
im_proposals: 83/1000 0.055s
im_proposals: 84/1000 0.055s
im_proposals: 85/1000 0.056s
im_proposals: 86/1000 0.056s
im_proposals: 87/1000 0.056s
im_proposals: 88/1000 0.056s
im_proposals: 89/1000 0.056s
im_proposals: 90/1000 0.056s
im_proposals: 91/1000 0.056s
im_proposals: 92/1000 0.056s
im_proposals: 93/1000 0.056s
im_proposals: 94/1000 0.056s
im_proposals: 95/1000 0.056s
im_proposals: 96/1000 0.056s
im_proposals: 97/1000 0.056s
im_proposals: 98/1000 0.056s
im_proposals: 99/1000 0.056s
im_proposals: 100/1000 0.056s
im_proposals: 101/1000 0.056s
im_proposals: 102/1000 0.056s
im_proposals: 103/1000 0.056s
im_proposals: 104/1000 0.056s
im_proposals: 105/1000 0.056s
im_proposals: 106/1000 0.056s
im_proposals: 107/1000 0.055s
im_proposals: 108/1000 0.055s
im_proposals: 109/1000 0.055s
im_proposals: 110/1000 0.055s
im_proposals: 111/1000 0.055s
im_proposals: 112/1000 0.055s
im_proposals: 113/1000 0.055s
im_proposals: 114/1000 0.055s
im_proposals: 115/1000 0.055s
im_proposals: 116/1000 0.055s
im_proposals: 117/1000 0.055s
im_proposals: 118/1000 0.055s
im_proposals: 119/1000 0.055s
im_proposals: 120/1000 0.055s
im_proposals: 121/1000 0.055s
im_proposals: 122/1000 0.055s
im_proposals: 123/1000 0.055s
im_proposals: 124/1000 0.055s
im_proposals: 125/1000 0.055s
im_proposals: 126/1000 0.055s
im_proposals: 127/1000 0.055s
im_proposals: 128/1000 0.055s
im_proposals: 129/1000 0.055s
im_proposals: 130/1000 0.055s
im_proposals: 131/1000 0.055s
im_proposals: 132/1000 0.055s
im_proposals: 133/1000 0.055s
im_proposals: 134/1000 0.055s
im_proposals: 135/1000 0.055s
im_proposals: 136/1000 0.055s
im_proposals: 137/1000 0.055s
im_proposals: 138/1000 0.055s
im_proposals: 139/1000 0.055s
im_proposals: 140/1000 0.055s
im_proposals: 141/1000 0.055s
im_proposals: 142/1000 0.055s
im_proposals: 143/1000 0.055s
im_proposals: 144/1000 0.055s
im_proposals: 145/1000 0.055s
im_proposals: 146/1000 0.055s
im_proposals: 147/1000 0.055s
im_proposals: 148/1000 0.055s
im_proposals: 149/1000 0.055s
im_proposals: 150/1000 0.055s
im_proposals: 151/1000 0.055s
im_proposals: 152/1000 0.055s
im_proposals: 153/1000 0.055s
im_proposals: 154/1000 0.055s
im_proposals: 155/1000 0.055s
im_proposals: 156/1000 0.055s
im_proposals: 157/1000 0.055s
im_proposals: 158/1000 0.055s
im_proposals: 159/1000 0.055s
im_proposals: 160/1000 0.055s
im_proposals: 161/1000 0.055s
im_proposals: 162/1000 0.055s
im_proposals: 163/1000 0.055s
im_proposals: 164/1000 0.055s
im_proposals: 165/1000 0.055s
im_proposals: 166/1000 0.055s
im_proposals: 167/1000 0.055s
im_proposals: 168/1000 0.055s
im_proposals: 169/1000 0.055s
im_proposals: 170/1000 0.055s
im_proposals: 171/1000 0.055s
im_proposals: 172/1000 0.055s
im_proposals: 173/1000 0.055s
im_proposals: 174/1000 0.055s
im_proposals: 175/1000 0.055s
im_proposals: 176/1000 0.055s
im_proposals: 177/1000 0.055s
im_proposals: 178/1000 0.055s
im_proposals: 179/1000 0.055s
im_proposals: 180/1000 0.055s
im_proposals: 181/1000 0.055s
im_proposals: 182/1000 0.055s
im_proposals: 183/1000 0.055s
im_proposals: 184/1000 0.055s
im_proposals: 185/1000 0.055s
im_proposals: 186/1000 0.055s
im_proposals: 187/1000 0.055s
im_proposals: 188/1000 0.055s
im_proposals: 189/1000 0.055s
im_proposals: 190/1000 0.055s
im_proposals: 191/1000 0.055s
im_proposals: 192/1000 0.055s
im_proposals: 193/1000 0.055s
im_proposals: 194/1000 0.055s
im_proposals: 195/1000 0.055s
im_proposals: 196/1000 0.055s
im_proposals: 197/1000 0.055s
im_proposals: 198/1000 0.055s
im_proposals: 199/1000 0.055s
im_proposals: 200/1000 0.055s
im_proposals: 201/1000 0.055s
im_proposals: 202/1000 0.055s
im_proposals: 203/1000 0.055s
im_proposals: 204/1000 0.055s
im_proposals: 205/1000 0.055s
im_proposals: 206/1000 0.055s
im_proposals: 207/1000 0.055s
im_proposals: 208/1000 0.055s
im_proposals: 209/1000 0.055s
im_proposals: 210/1000 0.055s
im_proposals: 211/1000 0.055s
im_proposals: 212/1000 0.055s
im_proposals: 213/1000 0.055s
im_proposals: 214/1000 0.055s
im_proposals: 215/1000 0.055s
im_proposals: 216/1000 0.055s
im_proposals: 217/1000 0.055s
im_proposals: 218/1000 0.055s
im_proposals: 219/1000 0.055s
im_proposals: 220/1000 0.055s
im_proposals: 221/1000 0.055s
im_proposals: 222/1000 0.055s
im_proposals: 223/1000 0.055s
im_proposals: 224/1000 0.055s
im_proposals: 225/1000 0.055s
im_proposals: 226/1000 0.055s
im_proposals: 227/1000 0.055s
im_proposals: 228/1000 0.055s
im_proposals: 229/1000 0.055s
im_proposals: 230/1000 0.055s
im_proposals: 231/1000 0.055s
im_proposals: 232/1000 0.055s
im_proposals: 233/1000 0.055s
im_proposals: 234/1000 0.055s
im_proposals: 235/1000 0.055s
im_proposals: 236/1000 0.055s
im_proposals: 237/1000 0.055s
im_proposals: 238/1000 0.055s
im_proposals: 239/1000 0.055s
im_proposals: 240/1000 0.055s
im_proposals: 241/1000 0.055s
im_proposals: 242/1000 0.055s
im_proposals: 243/1000 0.055s
im_proposals: 244/1000 0.055s
im_proposals: 245/1000 0.055s
im_proposals: 246/1000 0.055s
im_proposals: 247/1000 0.055s
im_proposals: 248/1000 0.055s
im_proposals: 249/1000 0.055s
im_proposals: 250/1000 0.055s
im_proposals: 251/1000 0.055s
im_proposals: 252/1000 0.055s
im_proposals: 253/1000 0.055s
im_proposals: 254/1000 0.054s
im_proposals: 255/1000 0.055s
im_proposals: 256/1000 0.055s
im_proposals: 257/1000 0.055s
im_proposals: 258/1000 0.055s
im_proposals: 259/1000 0.055s
im_proposals: 260/1000 0.055s
im_proposals: 261/1000 0.055s
im_proposals: 262/1000 0.055s
im_proposals: 263/1000 0.055s
im_proposals: 264/1000 0.055s
im_proposals: 265/1000 0.055s
im_proposals: 266/1000 0.055s
im_proposals: 267/1000 0.055s
im_proposals: 268/1000 0.055s
im_proposals: 269/1000 0.055s
im_proposals: 270/1000 0.055s
im_proposals: 271/1000 0.055s
im_proposals: 272/1000 0.055s
im_proposals: 273/1000 0.055s
im_proposals: 274/1000 0.055s
im_proposals: 275/1000 0.055s
im_proposals: 276/1000 0.055s
im_proposals: 277/1000 0.055s
im_proposals: 278/1000 0.055s
im_proposals: 279/1000 0.055s
im_proposals: 280/1000 0.055s
im_proposals: 281/1000 0.055s
im_proposals: 282/1000 0.055s
im_proposals: 283/1000 0.055s
im_proposals: 284/1000 0.055s
im_proposals: 285/1000 0.055s
im_proposals: 286/1000 0.055s
im_proposals: 287/1000 0.055s
im_proposals: 288/1000 0.055s
im_proposals: 289/1000 0.055s
im_proposals: 290/1000 0.055s
im_proposals: 291/1000 0.055s
im_proposals: 292/1000 0.055s
im_proposals: 293/1000 0.055s
im_proposals: 294/1000 0.055s
im_proposals: 295/1000 0.055s
im_proposals: 296/1000 0.055s
im_proposals: 297/1000 0.055s
im_proposals: 298/1000 0.055s
im_proposals: 299/1000 0.055s
im_proposals: 300/1000 0.055s
im_proposals: 301/1000 0.055s
im_proposals: 302/1000 0.055s
im_proposals: 303/1000 0.055s
im_proposals: 304/1000 0.055s
im_proposals: 305/1000 0.055s
im_proposals: 306/1000 0.055s
im_proposals: 307/1000 0.055s
im_proposals: 308/1000 0.055s
im_proposals: 309/1000 0.055s
im_proposals: 310/1000 0.055s
im_proposals: 311/1000 0.055s
im_proposals: 312/1000 0.055s
im_proposals: 313/1000 0.055s
im_proposals: 314/1000 0.055s
im_proposals: 315/1000 0.055s
im_proposals: 316/1000 0.055s
im_proposals: 317/1000 0.055s
im_proposals: 318/1000 0.055s
im_proposals: 319/1000 0.055s
im_proposals: 320/1000 0.055s
im_proposals: 321/1000 0.055s
im_proposals: 322/1000 0.055s
im_proposals: 323/1000 0.055s
im_proposals: 324/1000 0.055s
im_proposals: 325/1000 0.055s
im_proposals: 326/1000 0.055s
im_proposals: 327/1000 0.055s
im_proposals: 328/1000 0.055s
im_proposals: 329/1000 0.055s
im_proposals: 330/1000 0.055s
im_proposals: 331/1000 0.055s
im_proposals: 332/1000 0.055s
im_proposals: 333/1000 0.055s
im_proposals: 334/1000 0.055s
im_proposals: 335/1000 0.055s
im_proposals: 336/1000 0.055s
im_proposals: 337/1000 0.055s
im_proposals: 338/1000 0.055s
im_proposals: 339/1000 0.055s
im_proposals: 340/1000 0.055s
im_proposals: 341/1000 0.055s
im_proposals: 342/1000 0.055s
im_proposals: 343/1000 0.055s
im_proposals: 344/1000 0.055s
im_proposals: 345/1000 0.055s
im_proposals: 346/1000 0.055s
im_proposals: 347/1000 0.055s
im_proposals: 348/1000 0.055s
im_proposals: 349/1000 0.055s
im_proposals: 350/1000 0.055s
im_proposals: 351/1000 0.055s
im_proposals: 352/1000 0.055s
im_proposals: 353/1000 0.055s
im_proposals: 354/1000 0.055s
im_proposals: 355/1000 0.055s
im_proposals: 356/1000 0.055s
im_proposals: 357/1000 0.055s
im_proposals: 358/1000 0.055s
im_proposals: 359/1000 0.055s
im_proposals: 360/1000 0.055s
im_proposals: 361/1000 0.055s
im_proposals: 362/1000 0.055s
im_proposals: 363/1000 0.055s
im_proposals: 364/1000 0.055s
im_proposals: 365/1000 0.055s
im_proposals: 366/1000 0.055s
im_proposals: 367/1000 0.055s
im_proposals: 368/1000 0.055s
im_proposals: 369/1000 0.055s
im_proposals: 370/1000 0.055s
im_proposals: 371/1000 0.055s
im_proposals: 372/1000 0.055s
im_proposals: 373/1000 0.055s
im_proposals: 374/1000 0.055s
im_proposals: 375/1000 0.055s
im_proposals: 376/1000 0.055s
im_proposals: 377/1000 0.055s
im_proposals: 378/1000 0.055s
im_proposals: 379/1000 0.055s
im_proposals: 380/1000 0.055s
im_proposals: 381/1000 0.055s
im_proposals: 382/1000 0.055s
im_proposals: 383/1000 0.055s
im_proposals: 384/1000 0.055s
im_proposals: 385/1000 0.055s
im_proposals: 386/1000 0.055s
im_proposals: 387/1000 0.055s
im_proposals: 388/1000 0.055s
im_proposals: 389/1000 0.055s
im_proposals: 390/1000 0.055s
im_proposals: 391/1000 0.055s
im_proposals: 392/1000 0.055s
im_proposals: 393/1000 0.055s
im_proposals: 394/1000 0.055s
im_proposals: 395/1000 0.055s
im_proposals: 396/1000 0.055s
im_proposals: 397/1000 0.055s
im_proposals: 398/1000 0.055s
im_proposals: 399/1000 0.055s
im_proposals: 400/1000 0.055s
im_proposals: 401/1000 0.055s
im_proposals: 402/1000 0.055s
im_proposals: 403/1000 0.055s
im_proposals: 404/1000 0.055s
im_proposals: 405/1000 0.055s
im_proposals: 406/1000 0.055s
im_proposals: 407/1000 0.055s
im_proposals: 408/1000 0.055s
im_proposals: 409/1000 0.055s
im_proposals: 410/1000 0.055s
im_proposals: 411/1000 0.055s
im_proposals: 412/1000 0.055s
im_proposals: 413/1000 0.055s
im_proposals: 414/1000 0.055s
im_proposals: 415/1000 0.055s
im_proposals: 416/1000 0.055s
im_proposals: 417/1000 0.055s
im_proposals: 418/1000 0.055s
im_proposals: 419/1000 0.055s
im_proposals: 420/1000 0.055s
im_proposals: 421/1000 0.055s
im_proposals: 422/1000 0.055s
im_proposals: 423/1000 0.055s
im_proposals: 424/1000 0.055s
im_proposals: 425/1000 0.055s
im_proposals: 426/1000 0.055s
im_proposals: 427/1000 0.055s
im_proposals: 428/1000 0.055s
im_proposals: 429/1000 0.055s
im_proposals: 430/1000 0.055s
im_proposals: 431/1000 0.055s
im_proposals: 432/1000 0.055s
im_proposals: 433/1000 0.055s
im_proposals: 434/1000 0.055s
im_proposals: 435/1000 0.055s
im_proposals: 436/1000 0.055s
im_proposals: 437/1000 0.055s
im_proposals: 438/1000 0.055s
im_proposals: 439/1000 0.055s
im_proposals: 440/1000 0.055s
im_proposals: 441/1000 0.055s
im_proposals: 442/1000 0.055s
im_proposals: 443/1000 0.055s
im_proposals: 444/1000 0.055s
im_proposals: 445/1000 0.055s
im_proposals: 446/1000 0.055s
im_proposals: 447/1000 0.055s
im_proposals: 448/1000 0.055s
im_proposals: 449/1000 0.055s
im_proposals: 450/1000 0.055s
im_proposals: 451/1000 0.055s
im_proposals: 452/1000 0.055s
im_proposals: 453/1000 0.055s
im_proposals: 454/1000 0.055s
im_proposals: 455/1000 0.055s
im_proposals: 456/1000 0.055s
im_proposals: 457/1000 0.055s
im_proposals: 458/1000 0.055s
im_proposals: 459/1000 0.055s
im_proposals: 460/1000 0.055s
im_proposals: 461/1000 0.055s
im_proposals: 462/1000 0.055s
im_proposals: 463/1000 0.055s
im_proposals: 464/1000 0.055s
im_proposals: 465/1000 0.055s
im_proposals: 466/1000 0.055s
im_proposals: 467/1000 0.055s
im_proposals: 468/1000 0.055s
im_proposals: 469/1000 0.055s
im_proposals: 470/1000 0.055s
im_proposals: 471/1000 0.055s
im_proposals: 472/1000 0.055s
im_proposals: 473/1000 0.055s
im_proposals: 474/1000 0.055s
im_proposals: 475/1000 0.055s
im_proposals: 476/1000 0.055s
im_proposals: 477/1000 0.055s
im_proposals: 478/1000 0.055s
im_proposals: 479/1000 0.055s
im_proposals: 480/1000 0.055s
im_proposals: 481/1000 0.055s
im_proposals: 482/1000 0.055s
im_proposals: 483/1000 0.055s
im_proposals: 484/1000 0.055s
im_proposals: 485/1000 0.055s
im_proposals: 486/1000 0.055s
im_proposals: 487/1000 0.055s
im_proposals: 488/1000 0.055s
im_proposals: 489/1000 0.055s
im_proposals: 490/1000 0.055s
im_proposals: 491/1000 0.055s
im_proposals: 492/1000 0.055s
im_proposals: 493/1000 0.055s
im_proposals: 494/1000 0.055s
im_proposals: 495/1000 0.055s
im_proposals: 496/1000 0.055s
im_proposals: 497/1000 0.055s
im_proposals: 498/1000 0.055s
im_proposals: 499/1000 0.055s
im_proposals: 500/1000 0.055s
im_proposals: 501/1000 0.055s
im_proposals: 502/1000 0.055s
im_proposals: 503/1000 0.055s
im_proposals: 504/1000 0.055s
im_proposals: 505/1000 0.055s
im_proposals: 506/1000 0.055s
im_proposals: 507/1000 0.055s
im_proposals: 508/1000 0.055s
im_proposals: 509/1000 0.055s
im_proposals: 510/1000 0.055s
im_proposals: 511/1000 0.055s
im_proposals: 512/1000 0.055s
im_proposals: 513/1000 0.055s
im_proposals: 514/1000 0.055s
im_proposals: 515/1000 0.055s
im_proposals: 516/1000 0.055s
im_proposals: 517/1000 0.055s
im_proposals: 518/1000 0.055s
im_proposals: 519/1000 0.055s
im_proposals: 520/1000 0.055s
im_proposals: 521/1000 0.055s
im_proposals: 522/1000 0.055s
im_proposals: 523/1000 0.055s
im_proposals: 524/1000 0.055s
im_proposals: 525/1000 0.055s
im_proposals: 526/1000 0.055s
im_proposals: 527/1000 0.055s
im_proposals: 528/1000 0.055s
im_proposals: 529/1000 0.055s
im_proposals: 530/1000 0.055s
im_proposals: 531/1000 0.055s
im_proposals: 532/1000 0.055s
im_proposals: 533/1000 0.055s
im_proposals: 534/1000 0.055s
im_proposals: 535/1000 0.055s
im_proposals: 536/1000 0.055s
im_proposals: 537/1000 0.055s
im_proposals: 538/1000 0.055s
im_proposals: 539/1000 0.055s
im_proposals: 540/1000 0.055s
im_proposals: 541/1000 0.055s
im_proposals: 542/1000 0.055s
im_proposals: 543/1000 0.055s
im_proposals: 544/1000 0.055s
im_proposals: 545/1000 0.055s
im_proposals: 546/1000 0.055s
im_proposals: 547/1000 0.055s
im_proposals: 548/1000 0.055s
im_proposals: 549/1000 0.055s
im_proposals: 550/1000 0.055s
im_proposals: 551/1000 0.055s
im_proposals: 552/1000 0.055s
im_proposals: 553/1000 0.055s
im_proposals: 554/1000 0.055s
im_proposals: 555/1000 0.055s
im_proposals: 556/1000 0.055s
im_proposals: 557/1000 0.055s
im_proposals: 558/1000 0.055s
im_proposals: 559/1000 0.055s
im_proposals: 560/1000 0.055s
im_proposals: 561/1000 0.055s
im_proposals: 562/1000 0.055s
im_proposals: 563/1000 0.055s
im_proposals: 564/1000 0.055s
im_proposals: 565/1000 0.055s
im_proposals: 566/1000 0.055s
im_proposals: 567/1000 0.055s
im_proposals: 568/1000 0.055s
im_proposals: 569/1000 0.055s
im_proposals: 570/1000 0.055s
im_proposals: 571/1000 0.055s
im_proposals: 572/1000 0.055s
im_proposals: 573/1000 0.055s
im_proposals: 574/1000 0.055s
im_proposals: 575/1000 0.055s
im_proposals: 576/1000 0.055s
im_proposals: 577/1000 0.055s
im_proposals: 578/1000 0.055s
im_proposals: 579/1000 0.055s
im_proposals: 580/1000 0.055s
im_proposals: 581/1000 0.055s
im_proposals: 582/1000 0.055s
im_proposals: 583/1000 0.055s
im_proposals: 584/1000 0.055s
im_proposals: 585/1000 0.055s
im_proposals: 586/1000 0.055s
im_proposals: 587/1000 0.055s
im_proposals: 588/1000 0.055s
im_proposals: 589/1000 0.055s
im_proposals: 590/1000 0.055s
im_proposals: 591/1000 0.055s
im_proposals: 592/1000 0.055s
im_proposals: 593/1000 0.055s
im_proposals: 594/1000 0.055s
im_proposals: 595/1000 0.055s
im_proposals: 596/1000 0.055s
im_proposals: 597/1000 0.055s
im_proposals: 598/1000 0.055s
im_proposals: 599/1000 0.055s
im_proposals: 600/1000 0.055s
im_proposals: 601/1000 0.055s
im_proposals: 602/1000 0.055s
im_proposals: 603/1000 0.055s
im_proposals: 604/1000 0.055s
im_proposals: 605/1000 0.055s
im_proposals: 606/1000 0.055s
im_proposals: 607/1000 0.055s
im_proposals: 608/1000 0.055s
im_proposals: 609/1000 0.055s
im_proposals: 610/1000 0.055s
im_proposals: 611/1000 0.055s
im_proposals: 612/1000 0.055s
im_proposals: 613/1000 0.055s
im_proposals: 614/1000 0.055s
im_proposals: 615/1000 0.055s
im_proposals: 616/1000 0.055s
im_proposals: 617/1000 0.055s
im_proposals: 618/1000 0.055s
im_proposals: 619/1000 0.055s
im_proposals: 620/1000 0.055s
im_proposals: 621/1000 0.055s
im_proposals: 622/1000 0.055s
im_proposals: 623/1000 0.055s
im_proposals: 624/1000 0.055s
im_proposals: 625/1000 0.055s
im_proposals: 626/1000 0.055s
im_proposals: 627/1000 0.055s
im_proposals: 628/1000 0.055s
im_proposals: 629/1000 0.055s
im_proposals: 630/1000 0.055s
im_proposals: 631/1000 0.055s
im_proposals: 632/1000 0.055s
im_proposals: 633/1000 0.055s
im_proposals: 634/1000 0.055s
im_proposals: 635/1000 0.055s
im_proposals: 636/1000 0.055s
im_proposals: 637/1000 0.055s
im_proposals: 638/1000 0.055s
im_proposals: 639/1000 0.055s
im_proposals: 640/1000 0.055s
im_proposals: 641/1000 0.055s
im_proposals: 642/1000 0.055s
im_proposals: 643/1000 0.055s
im_proposals: 644/1000 0.055s
im_proposals: 645/1000 0.055s
im_proposals: 646/1000 0.055s
im_proposals: 647/1000 0.055s
im_proposals: 648/1000 0.055s
im_proposals: 649/1000 0.055s
im_proposals: 650/1000 0.055s
im_proposals: 651/1000 0.055s
im_proposals: 652/1000 0.055s
im_proposals: 653/1000 0.055s
im_proposals: 654/1000 0.055s
im_proposals: 655/1000 0.055s
im_proposals: 656/1000 0.055s
im_proposals: 657/1000 0.055s
im_proposals: 658/1000 0.055s
im_proposals: 659/1000 0.055s
im_proposals: 660/1000 0.055s
im_proposals: 661/1000 0.055s
im_proposals: 662/1000 0.055s
im_proposals: 663/1000 0.055s
im_proposals: 664/1000 0.055s
im_proposals: 665/1000 0.055s
im_proposals: 666/1000 0.055s
im_proposals: 667/1000 0.055s
im_proposals: 668/1000 0.055s
im_proposals: 669/1000 0.055s
im_proposals: 670/1000 0.055s
im_proposals: 671/1000 0.055s
im_proposals: 672/1000 0.055s
im_proposals: 673/1000 0.055s
im_proposals: 674/1000 0.055s
im_proposals: 675/1000 0.055s
im_proposals: 676/1000 0.055s
im_proposals: 677/1000 0.055s
im_proposals: 678/1000 0.055s
im_proposals: 679/1000 0.055s
im_proposals: 680/1000 0.055s
im_proposals: 681/1000 0.055s
im_proposals: 682/1000 0.055s
im_proposals: 683/1000 0.055s
im_proposals: 684/1000 0.055s
im_proposals: 685/1000 0.055s
im_proposals: 686/1000 0.055s
im_proposals: 687/1000 0.055s
im_proposals: 688/1000 0.055s
im_proposals: 689/1000 0.055s
im_proposals: 690/1000 0.055s
im_proposals: 691/1000 0.055s
im_proposals: 692/1000 0.055s
im_proposals: 693/1000 0.055s
im_proposals: 694/1000 0.055s
im_proposals: 695/1000 0.055s
im_proposals: 696/1000 0.055s
im_proposals: 697/1000 0.055s
im_proposals: 698/1000 0.055s
im_proposals: 699/1000 0.055s
im_proposals: 700/1000 0.055s
im_proposals: 701/1000 0.055s
im_proposals: 702/1000 0.055s
im_proposals: 703/1000 0.055s
im_proposals: 704/1000 0.055s
im_proposals: 705/1000 0.055s
im_proposals: 706/1000 0.055s
im_proposals: 707/1000 0.055s
im_proposals: 708/1000 0.055s
im_proposals: 709/1000 0.055s
im_proposals: 710/1000 0.055s
im_proposals: 711/1000 0.055s
im_proposals: 712/1000 0.055s
im_proposals: 713/1000 0.055s
im_proposals: 714/1000 0.055s
im_proposals: 715/1000 0.055s
im_proposals: 716/1000 0.055s
im_proposals: 717/1000 0.055s
im_proposals: 718/1000 0.055s
im_proposals: 719/1000 0.055s
im_proposals: 720/1000 0.055s
im_proposals: 721/1000 0.055s
im_proposals: 722/1000 0.055s
im_proposals: 723/1000 0.055s
im_proposals: 724/1000 0.055s
im_proposals: 725/1000 0.055s
im_proposals: 726/1000 0.055s
im_proposals: 727/1000 0.055s
im_proposals: 728/1000 0.055s
im_proposals: 729/1000 0.055s
im_proposals: 730/1000 0.055s
im_proposals: 731/1000 0.055s
im_proposals: 732/1000 0.055s
im_proposals: 733/1000 0.055s
im_proposals: 734/1000 0.055s
im_proposals: 735/1000 0.055s
im_proposals: 736/1000 0.055s
im_proposals: 737/1000 0.055s
im_proposals: 738/1000 0.055s
im_proposals: 739/1000 0.055s
im_proposals: 740/1000 0.055s
im_proposals: 741/1000 0.055s
im_proposals: 742/1000 0.055s
im_proposals: 743/1000 0.055s
im_proposals: 744/1000 0.055s
im_proposals: 745/1000 0.055s
im_proposals: 746/1000 0.055s
im_proposals: 747/1000 0.055s
im_proposals: 748/1000 0.055s
im_proposals: 749/1000 0.055s
im_proposals: 750/1000 0.055s
im_proposals: 751/1000 0.055s
im_proposals: 752/1000 0.055s
im_proposals: 753/1000 0.055s
im_proposals: 754/1000 0.055s
im_proposals: 755/1000 0.055s
im_proposals: 756/1000 0.055s
im_proposals: 757/1000 0.055s
im_proposals: 758/1000 0.055s
im_proposals: 759/1000 0.055s
im_proposals: 760/1000 0.055s
im_proposals: 761/1000 0.055s
im_proposals: 762/1000 0.055s
im_proposals: 763/1000 0.055s
im_proposals: 764/1000 0.055s
im_proposals: 765/1000 0.055s
im_proposals: 766/1000 0.055s
im_proposals: 767/1000 0.055s
im_proposals: 768/1000 0.055s
im_proposals: 769/1000 0.055s
im_proposals: 770/1000 0.055s
im_proposals: 771/1000 0.055s
im_proposals: 772/1000 0.055s
im_proposals: 773/1000 0.055s
im_proposals: 774/1000 0.055s
im_proposals: 775/1000 0.055s
im_proposals: 776/1000 0.055s
im_proposals: 777/1000 0.054s
im_proposals: 778/1000 0.054s
im_proposals: 779/1000 0.054s
im_proposals: 780/1000 0.054s
im_proposals: 781/1000 0.055s
im_proposals: 782/1000 0.055s
im_proposals: 783/1000 0.055s
im_proposals: 784/1000 0.055s
im_proposals: 785/1000 0.055s
im_proposals: 786/1000 0.055s
im_proposals: 787/1000 0.054s
im_proposals: 788/1000 0.054s
im_proposals: 789/1000 0.054s
im_proposals: 790/1000 0.054s
im_proposals: 791/1000 0.054s
im_proposals: 792/1000 0.054s
im_proposals: 793/1000 0.054s
im_proposals: 794/1000 0.054s
im_proposals: 795/1000 0.054s
im_proposals: 796/1000 0.054s
im_proposals: 797/1000 0.054s
im_proposals: 798/1000 0.054s
im_proposals: 799/1000 0.054s
im_proposals: 800/1000 0.054s
im_proposals: 801/1000 0.054s
im_proposals: 802/1000 0.054s
im_proposals: 803/1000 0.054s
im_proposals: 804/1000 0.054s
im_proposals: 805/1000 0.054s
im_proposals: 806/1000 0.054s
im_proposals: 807/1000 0.054s
im_proposals: 808/1000 0.054s
im_proposals: 809/1000 0.054s
im_proposals: 810/1000 0.054s
im_proposals: 811/1000 0.054s
im_proposals: 812/1000 0.054s
im_proposals: 813/1000 0.054s
im_proposals: 814/1000 0.054s
im_proposals: 815/1000 0.054s
im_proposals: 816/1000 0.054s
im_proposals: 817/1000 0.054s
im_proposals: 818/1000 0.054s
im_proposals: 819/1000 0.054s
im_proposals: 820/1000 0.054s
im_proposals: 821/1000 0.054s
im_proposals: 822/1000 0.054s
im_proposals: 823/1000 0.054s
im_proposals: 824/1000 0.054s
im_proposals: 825/1000 0.054s
im_proposals: 826/1000 0.054s
im_proposals: 827/1000 0.054s
im_proposals: 828/1000 0.054s
im_proposals: 829/1000 0.054s
im_proposals: 830/1000 0.054s
im_proposals: 831/1000 0.054s
im_proposals: 832/1000 0.054s
im_proposals: 833/1000 0.054s
im_proposals: 834/1000 0.054s
im_proposals: 835/1000 0.054s
im_proposals: 836/1000 0.054s
im_proposals: 837/1000 0.054s
im_proposals: 838/1000 0.054s
im_proposals: 839/1000 0.054s
im_proposals: 840/1000 0.054s
im_proposals: 841/1000 0.054s
im_proposals: 842/1000 0.054s
im_proposals: 843/1000 0.054s
im_proposals: 844/1000 0.054s
im_proposals: 845/1000 0.054s
im_proposals: 846/1000 0.054s
im_proposals: 847/1000 0.054s
im_proposals: 848/1000 0.054s
im_proposals: 849/1000 0.054s
im_proposals: 850/1000 0.054s
im_proposals: 851/1000 0.054s
im_proposals: 852/1000 0.054s
im_proposals: 853/1000 0.054s
im_proposals: 854/1000 0.054s
im_proposals: 855/1000 0.054s
im_proposals: 856/1000 0.054s
im_proposals: 857/1000 0.054s
im_proposals: 858/1000 0.054s
im_proposals: 859/1000 0.054s
im_proposals: 860/1000 0.054s
im_proposals: 861/1000 0.054s
im_proposals: 862/1000 0.054s
im_proposals: 863/1000 0.054s
im_proposals: 864/1000 0.054s
im_proposals: 865/1000 0.054s
im_proposals: 866/1000 0.054s
im_proposals: 867/1000 0.054s
im_proposals: 868/1000 0.054s
im_proposals: 869/1000 0.054s
im_proposals: 870/1000 0.054s
im_proposals: 871/1000 0.054s
im_proposals: 872/1000 0.054s
im_proposals: 873/1000 0.054s
im_proposals: 874/1000 0.054s
im_proposals: 875/1000 0.054s
im_proposals: 876/1000 0.054s
im_proposals: 877/1000 0.054s
im_proposals: 878/1000 0.054s
im_proposals: 879/1000 0.054s
im_proposals: 880/1000 0.054s
im_proposals: 881/1000 0.054s
im_proposals: 882/1000 0.054s
im_proposals: 883/1000 0.054s
im_proposals: 884/1000 0.054s
im_proposals: 885/1000 0.054s
im_proposals: 886/1000 0.054s
im_proposals: 887/1000 0.054s
im_proposals: 888/1000 0.054s
im_proposals: 889/1000 0.054s
im_proposals: 890/1000 0.054s
im_proposals: 891/1000 0.054s
im_proposals: 892/1000 0.054s
im_proposals: 893/1000 0.054s
im_proposals: 894/1000 0.054s
im_proposals: 895/1000 0.054s
im_proposals: 896/1000 0.054s
im_proposals: 897/1000 0.054s
im_proposals: 898/1000 0.054s
im_proposals: 899/1000 0.054s
im_proposals: 900/1000 0.054s
im_proposals: 901/1000 0.054s
im_proposals: 902/1000 0.054s
im_proposals: 903/1000 0.054s
im_proposals: 904/1000 0.054s
im_proposals: 905/1000 0.054s
im_proposals: 906/1000 0.054s
im_proposals: 907/1000 0.054s
im_proposals: 908/1000 0.054s
im_proposals: 909/1000 0.054s
im_proposals: 910/1000 0.054s
im_proposals: 911/1000 0.054s
im_proposals: 912/1000 0.054s
im_proposals: 913/1000 0.054s
im_proposals: 914/1000 0.054s
im_proposals: 915/1000 0.054s
im_proposals: 916/1000 0.054s
im_proposals: 917/1000 0.054s
im_proposals: 918/1000 0.054s
im_proposals: 919/1000 0.054s
im_proposals: 920/1000 0.054s
im_proposals: 921/1000 0.054s
im_proposals: 922/1000 0.054s
im_proposals: 923/1000 0.054s
im_proposals: 924/1000 0.054s
im_proposals: 925/1000 0.054s
im_proposals: 926/1000 0.054s
im_proposals: 927/1000 0.054s
im_proposals: 928/1000 0.054s
im_proposals: 929/1000 0.054s
im_proposals: 930/1000 0.054s
im_proposals: 931/1000 0.054s
im_proposals: 932/1000 0.054s
im_proposals: 933/1000 0.054s
im_proposals: 934/1000 0.054s
im_proposals: 935/1000 0.054s
im_proposals: 936/1000 0.054s
im_proposals: 937/1000 0.054s
im_proposals: 938/1000 0.054s
im_proposals: 939/1000 0.054s
im_proposals: 940/1000 0.054s
im_proposals: 941/1000 0.054s
im_proposals: 942/1000 0.054s
im_proposals: 943/1000 0.054s
im_proposals: 944/1000 0.054s
im_proposals: 945/1000 0.054s
im_proposals: 946/1000 0.054s
im_proposals: 947/1000 0.054s
im_proposals: 948/1000 0.054s
im_proposals: 949/1000 0.054s
im_proposals: 950/1000 0.054s
im_proposals: 951/1000 0.054s
im_proposals: 952/1000 0.054s
im_proposals: 953/1000 0.054s
im_proposals: 954/1000 0.054s
im_proposals: 955/1000 0.054s
im_proposals: 956/1000 0.054s
im_proposals: 957/1000 0.054s
im_proposals: 958/1000 0.054s
im_proposals: 959/1000 0.054s
im_proposals: 960/1000 0.054s
im_proposals: 961/1000 0.054s
im_proposals: 962/1000 0.054s
im_proposals: 963/1000 0.054s
im_proposals: 964/1000 0.054s
im_proposals: 965/1000 0.054s
im_proposals: 966/1000 0.054s
im_proposals: 967/1000 0.054s
im_proposals: 968/1000 0.054s
im_proposals: 969/1000 0.054s
im_proposals: 970/1000 0.054s
im_proposals: 971/1000 0.054s
im_proposals: 972/1000 0.054s
im_proposals: 973/1000 0.054s
im_proposals: 974/1000 0.054s
im_proposals: 975/1000 0.054s
im_proposals: 976/1000 0.054s
im_proposals: 977/1000 0.054s
im_proposals: 978/1000 0.054s
im_proposals: 979/1000 0.054s
im_proposals: 980/1000 0.054s
im_proposals: 981/1000 0.054s
im_proposals: 982/1000 0.054s
im_proposals: 983/1000 0.054s
im_proposals: 984/1000 0.054s
im_proposals: 985/1000 0.054s
im_proposals: 986/1000 0.054s
im_proposals: 987/1000 0.054s
im_proposals: 988/1000 0.054s
im_proposals: 989/1000 0.054s
im_proposals: 990/1000 0.054s
im_proposals: 991/1000 0.054s
im_proposals: 992/1000 0.054s
im_proposals: 993/1000 0.054s
im_proposals: 994/1000 0.054s
im_proposals: 995/1000 0.054s
im_proposals: 996/1000 0.054s
im_proposals: 997/1000 0.054s
im_proposals: 998/1000 0.054s
im_proposals: 999/1000 0.054s
im_proposals: 1000/1000 0.054s
Wrote RPN proposals to /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage1_iter_1000_proposals.pkl
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 Fast R-CNN using RPN proposals, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: data/imagenet_models/ZF.v2.caffemodel
RPN proposals: /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage1_iter_1000_proposals.pkl
Using config:
{'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.3,
           'BG_THRESH_LO': 0.01,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': False,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'rpn',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `voc_2007_trainval` for training
Set proposal method: rpn
Appending horizontally-flipped training examples...
done
Preparing training data...
voc_2007_trainval gt roidb loaded from /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/data/cache/voc_2007_trainval_gt_roidb.pkl
loading /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage1_iter_1000_proposals.pkl
done
Output will be saved to `/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval`
Computing bounding-box regression targets...
bbox target means:
[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  4.52162381e-03  -3.49549518e-03   1.53016884e-01  -5.21588186e-02]
 [ -7.70589060e-03   1.06561009e-03   3.47328174e-02   5.68748713e-02]
 [  6.03323732e-04   1.38506023e-03   4.33823202e-02   1.29994970e-02]
 [ -4.21943191e-03  -3.88401039e-03   5.01052883e-02  -1.60744594e-02]
 [  2.19350960e-03   2.13482225e-03  -7.74786415e-02   1.02947484e-01]
 [  1.71675419e-03   2.70409111e-03   4.40666686e-02  -4.72470272e-03]
 [  5.68499956e-04   2.88449775e-03   1.02170622e-01  -3.99783468e-02]
 [ -4.56120889e-03  -2.88687183e-03   1.02244470e-01   5.05907133e-02]
 [  8.07399793e-05   4.53521271e-03  -1.70698958e-02   3.41535545e-02]
 [ -1.65841363e-03  -1.07679679e-03   4.83665386e-02   3.06043117e-02]
 [  2.42373135e-03   1.30948803e-02   1.36651876e-01  -1.69334054e-02]
 [  1.96123555e-03   5.96704622e-03   7.86257784e-02   5.90099097e-02]
 [ -1.90691334e-03   7.62052685e-03   3.26174656e-02   5.51953354e-02]
 [  2.09807102e-03   1.18674298e-03   9.26878709e-02   2.94967151e-02]
 [  1.29905037e-03   7.46248508e-03  -3.47917477e-03   8.30293935e-02]
 [ -6.35350969e-03  -7.42011067e-03   1.55752660e-02   5.11194038e-02]
 [ -1.79522726e-03  -8.08349561e-03   8.27693647e-03  -4.32776721e-03]
 [  2.72801315e-06   5.47288711e-03   1.20197580e-01   1.90164799e-02]
 [  2.31086797e-03   1.27190356e-03   8.07032625e-02  -4.44459355e-03]
 [  3.12783994e-04  -1.61784581e-04  -3.41037987e-03  -4.48862585e-03]]
[-0.00040538  0.00148886  0.05209918  0.02209535]
bbox target stdevs:
[[ 0.          0.          0.          0.        ]
 [ 0.14059923  0.12090034  0.22855427  0.24062598]
 [ 0.1351013   0.13027408  0.25183305  0.24008919]
 [ 0.13592149  0.12707189  0.24977232  0.23891551]
 [ 0.13421344  0.12859007  0.24328445  0.24551671]
 [ 0.1107429   0.1359506   0.22429187  0.22207276]
 [ 0.13207942  0.13502903  0.25583079  0.24956172]
 [ 0.13539192  0.1269055   0.22836176  0.24046995]
 [ 0.14408826  0.13087096  0.25188691  0.23527438]
 [ 0.13161565  0.12938597  0.25328506  0.23706509]
 [ 0.1363514   0.13099748  0.24907299  0.24558857]
 [ 0.1483556   0.12925864  0.24569232  0.24883594]
 [ 0.1402481   0.13492325  0.24926366  0.23545869]
 [ 0.13588047  0.13795287  0.25082172  0.24649122]
 [ 0.14050555  0.13324967  0.24925755  0.24988367]
 [ 0.13183863  0.13458314  0.25614745  0.2344414 ]
 [ 0.12695556  0.130496    0.25267329  0.25120708]
 [ 0.12360238  0.12054114  0.22454272  0.22516734]
 [ 0.1440809   0.13764563  0.24171097  0.25221505]
 [ 0.13874328  0.1309196   0.24952657  0.24854379]
 [ 0.12306304  0.12961774  0.24696148  0.25585455]]
[ 0.13446893  0.13075818  0.24513856  0.24216393]
Normalizing targets
done
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0512 06:28:44.211282  9601 solver.cpp:54] Initializing solver from parameters: 
train_net: "models/ZF/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt"
base_lr: 0.001
display: 20
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 30000
snapshot: 0
snapshot_prefix: "zf_fast_rcnn"
average_loss: 100
I0512 06:28:44.211333  9601 solver.cpp:86] Creating training net from train_net file: models/ZF/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt
I0512 06:28:44.212116  9601 net.cpp:50] Initializing net from parameters: 
name: "ZF"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "rois"
  top: "labels"
  top: "bbox_targets"
  top: "bbox_inside_weights"
  top: "bbox_outside_weights"
  python_param {
    module: "roi_data_layer.layer"
    layer: "RoIDataLayer"
    param_str: "\'num_classes\': 21"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "roi_pool_conv5"
  type: "ROIPooling"
  bottom: "conv5"
  bottom: "rois"
  top: "roi_pool_conv5"
  roi_pooling_param {
    pooled_h: 6
    pooled_w: 6
    spatial_scale: 0.0625
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "roi_pool_conv5"
  top: "fc6"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
    scale_train: false
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
    scale_train: false
  }
}
layer {
  name: "cls_score"
  type: "InnerProduct"
  bottom: "fc7"
  top: "cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 21
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bbox_pred"
  type: "InnerProduct"
  bottom: "fc7"
  top: "bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 84
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "cls_score"
  bottom: "labels"
  top: "cls_loss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalize: true
  }
}
layer {
  name: "loss_bbox"
  type: "SmoothL1Loss"
  bottom: "bbox_pred"
  bottom: "bbox_targets"
  bottom: "bbox_inside_weights"
  bottom: "bbox_outside_weights"
  top: "bbox_loss"
  loss_weight: 1
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "silence_rpn_cls_score"
  type: "Silence"
  bottom: "rpn_cls_score"
}
layer {
  name: "silence_rpn_bbox_pred"
  type: "Silence"
  bottom: "rpn_bbox_pred"
}
I0512 06:28:44.212268  9601 layer_factory.hpp:76] Creating layer data
I0512 06:28:44.213034  9601 net.cpp:110] Creating Layer data
I0512 06:28:44.213060  9601 net.cpp:433] data -> data
I0512 06:28:44.213078  9601 net.cpp:433] data -> rois
I0512 06:28:44.213083  9601 net.cpp:433] data -> labels
I0512 06:28:44.213089  9601 net.cpp:433] data -> bbox_targets
I0512 06:28:44.213094  9601 net.cpp:433] data -> bbox_inside_weights
I0512 06:28:44.213099  9601 net.cpp:433] data -> bbox_outside_weights
RoiDataLayer: name_to_top: {'bbox_inside_weights': 4, 'labels': 2, 'rois': 1, 'bbox_targets': 3, 'bbox_outside_weights': 5, 'data': 0}
I0512 06:28:44.213958  9601 net.cpp:155] Setting up data
I0512 06:28:44.213989  9601 net.cpp:163] Top shape: 2 3 600 1000 (3600000)
I0512 06:28:44.214004  9601 net.cpp:163] Top shape: 1 5 (5)
I0512 06:28:44.214006  9601 net.cpp:163] Top shape: 1 (1)
I0512 06:28:44.214010  9601 net.cpp:163] Top shape: 1 84 (84)
I0512 06:28:44.214013  9601 net.cpp:163] Top shape: 1 84 (84)
I0512 06:28:44.214016  9601 net.cpp:163] Top shape: 1 84 (84)
I0512 06:28:44.214020  9601 layer_factory.hpp:76] Creating layer conv1
I0512 06:28:44.214040  9601 net.cpp:110] Creating Layer conv1
I0512 06:28:44.214045  9601 net.cpp:477] conv1 <- data
I0512 06:28:44.214051  9601 net.cpp:433] conv1 -> conv1
I0512 06:28:44.230187  9601 net.cpp:155] Setting up conv1
I0512 06:28:44.230204  9601 net.cpp:163] Top shape: 2 96 300 500 (28800000)
I0512 06:28:44.230218  9601 layer_factory.hpp:76] Creating layer relu1
I0512 06:28:44.230226  9601 net.cpp:110] Creating Layer relu1
I0512 06:28:44.230229  9601 net.cpp:477] relu1 <- conv1
I0512 06:28:44.230233  9601 net.cpp:419] relu1 -> conv1 (in-place)
I0512 06:28:44.230240  9601 net.cpp:155] Setting up relu1
I0512 06:28:44.230244  9601 net.cpp:163] Top shape: 2 96 300 500 (28800000)
I0512 06:28:44.230247  9601 layer_factory.hpp:76] Creating layer norm1
I0512 06:28:44.230253  9601 net.cpp:110] Creating Layer norm1
I0512 06:28:44.230257  9601 net.cpp:477] norm1 <- conv1
I0512 06:28:44.230262  9601 net.cpp:433] norm1 -> norm1
I0512 06:28:44.230290  9601 net.cpp:155] Setting up norm1
I0512 06:28:44.230295  9601 net.cpp:163] Top shape: 2 96 300 500 (28800000)
I0512 06:28:44.230298  9601 layer_factory.hpp:76] Creating layer pool1
I0512 06:28:44.230304  9601 net.cpp:110] Creating Layer pool1
I0512 06:28:44.230306  9601 net.cpp:477] pool1 <- norm1
I0512 06:28:44.230309  9601 net.cpp:433] pool1 -> pool1
I0512 06:28:44.230315  9601 net.cpp:155] Setting up pool1
I0512 06:28:44.230319  9601 net.cpp:163] Top shape: 2 96 151 251 (7276992)
I0512 06:28:44.230321  9601 layer_factory.hpp:76] Creating layer conv2
I0512 06:28:44.230326  9601 net.cpp:110] Creating Layer conv2
I0512 06:28:44.230329  9601 net.cpp:477] conv2 <- pool1
I0512 06:28:44.230332  9601 net.cpp:433] conv2 -> conv2
I0512 06:28:44.231860  9601 net.cpp:155] Setting up conv2
I0512 06:28:44.231871  9601 net.cpp:163] Top shape: 2 256 76 126 (4902912)
I0512 06:28:44.231879  9601 layer_factory.hpp:76] Creating layer relu2
I0512 06:28:44.231884  9601 net.cpp:110] Creating Layer relu2
I0512 06:28:44.231887  9601 net.cpp:477] relu2 <- conv2
I0512 06:28:44.231890  9601 net.cpp:419] relu2 -> conv2 (in-place)
I0512 06:28:44.231895  9601 net.cpp:155] Setting up relu2
I0512 06:28:44.231899  9601 net.cpp:163] Top shape: 2 256 76 126 (4902912)
I0512 06:28:44.231901  9601 layer_factory.hpp:76] Creating layer norm2
I0512 06:28:44.231905  9601 net.cpp:110] Creating Layer norm2
I0512 06:28:44.231909  9601 net.cpp:477] norm2 <- conv2
I0512 06:28:44.231911  9601 net.cpp:433] norm2 -> norm2
I0512 06:28:44.231925  9601 net.cpp:155] Setting up norm2
I0512 06:28:44.231930  9601 net.cpp:163] Top shape: 2 256 76 126 (4902912)
I0512 06:28:44.231932  9601 layer_factory.hpp:76] Creating layer pool2
I0512 06:28:44.231937  9601 net.cpp:110] Creating Layer pool2
I0512 06:28:44.231940  9601 net.cpp:477] pool2 <- norm2
I0512 06:28:44.231945  9601 net.cpp:433] pool2 -> pool2
I0512 06:28:44.231950  9601 net.cpp:155] Setting up pool2
I0512 06:28:44.231956  9601 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:28:44.231961  9601 layer_factory.hpp:76] Creating layer conv3
I0512 06:28:44.231967  9601 net.cpp:110] Creating Layer conv3
I0512 06:28:44.231969  9601 net.cpp:477] conv3 <- pool2
I0512 06:28:44.231973  9601 net.cpp:433] conv3 -> conv3
I0512 06:28:44.235101  9601 net.cpp:155] Setting up conv3
I0512 06:28:44.235112  9601 net.cpp:163] Top shape: 2 384 39 64 (1916928)
I0512 06:28:44.235121  9601 layer_factory.hpp:76] Creating layer relu3
I0512 06:28:44.235126  9601 net.cpp:110] Creating Layer relu3
I0512 06:28:44.235129  9601 net.cpp:477] relu3 <- conv3
I0512 06:28:44.235134  9601 net.cpp:419] relu3 -> conv3 (in-place)
I0512 06:28:44.235141  9601 net.cpp:155] Setting up relu3
I0512 06:28:44.235143  9601 net.cpp:163] Top shape: 2 384 39 64 (1916928)
I0512 06:28:44.235146  9601 layer_factory.hpp:76] Creating layer conv4
I0512 06:28:44.235151  9601 net.cpp:110] Creating Layer conv4
I0512 06:28:44.235153  9601 net.cpp:477] conv4 <- conv3
I0512 06:28:44.235158  9601 net.cpp:433] conv4 -> conv4
I0512 06:28:44.238272  9601 net.cpp:155] Setting up conv4
I0512 06:28:44.238282  9601 net.cpp:163] Top shape: 2 384 39 64 (1916928)
I0512 06:28:44.238288  9601 layer_factory.hpp:76] Creating layer relu4
I0512 06:28:44.238293  9601 net.cpp:110] Creating Layer relu4
I0512 06:28:44.238296  9601 net.cpp:477] relu4 <- conv4
I0512 06:28:44.238301  9601 net.cpp:419] relu4 -> conv4 (in-place)
I0512 06:28:44.238306  9601 net.cpp:155] Setting up relu4
I0512 06:28:44.238309  9601 net.cpp:163] Top shape: 2 384 39 64 (1916928)
I0512 06:28:44.238312  9601 layer_factory.hpp:76] Creating layer conv5
I0512 06:28:44.238317  9601 net.cpp:110] Creating Layer conv5
I0512 06:28:44.238319  9601 net.cpp:477] conv5 <- conv4
I0512 06:28:44.238323  9601 net.cpp:433] conv5 -> conv5
I0512 06:28:44.240452  9601 net.cpp:155] Setting up conv5
I0512 06:28:44.240463  9601 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:28:44.240470  9601 layer_factory.hpp:76] Creating layer relu5
I0512 06:28:44.240475  9601 net.cpp:110] Creating Layer relu5
I0512 06:28:44.240478  9601 net.cpp:477] relu5 <- conv5
I0512 06:28:44.240484  9601 net.cpp:419] relu5 -> conv5 (in-place)
I0512 06:28:44.240489  9601 net.cpp:155] Setting up relu5
I0512 06:28:44.240494  9601 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:28:44.240495  9601 layer_factory.hpp:76] Creating layer conv5_relu5_0_split
I0512 06:28:44.240504  9601 net.cpp:110] Creating Layer conv5_relu5_0_split
I0512 06:28:44.240505  9601 net.cpp:477] conv5_relu5_0_split <- conv5
I0512 06:28:44.240509  9601 net.cpp:433] conv5_relu5_0_split -> conv5_relu5_0_split_0
I0512 06:28:44.240514  9601 net.cpp:433] conv5_relu5_0_split -> conv5_relu5_0_split_1
I0512 06:28:44.240522  9601 net.cpp:155] Setting up conv5_relu5_0_split
I0512 06:28:44.240525  9601 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:28:44.240530  9601 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:28:44.240531  9601 layer_factory.hpp:76] Creating layer roi_pool_conv5
I0512 06:28:44.240540  9601 net.cpp:110] Creating Layer roi_pool_conv5
I0512 06:28:44.240542  9601 net.cpp:477] roi_pool_conv5 <- conv5_relu5_0_split_0
I0512 06:28:44.240545  9601 net.cpp:477] roi_pool_conv5 <- rois
I0512 06:28:44.240551  9601 net.cpp:433] roi_pool_conv5 -> roi_pool_conv5
I0512 06:28:44.240556  9601 roi_pooling_layer.cpp:30] Spatial scale: 0.0625
I0512 06:28:44.240571  9601 net.cpp:155] Setting up roi_pool_conv5
I0512 06:28:44.240574  9601 net.cpp:163] Top shape: 1 256 6 6 (9216)
I0512 06:28:44.240576  9601 layer_factory.hpp:76] Creating layer fc6
I0512 06:28:44.240583  9601 net.cpp:110] Creating Layer fc6
I0512 06:28:44.240586  9601 net.cpp:477] fc6 <- roi_pool_conv5
I0512 06:28:44.240591  9601 net.cpp:433] fc6 -> fc6
I0512 06:28:44.323123  9601 net.cpp:155] Setting up fc6
I0512 06:28:44.323158  9601 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:28:44.323170  9601 layer_factory.hpp:76] Creating layer relu6
I0512 06:28:44.323182  9601 net.cpp:110] Creating Layer relu6
I0512 06:28:44.323187  9601 net.cpp:477] relu6 <- fc6
I0512 06:28:44.323194  9601 net.cpp:419] relu6 -> fc6 (in-place)
I0512 06:28:44.323202  9601 net.cpp:155] Setting up relu6
I0512 06:28:44.323205  9601 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:28:44.323209  9601 layer_factory.hpp:76] Creating layer drop6
I0512 06:28:44.323221  9601 net.cpp:110] Creating Layer drop6
I0512 06:28:44.323225  9601 net.cpp:477] drop6 <- fc6
I0512 06:28:44.323228  9601 net.cpp:419] drop6 -> fc6 (in-place)
I0512 06:28:44.323235  9601 net.cpp:155] Setting up drop6
I0512 06:28:44.323238  9601 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:28:44.323241  9601 layer_factory.hpp:76] Creating layer fc7
I0512 06:28:44.323247  9601 net.cpp:110] Creating Layer fc7
I0512 06:28:44.323249  9601 net.cpp:477] fc7 <- fc6
I0512 06:28:44.323254  9601 net.cpp:433] fc7 -> fc7
I0512 06:28:44.360462  9601 net.cpp:155] Setting up fc7
I0512 06:28:44.360496  9601 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:28:44.360507  9601 layer_factory.hpp:76] Creating layer relu7
I0512 06:28:44.360515  9601 net.cpp:110] Creating Layer relu7
I0512 06:28:44.360522  9601 net.cpp:477] relu7 <- fc7
I0512 06:28:44.360527  9601 net.cpp:419] relu7 -> fc7 (in-place)
I0512 06:28:44.360534  9601 net.cpp:155] Setting up relu7
I0512 06:28:44.360538  9601 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:28:44.360540  9601 layer_factory.hpp:76] Creating layer drop7
I0512 06:28:44.360548  9601 net.cpp:110] Creating Layer drop7
I0512 06:28:44.360551  9601 net.cpp:477] drop7 <- fc7
I0512 06:28:44.360554  9601 net.cpp:419] drop7 -> fc7 (in-place)
I0512 06:28:44.360561  9601 net.cpp:155] Setting up drop7
I0512 06:28:44.360564  9601 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:28:44.360566  9601 layer_factory.hpp:76] Creating layer fc7_drop7_0_split
I0512 06:28:44.360571  9601 net.cpp:110] Creating Layer fc7_drop7_0_split
I0512 06:28:44.360574  9601 net.cpp:477] fc7_drop7_0_split <- fc7
I0512 06:28:44.360577  9601 net.cpp:433] fc7_drop7_0_split -> fc7_drop7_0_split_0
I0512 06:28:44.360584  9601 net.cpp:433] fc7_drop7_0_split -> fc7_drop7_0_split_1
I0512 06:28:44.360589  9601 net.cpp:155] Setting up fc7_drop7_0_split
I0512 06:28:44.360597  9601 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:28:44.360600  9601 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:28:44.360604  9601 layer_factory.hpp:76] Creating layer cls_score
I0512 06:28:44.360610  9601 net.cpp:110] Creating Layer cls_score
I0512 06:28:44.360613  9601 net.cpp:477] cls_score <- fc7_drop7_0_split_0
I0512 06:28:44.360620  9601 net.cpp:433] cls_score -> cls_score
I0512 06:28:44.362990  9601 net.cpp:155] Setting up cls_score
I0512 06:28:44.362998  9601 net.cpp:163] Top shape: 1 21 (21)
I0512 06:28:44.363003  9601 layer_factory.hpp:76] Creating layer bbox_pred
I0512 06:28:44.363010  9601 net.cpp:110] Creating Layer bbox_pred
I0512 06:28:44.363013  9601 net.cpp:477] bbox_pred <- fc7_drop7_0_split_1
I0512 06:28:44.363019  9601 net.cpp:433] bbox_pred -> bbox_pred
I0512 06:28:44.372948  9601 net.cpp:155] Setting up bbox_pred
I0512 06:28:44.372959  9601 net.cpp:163] Top shape: 1 84 (84)
I0512 06:28:44.372969  9601 layer_factory.hpp:76] Creating layer loss_cls
I0512 06:28:44.372980  9601 net.cpp:110] Creating Layer loss_cls
I0512 06:28:44.372984  9601 net.cpp:477] loss_cls <- cls_score
I0512 06:28:44.372990  9601 net.cpp:477] loss_cls <- labels
I0512 06:28:44.372995  9601 net.cpp:433] loss_cls -> cls_loss
I0512 06:28:44.373005  9601 layer_factory.hpp:76] Creating layer loss_cls
I0512 06:28:44.373050  9601 net.cpp:155] Setting up loss_cls
I0512 06:28:44.373056  9601 net.cpp:163] Top shape: (1)
I0512 06:28:44.373059  9601 net.cpp:168]     with loss weight 1
I0512 06:28:44.373070  9601 layer_factory.hpp:76] Creating layer loss_bbox
I0512 06:28:44.373077  9601 net.cpp:110] Creating Layer loss_bbox
I0512 06:28:44.373080  9601 net.cpp:477] loss_bbox <- bbox_pred
I0512 06:28:44.373083  9601 net.cpp:477] loss_bbox <- bbox_targets
I0512 06:28:44.373086  9601 net.cpp:477] loss_bbox <- bbox_inside_weights
I0512 06:28:44.373090  9601 net.cpp:477] loss_bbox <- bbox_outside_weights
I0512 06:28:44.373093  9601 net.cpp:433] loss_bbox -> bbox_loss
I0512 06:28:44.373126  9601 net.cpp:155] Setting up loss_bbox
I0512 06:28:44.373152  9601 net.cpp:163] Top shape: (1)
I0512 06:28:44.373155  9601 net.cpp:168]     with loss weight 1
I0512 06:28:44.373159  9601 layer_factory.hpp:76] Creating layer rpn_conv1
I0512 06:28:44.373167  9601 net.cpp:110] Creating Layer rpn_conv1
I0512 06:28:44.373178  9601 net.cpp:477] rpn_conv1 <- conv5_relu5_0_split_1
I0512 06:28:44.373181  9601 net.cpp:433] rpn_conv1 -> rpn_conv1
I0512 06:28:44.389880  9601 net.cpp:155] Setting up rpn_conv1
I0512 06:28:44.389892  9601 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:28:44.389899  9601 layer_factory.hpp:76] Creating layer rpn_relu1
I0512 06:28:44.389904  9601 net.cpp:110] Creating Layer rpn_relu1
I0512 06:28:44.389906  9601 net.cpp:477] rpn_relu1 <- rpn_conv1
I0512 06:28:44.389910  9601 net.cpp:419] rpn_relu1 -> rpn_conv1 (in-place)
I0512 06:28:44.389915  9601 net.cpp:155] Setting up rpn_relu1
I0512 06:28:44.389919  9601 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:28:44.389922  9601 layer_factory.hpp:76] Creating layer rpn_conv1_rpn_relu1_0_split
I0512 06:28:44.389926  9601 net.cpp:110] Creating Layer rpn_conv1_rpn_relu1_0_split
I0512 06:28:44.389928  9601 net.cpp:477] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I0512 06:28:44.389933  9601 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I0512 06:28:44.389938  9601 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I0512 06:28:44.389942  9601 net.cpp:155] Setting up rpn_conv1_rpn_relu1_0_split
I0512 06:28:44.389946  9601 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:28:44.389950  9601 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:28:44.389951  9601 layer_factory.hpp:76] Creating layer rpn_cls_score
I0512 06:28:44.389958  9601 net.cpp:110] Creating Layer rpn_cls_score
I0512 06:28:44.389961  9601 net.cpp:477] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I0512 06:28:44.389964  9601 net.cpp:433] rpn_cls_score -> rpn_cls_score
I0512 06:28:44.390136  9601 net.cpp:155] Setting up rpn_cls_score
I0512 06:28:44.390144  9601 net.cpp:163] Top shape: 2 18 39 64 (89856)
I0512 06:28:44.390149  9601 layer_factory.hpp:76] Creating layer rpn_bbox_pred
I0512 06:28:44.390158  9601 net.cpp:110] Creating Layer rpn_bbox_pred
I0512 06:28:44.390172  9601 net.cpp:477] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I0512 06:28:44.390177  9601 net.cpp:433] rpn_bbox_pred -> rpn_bbox_pred
I0512 06:28:44.390470  9601 net.cpp:155] Setting up rpn_bbox_pred
I0512 06:28:44.390475  9601 net.cpp:163] Top shape: 2 36 39 64 (179712)
I0512 06:28:44.390480  9601 layer_factory.hpp:76] Creating layer silence_rpn_cls_score
I0512 06:28:44.390487  9601 net.cpp:110] Creating Layer silence_rpn_cls_score
I0512 06:28:44.390491  9601 net.cpp:477] silence_rpn_cls_score <- rpn_cls_score
I0512 06:28:44.390494  9601 net.cpp:155] Setting up silence_rpn_cls_score
I0512 06:28:44.390497  9601 layer_factory.hpp:76] Creating layer silence_rpn_bbox_pred
I0512 06:28:44.390501  9601 net.cpp:110] Creating Layer silence_rpn_bbox_pred
I0512 06:28:44.390503  9601 net.cpp:477] silence_rpn_bbox_pred <- rpn_bbox_pred
I0512 06:28:44.390506  9601 net.cpp:155] Setting up silence_rpn_bbox_pred
I0512 06:28:44.390509  9601 net.cpp:240] silence_rpn_bbox_pred does not need backward computation.
I0512 06:28:44.390511  9601 net.cpp:240] silence_rpn_cls_score does not need backward computation.
I0512 06:28:44.390514  9601 net.cpp:240] rpn_bbox_pred does not need backward computation.
I0512 06:28:44.390516  9601 net.cpp:240] rpn_cls_score does not need backward computation.
I0512 06:28:44.390518  9601 net.cpp:240] rpn_conv1_rpn_relu1_0_split does not need backward computation.
I0512 06:28:44.390522  9601 net.cpp:240] rpn_relu1 does not need backward computation.
I0512 06:28:44.390523  9601 net.cpp:240] rpn_conv1 does not need backward computation.
I0512 06:28:44.390527  9601 net.cpp:236] loss_bbox needs backward computation.
I0512 06:28:44.390529  9601 net.cpp:236] loss_cls needs backward computation.
I0512 06:28:44.390532  9601 net.cpp:236] bbox_pred needs backward computation.
I0512 06:28:44.390535  9601 net.cpp:236] cls_score needs backward computation.
I0512 06:28:44.390537  9601 net.cpp:236] fc7_drop7_0_split needs backward computation.
I0512 06:28:44.390540  9601 net.cpp:236] drop7 needs backward computation.
I0512 06:28:44.390542  9601 net.cpp:236] relu7 needs backward computation.
I0512 06:28:44.390544  9601 net.cpp:236] fc7 needs backward computation.
I0512 06:28:44.390547  9601 net.cpp:236] drop6 needs backward computation.
I0512 06:28:44.390549  9601 net.cpp:236] relu6 needs backward computation.
I0512 06:28:44.390552  9601 net.cpp:236] fc6 needs backward computation.
I0512 06:28:44.390554  9601 net.cpp:236] roi_pool_conv5 needs backward computation.
I0512 06:28:44.390558  9601 net.cpp:236] conv5_relu5_0_split needs backward computation.
I0512 06:28:44.390560  9601 net.cpp:236] relu5 needs backward computation.
I0512 06:28:44.390563  9601 net.cpp:236] conv5 needs backward computation.
I0512 06:28:44.390565  9601 net.cpp:236] relu4 needs backward computation.
I0512 06:28:44.390568  9601 net.cpp:236] conv4 needs backward computation.
I0512 06:28:44.390570  9601 net.cpp:236] relu3 needs backward computation.
I0512 06:28:44.390573  9601 net.cpp:236] conv3 needs backward computation.
I0512 06:28:44.390575  9601 net.cpp:236] pool2 needs backward computation.
I0512 06:28:44.390578  9601 net.cpp:236] norm2 needs backward computation.
I0512 06:28:44.390580  9601 net.cpp:236] relu2 needs backward computation.
I0512 06:28:44.390583  9601 net.cpp:236] conv2 needs backward computation.
I0512 06:28:44.390585  9601 net.cpp:236] pool1 needs backward computation.
I0512 06:28:44.390588  9601 net.cpp:236] norm1 needs backward computation.
I0512 06:28:44.390591  9601 net.cpp:236] relu1 needs backward computation.
I0512 06:28:44.390594  9601 net.cpp:236] conv1 needs backward computation.
I0512 06:28:44.390597  9601 net.cpp:240] data does not need backward computation.
I0512 06:28:44.390600  9601 net.cpp:283] This network produces output bbox_loss
I0512 06:28:44.390602  9601 net.cpp:283] This network produces output cls_loss
I0512 06:28:44.390625  9601 net.cpp:297] Network initialization done.
I0512 06:28:44.390630  9601 net.cpp:298] Memory required for data: 525867700
I0512 06:28:44.390739  9601 solver.cpp:65] Solver scaffolding done.
Loading pretrained model weights from data/imagenet_models/ZF.v2.caffemodel
Solving...
I0512 06:28:44.956740  9601 solver.cpp:242] Iteration 0, loss = 3.61075
I0512 06:28:44.956785  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.387194 (* 1 = 0.387194 loss)
I0512 06:28:44.956792  9601 solver.cpp:258]     Train net output #1: cls_loss = 3.22356 (* 1 = 3.22356 loss)
I0512 06:28:44.956806  9601 solver.cpp:571] Iteration 0, lr = 0.001
I0512 06:28:47.620060  9601 solver.cpp:242] Iteration 20, loss = 1.57418
I0512 06:28:47.620106  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.455682 (* 1 = 0.455682 loss)
I0512 06:28:47.620113  9601 solver.cpp:258]     Train net output #1: cls_loss = 1.1185 (* 1 = 1.1185 loss)
I0512 06:28:47.620120  9601 solver.cpp:571] Iteration 20, lr = 0.001
I0512 06:28:50.338600  9601 solver.cpp:242] Iteration 40, loss = 2.04421
I0512 06:28:50.338646  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.458429 (* 1 = 0.458429 loss)
I0512 06:28:50.338654  9601 solver.cpp:258]     Train net output #1: cls_loss = 1.58578 (* 1 = 1.58578 loss)
I0512 06:28:50.338659  9601 solver.cpp:571] Iteration 40, lr = 0.001
I0512 06:28:53.046452  9601 solver.cpp:242] Iteration 60, loss = 1.17116
I0512 06:28:53.046500  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.2918 (* 1 = 0.2918 loss)
I0512 06:28:53.046507  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.879357 (* 1 = 0.879357 loss)
I0512 06:28:53.046514  9601 solver.cpp:571] Iteration 60, lr = 0.001
I0512 06:28:55.795424  9601 solver.cpp:242] Iteration 80, loss = 1.33706
I0512 06:28:55.795469  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.443149 (* 1 = 0.443149 loss)
I0512 06:28:55.795475  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.893916 (* 1 = 0.893916 loss)
I0512 06:28:55.795483  9601 solver.cpp:571] Iteration 80, lr = 0.001
I0512 06:28:58.516356  9601 solver.cpp:242] Iteration 100, loss = 1.4513
I0512 06:28:58.516398  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.473057 (* 1 = 0.473057 loss)
I0512 06:28:58.516405  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.978241 (* 1 = 0.978241 loss)
I0512 06:28:58.516412  9601 solver.cpp:571] Iteration 100, lr = 0.001
I0512 06:29:01.168704  9601 solver.cpp:242] Iteration 120, loss = 1.49513
I0512 06:29:01.168746  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.376643 (* 1 = 0.376643 loss)
I0512 06:29:01.168752  9601 solver.cpp:258]     Train net output #1: cls_loss = 1.11848 (* 1 = 1.11848 loss)
I0512 06:29:01.168758  9601 solver.cpp:571] Iteration 120, lr = 0.001
I0512 06:29:03.880641  9601 solver.cpp:242] Iteration 140, loss = 1.44683
I0512 06:29:03.880682  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.420215 (* 1 = 0.420215 loss)
I0512 06:29:03.880688  9601 solver.cpp:258]     Train net output #1: cls_loss = 1.02662 (* 1 = 1.02662 loss)
I0512 06:29:03.880694  9601 solver.cpp:571] Iteration 140, lr = 0.001
I0512 06:29:06.604697  9601 solver.cpp:242] Iteration 160, loss = 0.88676
I0512 06:29:06.604743  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.276752 (* 1 = 0.276752 loss)
I0512 06:29:06.604750  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.610008 (* 1 = 0.610008 loss)
I0512 06:29:06.604756  9601 solver.cpp:571] Iteration 160, lr = 0.001
I0512 06:29:09.351449  9601 solver.cpp:242] Iteration 180, loss = 1.49515
I0512 06:29:09.351490  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.432402 (* 1 = 0.432402 loss)
I0512 06:29:09.351496  9601 solver.cpp:258]     Train net output #1: cls_loss = 1.06275 (* 1 = 1.06275 loss)
I0512 06:29:09.351502  9601 solver.cpp:571] Iteration 180, lr = 0.001
speed: 0.136s / iter
I0512 06:29:12.193014  9601 solver.cpp:242] Iteration 200, loss = 1.13023
I0512 06:29:12.193058  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.416193 (* 1 = 0.416193 loss)
I0512 06:29:12.193064  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.714033 (* 1 = 0.714033 loss)
I0512 06:29:12.193070  9601 solver.cpp:571] Iteration 200, lr = 0.001
I0512 06:29:15.035187  9601 solver.cpp:242] Iteration 220, loss = 1.3561
I0512 06:29:15.035228  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.445176 (* 1 = 0.445176 loss)
I0512 06:29:15.035234  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.910925 (* 1 = 0.910925 loss)
I0512 06:29:15.035240  9601 solver.cpp:571] Iteration 220, lr = 0.001
I0512 06:29:17.851150  9601 solver.cpp:242] Iteration 240, loss = 1.44328
I0512 06:29:17.851191  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.51109 (* 1 = 0.51109 loss)
I0512 06:29:17.851197  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.93219 (* 1 = 0.93219 loss)
I0512 06:29:17.851203  9601 solver.cpp:571] Iteration 240, lr = 0.001
I0512 06:29:20.703091  9601 solver.cpp:242] Iteration 260, loss = 1.37859
I0512 06:29:20.703152  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.601346 (* 1 = 0.601346 loss)
I0512 06:29:20.703160  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.777244 (* 1 = 0.777244 loss)
I0512 06:29:20.703166  9601 solver.cpp:571] Iteration 260, lr = 0.001
I0512 06:29:23.569932  9601 solver.cpp:242] Iteration 280, loss = 0.80796
I0512 06:29:23.569974  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.276627 (* 1 = 0.276627 loss)
I0512 06:29:23.569980  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.531334 (* 1 = 0.531334 loss)
I0512 06:29:23.569990  9601 solver.cpp:571] Iteration 280, lr = 0.001
I0512 06:29:26.420548  9601 solver.cpp:242] Iteration 300, loss = 0.992871
I0512 06:29:26.420589  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.394151 (* 1 = 0.394151 loss)
I0512 06:29:26.420595  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.59872 (* 1 = 0.59872 loss)
I0512 06:29:26.420601  9601 solver.cpp:571] Iteration 300, lr = 0.001
I0512 06:29:29.142467  9601 solver.cpp:242] Iteration 320, loss = 1.01133
I0512 06:29:29.142508  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.3335 (* 1 = 0.3335 loss)
I0512 06:29:29.142514  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.677829 (* 1 = 0.677829 loss)
I0512 06:29:29.142520  9601 solver.cpp:571] Iteration 320, lr = 0.001
I0512 06:29:31.957562  9601 solver.cpp:242] Iteration 340, loss = 0.975425
I0512 06:29:31.957603  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.361214 (* 1 = 0.361214 loss)
I0512 06:29:31.957609  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.614211 (* 1 = 0.614211 loss)
I0512 06:29:31.957615  9601 solver.cpp:571] Iteration 340, lr = 0.001
I0512 06:29:34.659966  9601 solver.cpp:242] Iteration 360, loss = 1.72274
I0512 06:29:34.660009  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.450394 (* 1 = 0.450394 loss)
I0512 06:29:34.660017  9601 solver.cpp:258]     Train net output #1: cls_loss = 1.27234 (* 1 = 1.27234 loss)
I0512 06:29:34.660022  9601 solver.cpp:571] Iteration 360, lr = 0.001
I0512 06:29:37.428470  9601 solver.cpp:242] Iteration 380, loss = 0.7146
I0512 06:29:37.428509  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.228952 (* 1 = 0.228952 loss)
I0512 06:29:37.428515  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.485648 (* 1 = 0.485648 loss)
I0512 06:29:37.428521  9601 solver.cpp:571] Iteration 380, lr = 0.001
speed: 0.138s / iter
I0512 06:29:40.244997  9601 solver.cpp:242] Iteration 400, loss = 1.30579
I0512 06:29:40.245039  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.422387 (* 1 = 0.422387 loss)
I0512 06:29:40.245046  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.8834 (* 1 = 0.8834 loss)
I0512 06:29:40.245053  9601 solver.cpp:571] Iteration 400, lr = 0.001
I0512 06:29:42.980346  9601 solver.cpp:242] Iteration 420, loss = 0.847738
I0512 06:29:42.980389  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.307535 (* 1 = 0.307535 loss)
I0512 06:29:42.980396  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.540203 (* 1 = 0.540203 loss)
I0512 06:29:42.980402  9601 solver.cpp:571] Iteration 420, lr = 0.001
I0512 06:29:45.772336  9601 solver.cpp:242] Iteration 440, loss = 0.888492
I0512 06:29:45.772377  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.358861 (* 1 = 0.358861 loss)
I0512 06:29:45.772383  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.529631 (* 1 = 0.529631 loss)
I0512 06:29:45.772388  9601 solver.cpp:571] Iteration 440, lr = 0.001
I0512 06:29:48.503078  9601 solver.cpp:242] Iteration 460, loss = 1.0287
I0512 06:29:48.503118  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.521388 (* 1 = 0.521388 loss)
I0512 06:29:48.503125  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.507312 (* 1 = 0.507312 loss)
I0512 06:29:48.503131  9601 solver.cpp:571] Iteration 460, lr = 0.001
I0512 06:29:51.209808  9601 solver.cpp:242] Iteration 480, loss = 1.17639
I0512 06:29:51.209849  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.42653 (* 1 = 0.42653 loss)
I0512 06:29:51.209856  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.749861 (* 1 = 0.749861 loss)
I0512 06:29:51.209862  9601 solver.cpp:571] Iteration 480, lr = 0.001
I0512 06:29:53.960511  9601 solver.cpp:242] Iteration 500, loss = 0.862806
I0512 06:29:53.960552  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.428326 (* 1 = 0.428326 loss)
I0512 06:29:53.960559  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.43448 (* 1 = 0.43448 loss)
I0512 06:29:53.960566  9601 solver.cpp:571] Iteration 500, lr = 0.001
I0512 06:29:56.750514  9601 solver.cpp:242] Iteration 520, loss = 0.676904
I0512 06:29:56.750555  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.357186 (* 1 = 0.357186 loss)
I0512 06:29:56.750560  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.319718 (* 1 = 0.319718 loss)
I0512 06:29:56.750567  9601 solver.cpp:571] Iteration 520, lr = 0.001
I0512 06:29:59.494611  9601 solver.cpp:242] Iteration 540, loss = 0.342288
I0512 06:29:59.494652  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.13307 (* 1 = 0.13307 loss)
I0512 06:29:59.494658  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.209218 (* 1 = 0.209218 loss)
I0512 06:29:59.494664  9601 solver.cpp:571] Iteration 540, lr = 0.001
I0512 06:30:02.195910  9601 solver.cpp:242] Iteration 560, loss = 1.00134
I0512 06:30:02.195950  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.432499 (* 1 = 0.432499 loss)
I0512 06:30:02.195957  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.568838 (* 1 = 0.568838 loss)
I0512 06:30:02.195963  9601 solver.cpp:571] Iteration 560, lr = 0.001
I0512 06:30:04.981880  9601 solver.cpp:242] Iteration 580, loss = 0.915268
I0512 06:30:04.981922  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.256812 (* 1 = 0.256812 loss)
I0512 06:30:04.981930  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.658456 (* 1 = 0.658456 loss)
I0512 06:30:04.981935  9601 solver.cpp:571] Iteration 580, lr = 0.001
speed: 0.138s / iter
I0512 06:30:07.782340  9601 solver.cpp:242] Iteration 600, loss = 0.75903
I0512 06:30:07.782379  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.374657 (* 1 = 0.374657 loss)
I0512 06:30:07.782385  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.384373 (* 1 = 0.384373 loss)
I0512 06:30:07.782392  9601 solver.cpp:571] Iteration 600, lr = 0.001
I0512 06:30:10.563709  9601 solver.cpp:242] Iteration 620, loss = 0.873533
I0512 06:30:10.563751  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.359837 (* 1 = 0.359837 loss)
I0512 06:30:10.563757  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.513696 (* 1 = 0.513696 loss)
I0512 06:30:10.563763  9601 solver.cpp:571] Iteration 620, lr = 0.001
I0512 06:30:13.333147  9601 solver.cpp:242] Iteration 640, loss = 0.810696
I0512 06:30:13.333186  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.335431 (* 1 = 0.335431 loss)
I0512 06:30:13.333194  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.475265 (* 1 = 0.475265 loss)
I0512 06:30:13.333199  9601 solver.cpp:571] Iteration 640, lr = 0.001
I0512 06:30:16.052549  9601 solver.cpp:242] Iteration 660, loss = 1.17677
I0512 06:30:16.052592  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.354478 (* 1 = 0.354478 loss)
I0512 06:30:16.052597  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.822291 (* 1 = 0.822291 loss)
I0512 06:30:16.052603  9601 solver.cpp:571] Iteration 660, lr = 0.001
I0512 06:30:18.793135  9601 solver.cpp:242] Iteration 680, loss = 0.520548
I0512 06:30:18.793175  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.324855 (* 1 = 0.324855 loss)
I0512 06:30:18.793182  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.195692 (* 1 = 0.195692 loss)
I0512 06:30:18.793187  9601 solver.cpp:571] Iteration 680, lr = 0.001
I0512 06:30:21.577338  9601 solver.cpp:242] Iteration 700, loss = 0.943639
I0512 06:30:21.577380  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.424302 (* 1 = 0.424302 loss)
I0512 06:30:21.577386  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.519337 (* 1 = 0.519337 loss)
I0512 06:30:21.577392  9601 solver.cpp:571] Iteration 700, lr = 0.001
I0512 06:30:24.307054  9601 solver.cpp:242] Iteration 720, loss = 1.08238
I0512 06:30:24.307101  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.38601 (* 1 = 0.38601 loss)
I0512 06:30:24.307107  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.696366 (* 1 = 0.696366 loss)
I0512 06:30:24.307113  9601 solver.cpp:571] Iteration 720, lr = 0.001
I0512 06:30:27.083278  9601 solver.cpp:242] Iteration 740, loss = 1.12717
I0512 06:30:27.083318  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.504858 (* 1 = 0.504858 loss)
I0512 06:30:27.083325  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.622314 (* 1 = 0.622314 loss)
I0512 06:30:27.083331  9601 solver.cpp:571] Iteration 740, lr = 0.001
I0512 06:30:29.827096  9601 solver.cpp:242] Iteration 760, loss = 0.647176
I0512 06:30:29.827137  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.290581 (* 1 = 0.290581 loss)
I0512 06:30:29.827144  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.356595 (* 1 = 0.356595 loss)
I0512 06:30:29.827150  9601 solver.cpp:571] Iteration 760, lr = 0.001
I0512 06:30:32.565011  9601 solver.cpp:242] Iteration 780, loss = 1.13808
I0512 06:30:32.565052  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.462027 (* 1 = 0.462027 loss)
I0512 06:30:32.565058  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.676054 (* 1 = 0.676054 loss)
I0512 06:30:32.565064  9601 solver.cpp:571] Iteration 780, lr = 0.001
speed: 0.138s / iter
I0512 06:30:35.249483  9601 solver.cpp:242] Iteration 800, loss = 0.72338
I0512 06:30:35.249524  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.330788 (* 1 = 0.330788 loss)
I0512 06:30:35.249531  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.392592 (* 1 = 0.392592 loss)
I0512 06:30:35.249536  9601 solver.cpp:571] Iteration 800, lr = 0.001
I0512 06:30:37.991199  9601 solver.cpp:242] Iteration 820, loss = 0.725089
I0512 06:30:37.991240  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.359308 (* 1 = 0.359308 loss)
I0512 06:30:37.991245  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.365781 (* 1 = 0.365781 loss)
I0512 06:30:37.991251  9601 solver.cpp:571] Iteration 820, lr = 0.001
I0512 06:30:40.719099  9601 solver.cpp:242] Iteration 840, loss = 1.22617
I0512 06:30:40.719141  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.429575 (* 1 = 0.429575 loss)
I0512 06:30:40.719147  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.796592 (* 1 = 0.796592 loss)
I0512 06:30:40.719153  9601 solver.cpp:571] Iteration 840, lr = 0.001
I0512 06:30:43.484832  9601 solver.cpp:242] Iteration 860, loss = 0.796398
I0512 06:30:43.484872  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.388473 (* 1 = 0.388473 loss)
I0512 06:30:43.484879  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.407925 (* 1 = 0.407925 loss)
I0512 06:30:43.484885  9601 solver.cpp:571] Iteration 860, lr = 0.001
I0512 06:30:46.323463  9601 solver.cpp:242] Iteration 880, loss = 0.436402
I0512 06:30:46.323504  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.268176 (* 1 = 0.268176 loss)
I0512 06:30:46.323511  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.168225 (* 1 = 0.168225 loss)
I0512 06:30:46.323518  9601 solver.cpp:571] Iteration 880, lr = 0.001
I0512 06:30:49.052610  9601 solver.cpp:242] Iteration 900, loss = 0.676447
I0512 06:30:49.052649  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.277894 (* 1 = 0.277894 loss)
I0512 06:30:49.052657  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.398553 (* 1 = 0.398553 loss)
I0512 06:30:49.052664  9601 solver.cpp:571] Iteration 900, lr = 0.001
I0512 06:30:51.786536  9601 solver.cpp:242] Iteration 920, loss = 0.865087
I0512 06:30:51.786576  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.242755 (* 1 = 0.242755 loss)
I0512 06:30:51.786582  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.622332 (* 1 = 0.622332 loss)
I0512 06:30:51.786589  9601 solver.cpp:571] Iteration 920, lr = 0.001
I0512 06:30:54.550964  9601 solver.cpp:242] Iteration 940, loss = 0.892592
I0512 06:30:54.551009  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.37336 (* 1 = 0.37336 loss)
I0512 06:30:54.551017  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.519233 (* 1 = 0.519233 loss)
I0512 06:30:54.551023  9601 solver.cpp:571] Iteration 940, lr = 0.001
I0512 06:30:57.296284  9601 solver.cpp:242] Iteration 960, loss = 1.62378
I0512 06:30:57.296325  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.368474 (* 1 = 0.368474 loss)
I0512 06:30:57.296332  9601 solver.cpp:258]     Train net output #1: cls_loss = 1.25531 (* 1 = 1.25531 loss)
I0512 06:30:57.296339  9601 solver.cpp:571] Iteration 960, lr = 0.001
I0512 06:31:00.098222  9601 solver.cpp:242] Iteration 980, loss = 0.818304
I0512 06:31:00.098264  9601 solver.cpp:258]     Train net output #0: bbox_loss = 0.311909 (* 1 = 0.311909 loss)
I0512 06:31:00.098271  9601 solver.cpp:258]     Train net output #1: cls_loss = 0.506395 (* 1 = 0.506395 loss)
I0512 06:31:00.098278  9601 solver.cpp:571] Iteration 980, lr = 0.001
speed: 0.138s / iter
Wrote snapshot to: /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_fast_rcnn_stage1_iter_1000.caffemodel
done solving
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 2 RPN, init from stage 1 Fast R-CNN model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_fast_rcnn_stage1_iter_1000.caffemodel
Using config:
{'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.3,
           'BG_THRESH_LO': 0.01,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage2',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `voc_2007_trainval` for training
Set proposal method: gt
Appending horizontally-flipped training examples...
done
Preparing training data...
voc_2007_trainval gt roidb loaded from /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/data/cache/voc_2007_trainval_gt_roidb.pkl
done
roidb len: 1000
Output will be saved to `/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval`
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0512 06:31:04.129973  9605 solver.cpp:54] Initializing solver from parameters: 
train_net: "models/ZF/faster_rcnn_alt_opt/stage2_rpn_train.pt"
base_lr: 0.001
display: 20
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 60000
snapshot: 0
snapshot_prefix: "zf_rpn"
average_loss: 100
I0512 06:31:04.130049  9605 solver.cpp:86] Creating training net from train_net file: models/ZF/faster_rcnn_alt_opt/stage2_rpn_train.pt
I0512 06:31:04.130838  9605 net.cpp:50] Initializing net from parameters: 
name: "ZF"
state {
  phase: TRAIN
}
layer {
  name: "input-data"
  type: "Python"
  top: "data"
  top: "im_info"
  top: "gt_boxes"
  python_param {
    module: "roi_data_layer.layer"
    layer: "RoIDataLayer"
    param_str: "\'num_classes\': 21"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_cls_score"
  top: "rpn_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn-data"
  type: "Python"
  bottom: "rpn_cls_score"
  bottom: "gt_boxes"
  bottom: "im_info"
  bottom: "data"
  top: "rpn_labels"
  top: "rpn_bbox_targets"
  top: "rpn_bbox_inside_weights"
  top: "rpn_bbox_outside_weights"
  python_param {
    module: "rpn.anchor_target_layer"
    layer: "AnchorTargetLayer"
    param_str: "\'feat_stride\': 16"
  }
}
layer {
  name: "rpn_loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "rpn_cls_score_reshape"
  bottom: "rpn_labels"
  top: "rpn_cls_loss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalize: true
  }
}
layer {
  name: "rpn_loss_bbox"
  type: "SmoothL1Loss"
  bottom: "rpn_bbox_pred"
  bottom: "rpn_bbox_targets"
  bottom: "rpn_bbox_inside_weights"
  bottom: "rpn_bbox_outside_weights"
  top: "rpn_loss_bbox"
  loss_weight: 1
  smooth_l1_loss_param {
    sigma: 3
  }
}
layer {
  name: "dummy_roi_pool_conv5"
  type: "DummyData"
  top: "dummy_roi_pool_conv5"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 0.01
    }
    shape {
      dim: 1
      dim: 9216
    }
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "dummy_roi_pool_conv5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "silence_fc7"
  type: "Silence"
  bottom: "fc7"
}
I0512 06:31:04.131037  9605 layer_factory.hpp:76] Creating layer input-data
I0512 06:31:04.131645  9605 net.cpp:110] Creating Layer input-data
I0512 06:31:04.131672  9605 net.cpp:433] input-data -> data
I0512 06:31:04.131691  9605 net.cpp:433] input-data -> im_info
I0512 06:31:04.131697  9605 net.cpp:433] input-data -> gt_boxes
RoiDataLayer: name_to_top: {'gt_boxes': 2, 'data': 0, 'im_info': 1}
I0512 06:31:04.132345  9605 net.cpp:155] Setting up input-data
I0512 06:31:04.132369  9605 net.cpp:163] Top shape: 1 3 600 1000 (1800000)
I0512 06:31:04.132380  9605 net.cpp:163] Top shape: 1 3 (3)
I0512 06:31:04.132385  9605 net.cpp:163] Top shape: 1 4 (4)
I0512 06:31:04.132388  9605 layer_factory.hpp:76] Creating layer data_input-data_0_split
I0512 06:31:04.132397  9605 net.cpp:110] Creating Layer data_input-data_0_split
I0512 06:31:04.132405  9605 net.cpp:477] data_input-data_0_split <- data
I0512 06:31:04.132411  9605 net.cpp:433] data_input-data_0_split -> data_input-data_0_split_0
I0512 06:31:04.132418  9605 net.cpp:433] data_input-data_0_split -> data_input-data_0_split_1
I0512 06:31:04.132433  9605 net.cpp:155] Setting up data_input-data_0_split
I0512 06:31:04.132439  9605 net.cpp:163] Top shape: 1 3 600 1000 (1800000)
I0512 06:31:04.132442  9605 net.cpp:163] Top shape: 1 3 600 1000 (1800000)
I0512 06:31:04.132446  9605 layer_factory.hpp:76] Creating layer conv1
I0512 06:31:04.132454  9605 net.cpp:110] Creating Layer conv1
I0512 06:31:04.132458  9605 net.cpp:477] conv1 <- data_input-data_0_split_0
I0512 06:31:04.132463  9605 net.cpp:433] conv1 -> conv1
I0512 06:31:04.150930  9605 net.cpp:155] Setting up conv1
I0512 06:31:04.150967  9605 net.cpp:163] Top shape: 1 96 300 500 (14400000)
I0512 06:31:04.150991  9605 layer_factory.hpp:76] Creating layer relu1
I0512 06:31:04.151003  9605 net.cpp:110] Creating Layer relu1
I0512 06:31:04.151007  9605 net.cpp:477] relu1 <- conv1
I0512 06:31:04.151013  9605 net.cpp:419] relu1 -> conv1 (in-place)
I0512 06:31:04.151022  9605 net.cpp:155] Setting up relu1
I0512 06:31:04.151026  9605 net.cpp:163] Top shape: 1 96 300 500 (14400000)
I0512 06:31:04.151028  9605 layer_factory.hpp:76] Creating layer norm1
I0512 06:31:04.151053  9605 net.cpp:110] Creating Layer norm1
I0512 06:31:04.151059  9605 net.cpp:477] norm1 <- conv1
I0512 06:31:04.151075  9605 net.cpp:433] norm1 -> norm1
I0512 06:31:04.151118  9605 net.cpp:155] Setting up norm1
I0512 06:31:04.151129  9605 net.cpp:163] Top shape: 1 96 300 500 (14400000)
I0512 06:31:04.151132  9605 layer_factory.hpp:76] Creating layer pool1
I0512 06:31:04.151137  9605 net.cpp:110] Creating Layer pool1
I0512 06:31:04.151140  9605 net.cpp:477] pool1 <- norm1
I0512 06:31:04.151146  9605 net.cpp:433] pool1 -> pool1
I0512 06:31:04.151154  9605 net.cpp:155] Setting up pool1
I0512 06:31:04.151165  9605 net.cpp:163] Top shape: 1 96 151 251 (3638496)
I0512 06:31:04.151168  9605 layer_factory.hpp:76] Creating layer conv2
I0512 06:31:04.151175  9605 net.cpp:110] Creating Layer conv2
I0512 06:31:04.151178  9605 net.cpp:477] conv2 <- pool1
I0512 06:31:04.151185  9605 net.cpp:433] conv2 -> conv2
I0512 06:31:04.153609  9605 net.cpp:155] Setting up conv2
I0512 06:31:04.153621  9605 net.cpp:163] Top shape: 1 256 76 126 (2451456)
I0512 06:31:04.153630  9605 layer_factory.hpp:76] Creating layer relu2
I0512 06:31:04.153636  9605 net.cpp:110] Creating Layer relu2
I0512 06:31:04.153640  9605 net.cpp:477] relu2 <- conv2
I0512 06:31:04.153642  9605 net.cpp:419] relu2 -> conv2 (in-place)
I0512 06:31:04.153647  9605 net.cpp:155] Setting up relu2
I0512 06:31:04.153652  9605 net.cpp:163] Top shape: 1 256 76 126 (2451456)
I0512 06:31:04.153656  9605 layer_factory.hpp:76] Creating layer norm2
I0512 06:31:04.153663  9605 net.cpp:110] Creating Layer norm2
I0512 06:31:04.153666  9605 net.cpp:477] norm2 <- conv2
I0512 06:31:04.153669  9605 net.cpp:433] norm2 -> norm2
I0512 06:31:04.153695  9605 net.cpp:155] Setting up norm2
I0512 06:31:04.153700  9605 net.cpp:163] Top shape: 1 256 76 126 (2451456)
I0512 06:31:04.153703  9605 layer_factory.hpp:76] Creating layer pool2
I0512 06:31:04.153708  9605 net.cpp:110] Creating Layer pool2
I0512 06:31:04.153710  9605 net.cpp:477] pool2 <- norm2
I0512 06:31:04.153714  9605 net.cpp:433] pool2 -> pool2
I0512 06:31:04.153719  9605 net.cpp:155] Setting up pool2
I0512 06:31:04.153723  9605 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:31:04.153725  9605 layer_factory.hpp:76] Creating layer conv3
I0512 06:31:04.153733  9605 net.cpp:110] Creating Layer conv3
I0512 06:31:04.153736  9605 net.cpp:477] conv3 <- pool2
I0512 06:31:04.153740  9605 net.cpp:433] conv3 -> conv3
I0512 06:31:04.158313  9605 net.cpp:155] Setting up conv3
I0512 06:31:04.158324  9605 net.cpp:163] Top shape: 1 384 39 64 (958464)
I0512 06:31:04.158332  9605 layer_factory.hpp:76] Creating layer relu3
I0512 06:31:04.158339  9605 net.cpp:110] Creating Layer relu3
I0512 06:31:04.158341  9605 net.cpp:477] relu3 <- conv3
I0512 06:31:04.158345  9605 net.cpp:419] relu3 -> conv3 (in-place)
I0512 06:31:04.158351  9605 net.cpp:155] Setting up relu3
I0512 06:31:04.158354  9605 net.cpp:163] Top shape: 1 384 39 64 (958464)
I0512 06:31:04.158357  9605 layer_factory.hpp:76] Creating layer conv4
I0512 06:31:04.158365  9605 net.cpp:110] Creating Layer conv4
I0512 06:31:04.158367  9605 net.cpp:477] conv4 <- conv3
I0512 06:31:04.158371  9605 net.cpp:433] conv4 -> conv4
I0512 06:31:04.163331  9605 net.cpp:155] Setting up conv4
I0512 06:31:04.163342  9605 net.cpp:163] Top shape: 1 384 39 64 (958464)
I0512 06:31:04.163348  9605 layer_factory.hpp:76] Creating layer relu4
I0512 06:31:04.163357  9605 net.cpp:110] Creating Layer relu4
I0512 06:31:04.163359  9605 net.cpp:477] relu4 <- conv4
I0512 06:31:04.163363  9605 net.cpp:419] relu4 -> conv4 (in-place)
I0512 06:31:04.163368  9605 net.cpp:155] Setting up relu4
I0512 06:31:04.163372  9605 net.cpp:163] Top shape: 1 384 39 64 (958464)
I0512 06:31:04.163374  9605 layer_factory.hpp:76] Creating layer conv5
I0512 06:31:04.163383  9605 net.cpp:110] Creating Layer conv5
I0512 06:31:04.163384  9605 net.cpp:477] conv5 <- conv4
I0512 06:31:04.163388  9605 net.cpp:433] conv5 -> conv5
I0512 06:31:04.166792  9605 net.cpp:155] Setting up conv5
I0512 06:31:04.166805  9605 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:31:04.166812  9605 layer_factory.hpp:76] Creating layer relu5
I0512 06:31:04.166818  9605 net.cpp:110] Creating Layer relu5
I0512 06:31:04.166821  9605 net.cpp:477] relu5 <- conv5
I0512 06:31:04.166826  9605 net.cpp:419] relu5 -> conv5 (in-place)
I0512 06:31:04.166831  9605 net.cpp:155] Setting up relu5
I0512 06:31:04.166833  9605 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:31:04.166836  9605 layer_factory.hpp:76] Creating layer rpn_conv1
I0512 06:31:04.166848  9605 net.cpp:110] Creating Layer rpn_conv1
I0512 06:31:04.166851  9605 net.cpp:477] rpn_conv1 <- conv5
I0512 06:31:04.166869  9605 net.cpp:433] rpn_conv1 -> rpn_conv1
I0512 06:31:04.184376  9605 net.cpp:155] Setting up rpn_conv1
I0512 06:31:04.184386  9605 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:31:04.184392  9605 layer_factory.hpp:76] Creating layer rpn_relu1
I0512 06:31:04.184401  9605 net.cpp:110] Creating Layer rpn_relu1
I0512 06:31:04.184403  9605 net.cpp:477] rpn_relu1 <- rpn_conv1
I0512 06:31:04.184407  9605 net.cpp:419] rpn_relu1 -> rpn_conv1 (in-place)
I0512 06:31:04.184412  9605 net.cpp:155] Setting up rpn_relu1
I0512 06:31:04.184415  9605 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:31:04.184418  9605 layer_factory.hpp:76] Creating layer rpn_conv1_rpn_relu1_0_split
I0512 06:31:04.184423  9605 net.cpp:110] Creating Layer rpn_conv1_rpn_relu1_0_split
I0512 06:31:04.184427  9605 net.cpp:477] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I0512 06:31:04.184429  9605 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I0512 06:31:04.184434  9605 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I0512 06:31:04.184453  9605 net.cpp:155] Setting up rpn_conv1_rpn_relu1_0_split
I0512 06:31:04.184456  9605 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:31:04.184459  9605 net.cpp:163] Top shape: 1 256 39 64 (638976)
I0512 06:31:04.184463  9605 layer_factory.hpp:76] Creating layer rpn_cls_score
I0512 06:31:04.184471  9605 net.cpp:110] Creating Layer rpn_cls_score
I0512 06:31:04.184478  9605 net.cpp:477] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I0512 06:31:04.184483  9605 net.cpp:433] rpn_cls_score -> rpn_cls_score
I0512 06:31:04.184685  9605 net.cpp:155] Setting up rpn_cls_score
I0512 06:31:04.184694  9605 net.cpp:163] Top shape: 1 18 39 64 (44928)
I0512 06:31:04.184698  9605 layer_factory.hpp:76] Creating layer rpn_cls_score_rpn_cls_score_0_split
I0512 06:31:04.184705  9605 net.cpp:110] Creating Layer rpn_cls_score_rpn_cls_score_0_split
I0512 06:31:04.184707  9605 net.cpp:477] rpn_cls_score_rpn_cls_score_0_split <- rpn_cls_score
I0512 06:31:04.184713  9605 net.cpp:433] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_0
I0512 06:31:04.184718  9605 net.cpp:433] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_1
I0512 06:31:04.184731  9605 net.cpp:155] Setting up rpn_cls_score_rpn_cls_score_0_split
I0512 06:31:04.184734  9605 net.cpp:163] Top shape: 1 18 39 64 (44928)
I0512 06:31:04.184737  9605 net.cpp:163] Top shape: 1 18 39 64 (44928)
I0512 06:31:04.184741  9605 layer_factory.hpp:76] Creating layer rpn_bbox_pred
I0512 06:31:04.184749  9605 net.cpp:110] Creating Layer rpn_bbox_pred
I0512 06:31:04.184759  9605 net.cpp:477] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I0512 06:31:04.184764  9605 net.cpp:433] rpn_bbox_pred -> rpn_bbox_pred
I0512 06:31:04.185104  9605 net.cpp:155] Setting up rpn_bbox_pred
I0512 06:31:04.185111  9605 net.cpp:163] Top shape: 1 36 39 64 (89856)
I0512 06:31:04.185128  9605 layer_factory.hpp:76] Creating layer rpn_cls_score_reshape
I0512 06:31:04.185139  9605 net.cpp:110] Creating Layer rpn_cls_score_reshape
I0512 06:31:04.185142  9605 net.cpp:477] rpn_cls_score_reshape <- rpn_cls_score_rpn_cls_score_0_split_0
I0512 06:31:04.185148  9605 net.cpp:433] rpn_cls_score_reshape -> rpn_cls_score_reshape
I0512 06:31:04.185163  9605 net.cpp:155] Setting up rpn_cls_score_reshape
I0512 06:31:04.185168  9605 net.cpp:163] Top shape: 1 2 351 64 (44928)
I0512 06:31:04.185170  9605 layer_factory.hpp:76] Creating layer rpn-data
I0512 06:31:04.185899  9605 net.cpp:110] Creating Layer rpn-data
I0512 06:31:04.185909  9605 net.cpp:477] rpn-data <- rpn_cls_score_rpn_cls_score_0_split_1
I0512 06:31:04.185914  9605 net.cpp:477] rpn-data <- gt_boxes
I0512 06:31:04.185917  9605 net.cpp:477] rpn-data <- im_info
I0512 06:31:04.185921  9605 net.cpp:477] rpn-data <- data_input-data_0_split_1
I0512 06:31:04.185925  9605 net.cpp:433] rpn-data -> rpn_labels
I0512 06:31:04.185932  9605 net.cpp:433] rpn-data -> rpn_bbox_targets
I0512 06:31:04.185937  9605 net.cpp:433] rpn-data -> rpn_bbox_inside_weights
I0512 06:31:04.185942  9605 net.cpp:433] rpn-data -> rpn_bbox_outside_weights
I0512 06:31:04.187495  9605 net.cpp:155] Setting up rpn-data
I0512 06:31:04.187508  9605 net.cpp:163] Top shape: 1 1 351 64 (22464)
I0512 06:31:04.187512  9605 net.cpp:163] Top shape: 1 36 39 64 (89856)
I0512 06:31:04.187515  9605 net.cpp:163] Top shape: 1 36 39 64 (89856)
I0512 06:31:04.187517  9605 net.cpp:163] Top shape: 1 36 39 64 (89856)
I0512 06:31:04.187521  9605 layer_factory.hpp:76] Creating layer rpn_loss_cls
I0512 06:31:04.187531  9605 net.cpp:110] Creating Layer rpn_loss_cls
I0512 06:31:04.187535  9605 net.cpp:477] rpn_loss_cls <- rpn_cls_score_reshape
I0512 06:31:04.187539  9605 net.cpp:477] rpn_loss_cls <- rpn_labels
I0512 06:31:04.187543  9605 net.cpp:433] rpn_loss_cls -> rpn_cls_loss
I0512 06:31:04.187556  9605 layer_factory.hpp:76] Creating layer rpn_loss_cls
I0512 06:31:04.187654  9605 net.cpp:155] Setting up rpn_loss_cls
I0512 06:31:04.187661  9605 net.cpp:163] Top shape: (1)
I0512 06:31:04.187664  9605 net.cpp:168]     with loss weight 1
I0512 06:31:04.187693  9605 layer_factory.hpp:76] Creating layer rpn_loss_bbox
I0512 06:31:04.187703  9605 net.cpp:110] Creating Layer rpn_loss_bbox
I0512 06:31:04.187708  9605 net.cpp:477] rpn_loss_bbox <- rpn_bbox_pred
I0512 06:31:04.187712  9605 net.cpp:477] rpn_loss_bbox <- rpn_bbox_targets
I0512 06:31:04.187716  9605 net.cpp:477] rpn_loss_bbox <- rpn_bbox_inside_weights
I0512 06:31:04.187721  9605 net.cpp:477] rpn_loss_bbox <- rpn_bbox_outside_weights
I0512 06:31:04.187724  9605 net.cpp:433] rpn_loss_bbox -> rpn_loss_bbox
I0512 06:31:04.188302  9605 net.cpp:155] Setting up rpn_loss_bbox
I0512 06:31:04.188308  9605 net.cpp:163] Top shape: (1)
I0512 06:31:04.188310  9605 net.cpp:168]     with loss weight 1
I0512 06:31:04.188315  9605 layer_factory.hpp:76] Creating layer dummy_roi_pool_conv5
I0512 06:31:04.188325  9605 net.cpp:110] Creating Layer dummy_roi_pool_conv5
I0512 06:31:04.188330  9605 net.cpp:433] dummy_roi_pool_conv5 -> dummy_roi_pool_conv5
I0512 06:31:04.188345  9605 net.cpp:155] Setting up dummy_roi_pool_conv5
I0512 06:31:04.188349  9605 net.cpp:163] Top shape: 1 9216 (9216)
I0512 06:31:04.188352  9605 layer_factory.hpp:76] Creating layer fc6
I0512 06:31:04.188370  9605 net.cpp:110] Creating Layer fc6
I0512 06:31:04.188374  9605 net.cpp:477] fc6 <- dummy_roi_pool_conv5
I0512 06:31:04.188377  9605 net.cpp:433] fc6 -> fc6
I0512 06:31:04.331861  9605 net.cpp:155] Setting up fc6
I0512 06:31:04.331928  9605 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:31:04.331959  9605 layer_factory.hpp:76] Creating layer relu6
I0512 06:31:04.331979  9605 net.cpp:110] Creating Layer relu6
I0512 06:31:04.331990  9605 net.cpp:477] relu6 <- fc6
I0512 06:31:04.331997  9605 net.cpp:419] relu6 -> fc6 (in-place)
I0512 06:31:04.332010  9605 net.cpp:155] Setting up relu6
I0512 06:31:04.332013  9605 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:31:04.332016  9605 layer_factory.hpp:76] Creating layer fc7
I0512 06:31:04.332026  9605 net.cpp:110] Creating Layer fc7
I0512 06:31:04.332029  9605 net.cpp:477] fc7 <- fc6
I0512 06:31:04.332036  9605 net.cpp:433] fc7 -> fc7
I0512 06:31:04.394973  9605 net.cpp:155] Setting up fc7
I0512 06:31:04.395033  9605 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:31:04.395052  9605 layer_factory.hpp:76] Creating layer silence_fc7
I0512 06:31:04.395077  9605 net.cpp:110] Creating Layer silence_fc7
I0512 06:31:04.395083  9605 net.cpp:477] silence_fc7 <- fc7
I0512 06:31:04.395095  9605 net.cpp:155] Setting up silence_fc7
I0512 06:31:04.395098  9605 net.cpp:240] silence_fc7 does not need backward computation.
I0512 06:31:04.395102  9605 net.cpp:240] fc7 does not need backward computation.
I0512 06:31:04.395105  9605 net.cpp:240] relu6 does not need backward computation.
I0512 06:31:04.395108  9605 net.cpp:240] fc6 does not need backward computation.
I0512 06:31:04.395112  9605 net.cpp:240] dummy_roi_pool_conv5 does not need backward computation.
I0512 06:31:04.395115  9605 net.cpp:236] rpn_loss_bbox needs backward computation.
I0512 06:31:04.395131  9605 net.cpp:236] rpn_loss_cls needs backward computation.
I0512 06:31:04.395135  9605 net.cpp:236] rpn-data needs backward computation.
I0512 06:31:04.395143  9605 net.cpp:236] rpn_cls_score_reshape needs backward computation.
I0512 06:31:04.395146  9605 net.cpp:236] rpn_bbox_pred needs backward computation.
I0512 06:31:04.395150  9605 net.cpp:236] rpn_cls_score_rpn_cls_score_0_split needs backward computation.
I0512 06:31:04.395154  9605 net.cpp:236] rpn_cls_score needs backward computation.
I0512 06:31:04.395158  9605 net.cpp:236] rpn_conv1_rpn_relu1_0_split needs backward computation.
I0512 06:31:04.395161  9605 net.cpp:236] rpn_relu1 needs backward computation.
I0512 06:31:04.395164  9605 net.cpp:236] rpn_conv1 needs backward computation.
I0512 06:31:04.395175  9605 net.cpp:240] relu5 does not need backward computation.
I0512 06:31:04.395179  9605 net.cpp:240] conv5 does not need backward computation.
I0512 06:31:04.395182  9605 net.cpp:240] relu4 does not need backward computation.
I0512 06:31:04.395185  9605 net.cpp:240] conv4 does not need backward computation.
I0512 06:31:04.395189  9605 net.cpp:240] relu3 does not need backward computation.
I0512 06:31:04.395191  9605 net.cpp:240] conv3 does not need backward computation.
I0512 06:31:04.395196  9605 net.cpp:240] pool2 does not need backward computation.
I0512 06:31:04.395200  9605 net.cpp:240] norm2 does not need backward computation.
I0512 06:31:04.395205  9605 net.cpp:240] relu2 does not need backward computation.
I0512 06:31:04.395207  9605 net.cpp:240] conv2 does not need backward computation.
I0512 06:31:04.395211  9605 net.cpp:240] pool1 does not need backward computation.
I0512 06:31:04.395215  9605 net.cpp:240] norm1 does not need backward computation.
I0512 06:31:04.395220  9605 net.cpp:240] relu1 does not need backward computation.
I0512 06:31:04.395222  9605 net.cpp:240] conv1 does not need backward computation.
I0512 06:31:04.395282  9605 net.cpp:240] data_input-data_0_split does not need backward computation.
I0512 06:31:04.395287  9605 net.cpp:240] input-data does not need backward computation.
I0512 06:31:04.395289  9605 net.cpp:283] This network produces output rpn_cls_loss
I0512 06:31:04.395294  9605 net.cpp:283] This network produces output rpn_loss_bbox
I0512 06:31:04.395321  9605 net.cpp:297] Network initialization done.
I0512 06:31:04.395324  9605 net.cpp:298] Memory required for data: 273930660
I0512 06:31:04.395470  9605 solver.cpp:65] Solver scaffolding done.
Loading pretrained model weights from /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_fast_rcnn_stage1_iter_1000.caffemodel
Solving...
I0512 06:31:05.142457  9605 solver.cpp:242] Iteration 0, loss = 1.44579
I0512 06:31:05.142523  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.822728 (* 1 = 0.822728 loss)
I0512 06:31:05.142539  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.623061 (* 1 = 0.623061 loss)
I0512 06:31:05.142555  9605 solver.cpp:571] Iteration 0, lr = 0.001
I0512 06:31:05.869068  9605 solver.cpp:242] Iteration 20, loss = 0.781222
I0512 06:31:05.869132  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.335169 (* 1 = 0.335169 loss)
I0512 06:31:05.869140  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.446054 (* 1 = 0.446054 loss)
I0512 06:31:05.869148  9605 solver.cpp:571] Iteration 20, lr = 0.001
I0512 06:31:06.601392  9605 solver.cpp:242] Iteration 40, loss = 0.440924
I0512 06:31:06.601462  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.31827 (* 1 = 0.31827 loss)
I0512 06:31:06.601470  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.122654 (* 1 = 0.122654 loss)
I0512 06:31:06.601477  9605 solver.cpp:571] Iteration 40, lr = 0.001
I0512 06:31:07.346498  9605 solver.cpp:242] Iteration 60, loss = 0.228645
I0512 06:31:07.346562  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.141916 (* 1 = 0.141916 loss)
I0512 06:31:07.346571  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0867282 (* 1 = 0.0867282 loss)
I0512 06:31:07.346577  9605 solver.cpp:571] Iteration 60, lr = 0.001
I0512 06:31:08.073787  9605 solver.cpp:242] Iteration 80, loss = 0.362616
I0512 06:31:08.073848  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.267733 (* 1 = 0.267733 loss)
I0512 06:31:08.073855  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0948829 (* 1 = 0.0948829 loss)
I0512 06:31:08.073861  9605 solver.cpp:571] Iteration 80, lr = 0.001
I0512 06:31:08.787452  9605 solver.cpp:242] Iteration 100, loss = 0.286692
I0512 06:31:08.787518  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.119497 (* 1 = 0.119497 loss)
I0512 06:31:08.787525  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.167195 (* 1 = 0.167195 loss)
I0512 06:31:08.787533  9605 solver.cpp:571] Iteration 100, lr = 0.001
I0512 06:31:09.511240  9605 solver.cpp:242] Iteration 120, loss = 0.157887
I0512 06:31:09.511304  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.144919 (* 1 = 0.144919 loss)
I0512 06:31:09.511312  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0129683 (* 1 = 0.0129683 loss)
I0512 06:31:09.511319  9605 solver.cpp:571] Iteration 120, lr = 0.001
I0512 06:31:10.255131  9605 solver.cpp:242] Iteration 140, loss = 0.417531
I0512 06:31:10.255199  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.309686 (* 1 = 0.309686 loss)
I0512 06:31:10.255208  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.107846 (* 1 = 0.107846 loss)
I0512 06:31:10.255215  9605 solver.cpp:571] Iteration 140, lr = 0.001
I0512 06:31:11.000103  9605 solver.cpp:242] Iteration 160, loss = 0.478048
I0512 06:31:11.000162  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.379009 (* 1 = 0.379009 loss)
I0512 06:31:11.000170  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0990386 (* 1 = 0.0990386 loss)
I0512 06:31:11.000176  9605 solver.cpp:571] Iteration 160, lr = 0.001
I0512 06:31:11.748858  9605 solver.cpp:242] Iteration 180, loss = 0.175468
I0512 06:31:11.748944  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.168521 (* 1 = 0.168521 loss)
I0512 06:31:11.748953  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.006947 (* 1 = 0.006947 loss)
I0512 06:31:11.748960  9605 solver.cpp:571] Iteration 180, lr = 0.001
speed: 0.037s / iter
I0512 06:31:12.484325  9605 solver.cpp:242] Iteration 200, loss = 0.609072
I0512 06:31:12.484383  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.497902 (* 1 = 0.497902 loss)
I0512 06:31:12.484390  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.11117 (* 1 = 0.11117 loss)
I0512 06:31:12.484397  9605 solver.cpp:571] Iteration 200, lr = 0.001
I0512 06:31:13.224516  9605 solver.cpp:242] Iteration 220, loss = 0.718742
I0512 06:31:13.224580  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.240438 (* 1 = 0.240438 loss)
I0512 06:31:13.224587  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.478304 (* 1 = 0.478304 loss)
I0512 06:31:13.224593  9605 solver.cpp:571] Iteration 220, lr = 0.001
I0512 06:31:13.941694  9605 solver.cpp:242] Iteration 240, loss = 0.61898
I0512 06:31:13.941754  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.476367 (* 1 = 0.476367 loss)
I0512 06:31:13.941761  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.142613 (* 1 = 0.142613 loss)
I0512 06:31:13.941767  9605 solver.cpp:571] Iteration 240, lr = 0.001
I0512 06:31:14.680050  9605 solver.cpp:242] Iteration 260, loss = 0.435742
I0512 06:31:14.680109  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0780793 (* 1 = 0.0780793 loss)
I0512 06:31:14.680116  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.357663 (* 1 = 0.357663 loss)
I0512 06:31:14.680122  9605 solver.cpp:571] Iteration 260, lr = 0.001
I0512 06:31:15.416743  9605 solver.cpp:242] Iteration 280, loss = 0.296281
I0512 06:31:15.416803  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.233526 (* 1 = 0.233526 loss)
I0512 06:31:15.416811  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0627552 (* 1 = 0.0627552 loss)
I0512 06:31:15.416817  9605 solver.cpp:571] Iteration 280, lr = 0.001
I0512 06:31:16.145248  9605 solver.cpp:242] Iteration 300, loss = 0.533491
I0512 06:31:16.145308  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.378772 (* 1 = 0.378772 loss)
I0512 06:31:16.145315  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.154719 (* 1 = 0.154719 loss)
I0512 06:31:16.145320  9605 solver.cpp:571] Iteration 300, lr = 0.001
I0512 06:31:16.867241  9605 solver.cpp:242] Iteration 320, loss = 0.248229
I0512 06:31:16.867308  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.224488 (* 1 = 0.224488 loss)
I0512 06:31:16.867316  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0237406 (* 1 = 0.0237406 loss)
I0512 06:31:16.867322  9605 solver.cpp:571] Iteration 320, lr = 0.001
I0512 06:31:17.605164  9605 solver.cpp:242] Iteration 340, loss = 0.228981
I0512 06:31:17.605226  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.217311 (* 1 = 0.217311 loss)
I0512 06:31:17.605233  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0116692 (* 1 = 0.0116692 loss)
I0512 06:31:17.605240  9605 solver.cpp:571] Iteration 340, lr = 0.001
I0512 06:31:18.302151  9605 solver.cpp:242] Iteration 360, loss = 0.289926
I0512 06:31:18.302211  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.26007 (* 1 = 0.26007 loss)
I0512 06:31:18.302219  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0298569 (* 1 = 0.0298569 loss)
I0512 06:31:18.302225  9605 solver.cpp:571] Iteration 360, lr = 0.001
I0512 06:31:19.040541  9605 solver.cpp:242] Iteration 380, loss = 0.129669
I0512 06:31:19.040602  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.119955 (* 1 = 0.119955 loss)
I0512 06:31:19.040611  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.00971418 (* 1 = 0.00971418 loss)
I0512 06:31:19.040616  9605 solver.cpp:571] Iteration 380, lr = 0.001
speed: 0.037s / iter
I0512 06:31:19.778576  9605 solver.cpp:242] Iteration 400, loss = 0.43126
I0512 06:31:19.778636  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.175328 (* 1 = 0.175328 loss)
I0512 06:31:19.778643  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.255932 (* 1 = 0.255932 loss)
I0512 06:31:19.778650  9605 solver.cpp:571] Iteration 400, lr = 0.001
I0512 06:31:20.502709  9605 solver.cpp:242] Iteration 420, loss = 0.130434
I0512 06:31:20.502774  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0598472 (* 1 = 0.0598472 loss)
I0512 06:31:20.502782  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0705864 (* 1 = 0.0705864 loss)
I0512 06:31:20.502789  9605 solver.cpp:571] Iteration 420, lr = 0.001
I0512 06:31:21.222775  9605 solver.cpp:242] Iteration 440, loss = 0.322856
I0512 06:31:21.222841  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.219862 (* 1 = 0.219862 loss)
I0512 06:31:21.222849  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.102994 (* 1 = 0.102994 loss)
I0512 06:31:21.222856  9605 solver.cpp:571] Iteration 440, lr = 0.001
I0512 06:31:21.911715  9605 solver.cpp:242] Iteration 460, loss = 0.226264
I0512 06:31:21.911777  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0353997 (* 1 = 0.0353997 loss)
I0512 06:31:21.911785  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.190865 (* 1 = 0.190865 loss)
I0512 06:31:21.911790  9605 solver.cpp:571] Iteration 460, lr = 0.001
I0512 06:31:22.637130  9605 solver.cpp:242] Iteration 480, loss = 0.423448
I0512 06:31:22.637192  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.310717 (* 1 = 0.310717 loss)
I0512 06:31:22.637200  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.11273 (* 1 = 0.11273 loss)
I0512 06:31:22.637207  9605 solver.cpp:571] Iteration 480, lr = 0.001
I0512 06:31:23.375735  9605 solver.cpp:242] Iteration 500, loss = 0.102051
I0512 06:31:23.375799  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0675286 (* 1 = 0.0675286 loss)
I0512 06:31:23.375808  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0345226 (* 1 = 0.0345226 loss)
I0512 06:31:23.375813  9605 solver.cpp:571] Iteration 500, lr = 0.001
I0512 06:31:24.105765  9605 solver.cpp:242] Iteration 520, loss = 0.564637
I0512 06:31:24.105825  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.410882 (* 1 = 0.410882 loss)
I0512 06:31:24.105834  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.153755 (* 1 = 0.153755 loss)
I0512 06:31:24.105840  9605 solver.cpp:571] Iteration 520, lr = 0.001
I0512 06:31:24.836405  9605 solver.cpp:242] Iteration 540, loss = 0.263487
I0512 06:31:24.836473  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.151313 (* 1 = 0.151313 loss)
I0512 06:31:24.836482  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.112174 (* 1 = 0.112174 loss)
I0512 06:31:24.836488  9605 solver.cpp:571] Iteration 540, lr = 0.001
I0512 06:31:25.568470  9605 solver.cpp:242] Iteration 560, loss = 0.348945
I0512 06:31:25.568531  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.230209 (* 1 = 0.230209 loss)
I0512 06:31:25.568541  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.118737 (* 1 = 0.118737 loss)
I0512 06:31:25.568547  9605 solver.cpp:571] Iteration 560, lr = 0.001
I0512 06:31:26.304024  9605 solver.cpp:242] Iteration 580, loss = 0.297519
I0512 06:31:26.304088  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.265256 (* 1 = 0.265256 loss)
I0512 06:31:26.304095  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0322636 (* 1 = 0.0322636 loss)
I0512 06:31:26.304101  9605 solver.cpp:571] Iteration 580, lr = 0.001
speed: 0.037s / iter
I0512 06:31:27.041213  9605 solver.cpp:242] Iteration 600, loss = 0.135434
I0512 06:31:27.041283  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.126908 (* 1 = 0.126908 loss)
I0512 06:31:27.041301  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.00852511 (* 1 = 0.00852511 loss)
I0512 06:31:27.041306  9605 solver.cpp:571] Iteration 600, lr = 0.001
I0512 06:31:27.770289  9605 solver.cpp:242] Iteration 620, loss = 0.618015
I0512 06:31:27.770354  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.334969 (* 1 = 0.334969 loss)
I0512 06:31:27.770361  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.283046 (* 1 = 0.283046 loss)
I0512 06:31:27.770367  9605 solver.cpp:571] Iteration 620, lr = 0.001
I0512 06:31:28.507665  9605 solver.cpp:242] Iteration 640, loss = 0.469717
I0512 06:31:28.507730  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0984446 (* 1 = 0.0984446 loss)
I0512 06:31:28.507738  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.371273 (* 1 = 0.371273 loss)
I0512 06:31:28.507745  9605 solver.cpp:571] Iteration 640, lr = 0.001
I0512 06:31:29.259585  9605 solver.cpp:242] Iteration 660, loss = 0.169593
I0512 06:31:29.259649  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.14918 (* 1 = 0.14918 loss)
I0512 06:31:29.259659  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0204138 (* 1 = 0.0204138 loss)
I0512 06:31:29.259665  9605 solver.cpp:571] Iteration 660, lr = 0.001
I0512 06:31:29.994168  9605 solver.cpp:242] Iteration 680, loss = 0.101183
I0512 06:31:29.994226  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0868853 (* 1 = 0.0868853 loss)
I0512 06:31:29.994235  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0142978 (* 1 = 0.0142978 loss)
I0512 06:31:29.994240  9605 solver.cpp:571] Iteration 680, lr = 0.001
I0512 06:31:30.716684  9605 solver.cpp:242] Iteration 700, loss = 0.0981661
I0512 06:31:30.716744  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0921284 (* 1 = 0.0921284 loss)
I0512 06:31:30.716753  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0060377 (* 1 = 0.0060377 loss)
I0512 06:31:30.716758  9605 solver.cpp:571] Iteration 700, lr = 0.001
I0512 06:31:31.442745  9605 solver.cpp:242] Iteration 720, loss = 0.503899
I0512 06:31:31.442809  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.418795 (* 1 = 0.418795 loss)
I0512 06:31:31.442817  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0851039 (* 1 = 0.0851039 loss)
I0512 06:31:31.442824  9605 solver.cpp:571] Iteration 720, lr = 0.001
I0512 06:31:32.176045  9605 solver.cpp:242] Iteration 740, loss = 0.19108
I0512 06:31:32.176112  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0496654 (* 1 = 0.0496654 loss)
I0512 06:31:32.176120  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.141414 (* 1 = 0.141414 loss)
I0512 06:31:32.176126  9605 solver.cpp:571] Iteration 740, lr = 0.001
I0512 06:31:32.894381  9605 solver.cpp:242] Iteration 760, loss = 0.0944795
I0512 06:31:32.894449  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0861283 (* 1 = 0.0861283 loss)
I0512 06:31:32.894457  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.00835119 (* 1 = 0.00835119 loss)
I0512 06:31:32.894464  9605 solver.cpp:571] Iteration 760, lr = 0.001
I0512 06:31:33.633143  9605 solver.cpp:242] Iteration 780, loss = 0.31143
I0512 06:31:33.633204  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.257421 (* 1 = 0.257421 loss)
I0512 06:31:33.633213  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0540091 (* 1 = 0.0540091 loss)
I0512 06:31:33.633219  9605 solver.cpp:571] Iteration 780, lr = 0.001
speed: 0.037s / iter
I0512 06:31:34.367460  9605 solver.cpp:242] Iteration 800, loss = 0.755994
I0512 06:31:34.367521  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.462394 (* 1 = 0.462394 loss)
I0512 06:31:34.367527  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.2936 (* 1 = 0.2936 loss)
I0512 06:31:34.367533  9605 solver.cpp:571] Iteration 800, lr = 0.001
I0512 06:31:35.106279  9605 solver.cpp:242] Iteration 820, loss = 0.271716
I0512 06:31:35.106353  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.153731 (* 1 = 0.153731 loss)
I0512 06:31:35.106365  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.117985 (* 1 = 0.117985 loss)
I0512 06:31:35.106374  9605 solver.cpp:571] Iteration 820, lr = 0.001
I0512 06:31:35.824342  9605 solver.cpp:242] Iteration 840, loss = 0.227597
I0512 06:31:35.824404  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.134519 (* 1 = 0.134519 loss)
I0512 06:31:35.824414  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0930785 (* 1 = 0.0930785 loss)
I0512 06:31:35.824419  9605 solver.cpp:571] Iteration 840, lr = 0.001
I0512 06:31:36.556453  9605 solver.cpp:242] Iteration 860, loss = 0.463485
I0512 06:31:36.556519  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.323463 (* 1 = 0.323463 loss)
I0512 06:31:36.556535  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.140022 (* 1 = 0.140022 loss)
I0512 06:31:36.556542  9605 solver.cpp:571] Iteration 860, lr = 0.001
I0512 06:31:37.296895  9605 solver.cpp:242] Iteration 880, loss = 0.275376
I0512 06:31:37.296972  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.169707 (* 1 = 0.169707 loss)
I0512 06:31:37.296979  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.105669 (* 1 = 0.105669 loss)
I0512 06:31:37.296989  9605 solver.cpp:571] Iteration 880, lr = 0.001
I0512 06:31:38.014789  9605 solver.cpp:242] Iteration 900, loss = 0.198799
I0512 06:31:38.014859  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.15985 (* 1 = 0.15985 loss)
I0512 06:31:38.014868  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0389481 (* 1 = 0.0389481 loss)
I0512 06:31:38.014874  9605 solver.cpp:571] Iteration 900, lr = 0.001
I0512 06:31:38.765511  9605 solver.cpp:242] Iteration 920, loss = 0.493116
I0512 06:31:38.765581  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.419586 (* 1 = 0.419586 loss)
I0512 06:31:38.765590  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.07353 (* 1 = 0.07353 loss)
I0512 06:31:38.765596  9605 solver.cpp:571] Iteration 920, lr = 0.001
I0512 06:31:39.487855  9605 solver.cpp:242] Iteration 940, loss = 0.135865
I0512 06:31:39.487922  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.068842 (* 1 = 0.068842 loss)
I0512 06:31:39.487931  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0670231 (* 1 = 0.0670231 loss)
I0512 06:31:39.487937  9605 solver.cpp:571] Iteration 940, lr = 0.001
I0512 06:31:40.203763  9605 solver.cpp:242] Iteration 960, loss = 0.169186
I0512 06:31:40.203825  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.0835075 (* 1 = 0.0835075 loss)
I0512 06:31:40.203832  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0856782 (* 1 = 0.0856782 loss)
I0512 06:31:40.203840  9605 solver.cpp:571] Iteration 960, lr = 0.001
I0512 06:31:40.937114  9605 solver.cpp:242] Iteration 980, loss = 0.193125
I0512 06:31:40.937172  9605 solver.cpp:258]     Train net output #0: rpn_cls_loss = 0.180765 (* 1 = 0.180765 loss)
I0512 06:31:40.937180  9605 solver.cpp:258]     Train net output #1: rpn_loss_bbox = 0.0123595 (* 1 = 0.0123595 loss)
I0512 06:31:40.937187  9605 solver.cpp:571] Iteration 980, lr = 0.001
speed: 0.037s / iter
Wrote snapshot to: /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage2_iter_1000.caffemodel
done solving
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 2 RPN, generate proposals
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
RPN model: /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage2_iter_1000.caffemodel
Using config:
{'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 2000,
          'RPN_PRE_NMS_TOP_N': -1,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.3,
           'BG_THRESH_LO': 0.01,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': False,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'selective_search',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage2',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `voc_2007_trainval` for proposal generation
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0512 06:31:42.763543  9609 net.cpp:50] Initializing net from parameters: 
name: "ZF"
input: "data"
input: "im_info"
state {
  phase: TEST
}
input_shape {
  dim: 1
  dim: 3
  dim: 224
  dim: 224
}
input_shape {
  dim: 1
  dim: 3
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "rpn_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_cls_score"
  top: "rpn_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn_cls_prob"
  type: "Softmax"
  bottom: "rpn_cls_score_reshape"
  top: "rpn_cls_prob"
}
layer {
  name: "rpn_cls_prob_reshape"
  type: "Reshape"
  bottom: "rpn_cls_prob"
  top: "rpn_cls_prob_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 18
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "proposal"
  type: "Python"
  bottom: "rpn_cls_prob_reshape"
  bottom: "rpn_bbox_pred"
  bottom: "im_info"
  top: "rois"
  top: "scores"
  python_param {
    module: "rpn.proposal_layer"
    layer: "ProposalLayer"
    param_str: "\'feat_stride\': 16"
  }
}
I0512 06:31:42.763675  9609 net.cpp:435] Input 0 -> data
I0512 06:31:42.763712  9609 net.cpp:435] Input 1 -> im_info
I0512 06:31:42.763767  9609 layer_factory.hpp:76] Creating layer conv1
I0512 06:31:42.763780  9609 net.cpp:110] Creating Layer conv1
I0512 06:31:42.763788  9609 net.cpp:477] conv1 <- data
I0512 06:31:42.763793  9609 net.cpp:433] conv1 -> conv1
I0512 06:31:42.780946  9609 net.cpp:155] Setting up conv1
I0512 06:31:42.780982  9609 net.cpp:163] Top shape: 1 96 112 112 (1204224)
I0512 06:31:42.781004  9609 layer_factory.hpp:76] Creating layer relu1
I0512 06:31:42.781016  9609 net.cpp:110] Creating Layer relu1
I0512 06:31:42.781020  9609 net.cpp:477] relu1 <- conv1
I0512 06:31:42.781025  9609 net.cpp:419] relu1 -> conv1 (in-place)
I0512 06:31:42.781033  9609 net.cpp:155] Setting up relu1
I0512 06:31:42.781039  9609 net.cpp:163] Top shape: 1 96 112 112 (1204224)
I0512 06:31:42.781043  9609 layer_factory.hpp:76] Creating layer norm1
I0512 06:31:42.781054  9609 net.cpp:110] Creating Layer norm1
I0512 06:31:42.781057  9609 net.cpp:477] norm1 <- conv1
I0512 06:31:42.781061  9609 net.cpp:433] norm1 -> norm1
I0512 06:31:42.781105  9609 net.cpp:155] Setting up norm1
I0512 06:31:42.781112  9609 net.cpp:163] Top shape: 1 96 112 112 (1204224)
I0512 06:31:42.781116  9609 layer_factory.hpp:76] Creating layer pool1
I0512 06:31:42.781121  9609 net.cpp:110] Creating Layer pool1
I0512 06:31:42.781122  9609 net.cpp:477] pool1 <- norm1
I0512 06:31:42.781128  9609 net.cpp:433] pool1 -> pool1
I0512 06:31:42.781141  9609 net.cpp:155] Setting up pool1
I0512 06:31:42.781144  9609 net.cpp:163] Top shape: 1 96 57 57 (311904)
I0512 06:31:42.781147  9609 layer_factory.hpp:76] Creating layer conv2
I0512 06:31:42.781152  9609 net.cpp:110] Creating Layer conv2
I0512 06:31:42.781155  9609 net.cpp:477] conv2 <- pool1
I0512 06:31:42.781162  9609 net.cpp:433] conv2 -> conv2
I0512 06:31:42.784071  9609 net.cpp:155] Setting up conv2
I0512 06:31:42.784082  9609 net.cpp:163] Top shape: 1 256 29 29 (215296)
I0512 06:31:42.784090  9609 layer_factory.hpp:76] Creating layer relu2
I0512 06:31:42.784096  9609 net.cpp:110] Creating Layer relu2
I0512 06:31:42.784098  9609 net.cpp:477] relu2 <- conv2
I0512 06:31:42.784122  9609 net.cpp:419] relu2 -> conv2 (in-place)
I0512 06:31:42.784129  9609 net.cpp:155] Setting up relu2
I0512 06:31:42.784133  9609 net.cpp:163] Top shape: 1 256 29 29 (215296)
I0512 06:31:42.784135  9609 layer_factory.hpp:76] Creating layer norm2
I0512 06:31:42.784140  9609 net.cpp:110] Creating Layer norm2
I0512 06:31:42.784143  9609 net.cpp:477] norm2 <- conv2
I0512 06:31:42.784147  9609 net.cpp:433] norm2 -> norm2
I0512 06:31:42.784162  9609 net.cpp:155] Setting up norm2
I0512 06:31:42.784167  9609 net.cpp:163] Top shape: 1 256 29 29 (215296)
I0512 06:31:42.784169  9609 layer_factory.hpp:76] Creating layer pool2
I0512 06:31:42.784174  9609 net.cpp:110] Creating Layer pool2
I0512 06:31:42.784204  9609 net.cpp:477] pool2 <- norm2
I0512 06:31:42.784206  9609 net.cpp:433] pool2 -> pool2
I0512 06:31:42.784212  9609 net.cpp:155] Setting up pool2
I0512 06:31:42.784215  9609 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:31:42.784219  9609 layer_factory.hpp:76] Creating layer conv3
I0512 06:31:42.784226  9609 net.cpp:110] Creating Layer conv3
I0512 06:31:42.784232  9609 net.cpp:477] conv3 <- pool2
I0512 06:31:42.784236  9609 net.cpp:433] conv3 -> conv3
I0512 06:31:42.787495  9609 net.cpp:155] Setting up conv3
I0512 06:31:42.787506  9609 net.cpp:163] Top shape: 1 384 15 15 (86400)
I0512 06:31:42.787514  9609 layer_factory.hpp:76] Creating layer relu3
I0512 06:31:42.787520  9609 net.cpp:110] Creating Layer relu3
I0512 06:31:42.787523  9609 net.cpp:477] relu3 <- conv3
I0512 06:31:42.787526  9609 net.cpp:419] relu3 -> conv3 (in-place)
I0512 06:31:42.787542  9609 net.cpp:155] Setting up relu3
I0512 06:31:42.787547  9609 net.cpp:163] Top shape: 1 384 15 15 (86400)
I0512 06:31:42.787550  9609 layer_factory.hpp:76] Creating layer conv4
I0512 06:31:42.787556  9609 net.cpp:110] Creating Layer conv4
I0512 06:31:42.787559  9609 net.cpp:477] conv4 <- conv3
I0512 06:31:42.787562  9609 net.cpp:433] conv4 -> conv4
I0512 06:31:42.792398  9609 net.cpp:155] Setting up conv4
I0512 06:31:42.792410  9609 net.cpp:163] Top shape: 1 384 15 15 (86400)
I0512 06:31:42.792417  9609 layer_factory.hpp:76] Creating layer relu4
I0512 06:31:42.792433  9609 net.cpp:110] Creating Layer relu4
I0512 06:31:42.792438  9609 net.cpp:477] relu4 <- conv4
I0512 06:31:42.792441  9609 net.cpp:419] relu4 -> conv4 (in-place)
I0512 06:31:42.792448  9609 net.cpp:155] Setting up relu4
I0512 06:31:42.792450  9609 net.cpp:163] Top shape: 1 384 15 15 (86400)
I0512 06:31:42.792454  9609 layer_factory.hpp:76] Creating layer conv5
I0512 06:31:42.792459  9609 net.cpp:110] Creating Layer conv5
I0512 06:31:42.792461  9609 net.cpp:477] conv5 <- conv4
I0512 06:31:42.792467  9609 net.cpp:433] conv5 -> conv5
I0512 06:31:42.795763  9609 net.cpp:155] Setting up conv5
I0512 06:31:42.795773  9609 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:31:42.795781  9609 layer_factory.hpp:76] Creating layer relu5
I0512 06:31:42.795786  9609 net.cpp:110] Creating Layer relu5
I0512 06:31:42.795789  9609 net.cpp:477] relu5 <- conv5
I0512 06:31:42.795795  9609 net.cpp:419] relu5 -> conv5 (in-place)
I0512 06:31:42.795812  9609 net.cpp:155] Setting up relu5
I0512 06:31:42.795816  9609 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:31:42.795819  9609 layer_factory.hpp:76] Creating layer rpn_conv1
I0512 06:31:42.795825  9609 net.cpp:110] Creating Layer rpn_conv1
I0512 06:31:42.795830  9609 net.cpp:477] rpn_conv1 <- conv5
I0512 06:31:42.795833  9609 net.cpp:433] rpn_conv1 -> rpn_conv1
I0512 06:31:42.798105  9609 net.cpp:155] Setting up rpn_conv1
I0512 06:31:42.798115  9609 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:31:42.798121  9609 layer_factory.hpp:76] Creating layer rpn_relu1
I0512 06:31:42.798128  9609 net.cpp:110] Creating Layer rpn_relu1
I0512 06:31:42.798131  9609 net.cpp:477] rpn_relu1 <- rpn_conv1
I0512 06:31:42.798135  9609 net.cpp:419] rpn_relu1 -> rpn_conv1 (in-place)
I0512 06:31:42.798141  9609 net.cpp:155] Setting up rpn_relu1
I0512 06:31:42.798154  9609 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:31:42.798157  9609 layer_factory.hpp:76] Creating layer rpn_conv1_rpn_relu1_0_split
I0512 06:31:42.798171  9609 net.cpp:110] Creating Layer rpn_conv1_rpn_relu1_0_split
I0512 06:31:42.798174  9609 net.cpp:477] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I0512 06:31:42.798178  9609 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I0512 06:31:42.798183  9609 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I0512 06:31:42.798188  9609 net.cpp:155] Setting up rpn_conv1_rpn_relu1_0_split
I0512 06:31:42.798192  9609 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:31:42.798195  9609 net.cpp:163] Top shape: 1 256 15 15 (57600)
I0512 06:31:42.798198  9609 layer_factory.hpp:76] Creating layer rpn_cls_score
I0512 06:31:42.798215  9609 net.cpp:110] Creating Layer rpn_cls_score
I0512 06:31:42.798228  9609 net.cpp:477] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I0512 06:31:42.798233  9609 net.cpp:433] rpn_cls_score -> rpn_cls_score
I0512 06:31:42.798286  9609 net.cpp:155] Setting up rpn_cls_score
I0512 06:31:42.798292  9609 net.cpp:163] Top shape: 1 18 15 15 (4050)
I0512 06:31:42.798298  9609 layer_factory.hpp:76] Creating layer rpn_bbox_pred
I0512 06:31:42.798303  9609 net.cpp:110] Creating Layer rpn_bbox_pred
I0512 06:31:42.798306  9609 net.cpp:477] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I0512 06:31:42.798312  9609 net.cpp:433] rpn_bbox_pred -> rpn_bbox_pred
I0512 06:31:42.798367  9609 net.cpp:155] Setting up rpn_bbox_pred
I0512 06:31:42.798372  9609 net.cpp:163] Top shape: 1 36 15 15 (8100)
I0512 06:31:42.798378  9609 layer_factory.hpp:76] Creating layer rpn_cls_score_reshape
I0512 06:31:42.798388  9609 net.cpp:110] Creating Layer rpn_cls_score_reshape
I0512 06:31:42.798394  9609 net.cpp:477] rpn_cls_score_reshape <- rpn_cls_score
I0512 06:31:42.798398  9609 net.cpp:433] rpn_cls_score_reshape -> rpn_cls_score_reshape
I0512 06:31:42.798416  9609 net.cpp:155] Setting up rpn_cls_score_reshape
I0512 06:31:42.798423  9609 net.cpp:163] Top shape: 1 2 135 15 (4050)
I0512 06:31:42.798425  9609 layer_factory.hpp:76] Creating layer rpn_cls_prob
I0512 06:31:42.798434  9609 net.cpp:110] Creating Layer rpn_cls_prob
I0512 06:31:42.798439  9609 net.cpp:477] rpn_cls_prob <- rpn_cls_score_reshape
I0512 06:31:42.798444  9609 net.cpp:433] rpn_cls_prob -> rpn_cls_prob
I0512 06:31:42.798466  9609 net.cpp:155] Setting up rpn_cls_prob
I0512 06:31:42.798472  9609 net.cpp:163] Top shape: 1 2 135 15 (4050)
I0512 06:31:42.798475  9609 layer_factory.hpp:76] Creating layer rpn_cls_prob_reshape
I0512 06:31:42.798480  9609 net.cpp:110] Creating Layer rpn_cls_prob_reshape
I0512 06:31:42.798483  9609 net.cpp:477] rpn_cls_prob_reshape <- rpn_cls_prob
I0512 06:31:42.798490  9609 net.cpp:433] rpn_cls_prob_reshape -> rpn_cls_prob_reshape
I0512 06:31:42.798496  9609 net.cpp:155] Setting up rpn_cls_prob_reshape
I0512 06:31:42.798501  9609 net.cpp:163] Top shape: 1 18 15 15 (4050)
I0512 06:31:42.798504  9609 layer_factory.hpp:76] Creating layer proposal
I0512 06:31:42.800614  9609 net.cpp:110] Creating Layer proposal
I0512 06:31:42.800626  9609 net.cpp:477] proposal <- rpn_cls_prob_reshape
I0512 06:31:42.800631  9609 net.cpp:477] proposal <- rpn_bbox_pred
I0512 06:31:42.800635  9609 net.cpp:477] proposal <- im_info
I0512 06:31:42.800652  9609 net.cpp:433] proposal -> rois
I0512 06:31:42.800659  9609 net.cpp:433] proposal -> scores
I0512 06:31:42.801957  9609 net.cpp:155] Setting up proposal
I0512 06:31:42.801970  9609 net.cpp:163] Top shape: 1 5 (5)
I0512 06:31:42.801975  9609 net.cpp:163] Top shape: 1 1 1 1 (1)
I0512 06:31:42.801978  9609 net.cpp:240] proposal does not need backward computation.
I0512 06:31:42.802000  9609 net.cpp:240] rpn_cls_prob_reshape does not need backward computation.
I0512 06:31:42.802003  9609 net.cpp:240] rpn_cls_prob does not need backward computation.
I0512 06:31:42.802006  9609 net.cpp:240] rpn_cls_score_reshape does not need backward computation.
I0512 06:31:42.802008  9609 net.cpp:240] rpn_bbox_pred does not need backward computation.
I0512 06:31:42.802011  9609 net.cpp:240] rpn_cls_score does not need backward computation.
I0512 06:31:42.802014  9609 net.cpp:240] rpn_conv1_rpn_relu1_0_split does not need backward computation.
I0512 06:31:42.802017  9609 net.cpp:240] rpn_relu1 does not need backward computation.
I0512 06:31:42.802019  9609 net.cpp:240] rpn_conv1 does not need backward computation.
I0512 06:31:42.802022  9609 net.cpp:240] relu5 does not need backward computation.
I0512 06:31:42.802024  9609 net.cpp:240] conv5 does not need backward computation.
I0512 06:31:42.802026  9609 net.cpp:240] relu4 does not need backward computation.
I0512 06:31:42.802029  9609 net.cpp:240] conv4 does not need backward computation.
I0512 06:31:42.802031  9609 net.cpp:240] relu3 does not need backward computation.
I0512 06:31:42.802034  9609 net.cpp:240] conv3 does not need backward computation.
I0512 06:31:42.802037  9609 net.cpp:240] pool2 does not need backward computation.
I0512 06:31:42.802039  9609 net.cpp:240] norm2 does not need backward computation.
I0512 06:31:42.802042  9609 net.cpp:240] relu2 does not need backward computation.
I0512 06:31:42.802057  9609 net.cpp:240] conv2 does not need backward computation.
I0512 06:31:42.802059  9609 net.cpp:240] pool1 does not need backward computation.
I0512 06:31:42.802062  9609 net.cpp:240] norm1 does not need backward computation.
I0512 06:31:42.802065  9609 net.cpp:240] relu1 does not need backward computation.
I0512 06:31:42.802068  9609 net.cpp:240] conv1 does not need backward computation.
I0512 06:31:42.802070  9609 net.cpp:283] This network produces output rois
I0512 06:31:42.802074  9609 net.cpp:283] This network produces output scores
I0512 06:31:42.802090  9609 net.cpp:297] Network initialization done.
I0512 06:31:42.802093  9609 net.cpp:298] Memory required for data: 21374280
Output will be saved to `/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval`
im_proposals: 1/1000 0.092s
im_proposals: 2/1000 0.096s
im_proposals: 3/1000 0.088s
im_proposals: 4/1000 0.088s
im_proposals: 5/1000 0.088s
im_proposals: 6/1000 0.085s
im_proposals: 7/1000 0.085s
im_proposals: 8/1000 0.084s
im_proposals: 9/1000 0.084s
im_proposals: 10/1000 0.085s
im_proposals: 11/1000 0.085s
im_proposals: 12/1000 0.085s
im_proposals: 13/1000 0.084s
im_proposals: 14/1000 0.086s
im_proposals: 15/1000 0.085s
im_proposals: 16/1000 0.085s
im_proposals: 17/1000 0.084s
im_proposals: 18/1000 0.085s
im_proposals: 19/1000 0.084s
im_proposals: 20/1000 0.084s
im_proposals: 21/1000 0.085s
im_proposals: 22/1000 0.085s
im_proposals: 23/1000 0.085s
im_proposals: 24/1000 0.085s
im_proposals: 25/1000 0.085s
im_proposals: 26/1000 0.084s
im_proposals: 27/1000 0.084s
im_proposals: 28/1000 0.084s
im_proposals: 29/1000 0.084s
im_proposals: 30/1000 0.084s
im_proposals: 31/1000 0.084s
im_proposals: 32/1000 0.083s
im_proposals: 33/1000 0.083s
im_proposals: 34/1000 0.083s
im_proposals: 35/1000 0.083s
im_proposals: 36/1000 0.083s
im_proposals: 37/1000 0.083s
im_proposals: 38/1000 0.082s
im_proposals: 39/1000 0.082s
im_proposals: 40/1000 0.082s
im_proposals: 41/1000 0.082s
im_proposals: 42/1000 0.082s
im_proposals: 43/1000 0.082s
im_proposals: 44/1000 0.082s
im_proposals: 45/1000 0.082s
im_proposals: 46/1000 0.082s
im_proposals: 47/1000 0.082s
im_proposals: 48/1000 0.082s
im_proposals: 49/1000 0.082s
im_proposals: 50/1000 0.082s
im_proposals: 51/1000 0.082s
im_proposals: 52/1000 0.082s
im_proposals: 53/1000 0.082s
im_proposals: 54/1000 0.082s
im_proposals: 55/1000 0.082s
im_proposals: 56/1000 0.082s
im_proposals: 57/1000 0.082s
im_proposals: 58/1000 0.081s
im_proposals: 59/1000 0.081s
im_proposals: 60/1000 0.081s
im_proposals: 61/1000 0.081s
im_proposals: 62/1000 0.081s
im_proposals: 63/1000 0.081s
im_proposals: 64/1000 0.081s
im_proposals: 65/1000 0.080s
im_proposals: 66/1000 0.080s
im_proposals: 67/1000 0.080s
im_proposals: 68/1000 0.080s
im_proposals: 69/1000 0.080s
im_proposals: 70/1000 0.080s
im_proposals: 71/1000 0.080s
im_proposals: 72/1000 0.080s
im_proposals: 73/1000 0.080s
im_proposals: 74/1000 0.080s
im_proposals: 75/1000 0.080s
im_proposals: 76/1000 0.080s
im_proposals: 77/1000 0.080s
im_proposals: 78/1000 0.080s
im_proposals: 79/1000 0.080s
im_proposals: 80/1000 0.080s
im_proposals: 81/1000 0.080s
im_proposals: 82/1000 0.079s
im_proposals: 83/1000 0.079s
im_proposals: 84/1000 0.079s
im_proposals: 85/1000 0.080s
im_proposals: 86/1000 0.080s
im_proposals: 87/1000 0.080s
im_proposals: 88/1000 0.080s
im_proposals: 89/1000 0.080s
im_proposals: 90/1000 0.080s
im_proposals: 91/1000 0.080s
im_proposals: 92/1000 0.080s
im_proposals: 93/1000 0.080s
im_proposals: 94/1000 0.079s
im_proposals: 95/1000 0.079s
im_proposals: 96/1000 0.079s
im_proposals: 97/1000 0.079s
im_proposals: 98/1000 0.079s
im_proposals: 99/1000 0.079s
im_proposals: 100/1000 0.079s
im_proposals: 101/1000 0.079s
im_proposals: 102/1000 0.079s
im_proposals: 103/1000 0.079s
im_proposals: 104/1000 0.079s
im_proposals: 105/1000 0.079s
im_proposals: 106/1000 0.079s
im_proposals: 107/1000 0.079s
im_proposals: 108/1000 0.079s
im_proposals: 109/1000 0.079s
im_proposals: 110/1000 0.079s
im_proposals: 111/1000 0.079s
im_proposals: 112/1000 0.079s
im_proposals: 113/1000 0.079s
im_proposals: 114/1000 0.079s
im_proposals: 115/1000 0.079s
im_proposals: 116/1000 0.079s
im_proposals: 117/1000 0.079s
im_proposals: 118/1000 0.079s
im_proposals: 119/1000 0.079s
im_proposals: 120/1000 0.079s
im_proposals: 121/1000 0.079s
im_proposals: 122/1000 0.079s
im_proposals: 123/1000 0.079s
im_proposals: 124/1000 0.079s
im_proposals: 125/1000 0.079s
im_proposals: 126/1000 0.079s
im_proposals: 127/1000 0.079s
im_proposals: 128/1000 0.079s
im_proposals: 129/1000 0.079s
im_proposals: 130/1000 0.079s
im_proposals: 131/1000 0.079s
im_proposals: 132/1000 0.079s
im_proposals: 133/1000 0.079s
im_proposals: 134/1000 0.079s
im_proposals: 135/1000 0.079s
im_proposals: 136/1000 0.079s
im_proposals: 137/1000 0.079s
im_proposals: 138/1000 0.079s
im_proposals: 139/1000 0.079s
im_proposals: 140/1000 0.079s
im_proposals: 141/1000 0.079s
im_proposals: 142/1000 0.079s
im_proposals: 143/1000 0.079s
im_proposals: 144/1000 0.079s
im_proposals: 145/1000 0.079s
im_proposals: 146/1000 0.079s
im_proposals: 147/1000 0.079s
im_proposals: 148/1000 0.079s
im_proposals: 149/1000 0.079s
im_proposals: 150/1000 0.079s
im_proposals: 151/1000 0.079s
im_proposals: 152/1000 0.079s
im_proposals: 153/1000 0.078s
im_proposals: 154/1000 0.078s
im_proposals: 155/1000 0.078s
im_proposals: 156/1000 0.078s
im_proposals: 157/1000 0.078s
im_proposals: 158/1000 0.078s
im_proposals: 159/1000 0.078s
im_proposals: 160/1000 0.078s
im_proposals: 161/1000 0.078s
im_proposals: 162/1000 0.078s
im_proposals: 163/1000 0.078s
im_proposals: 164/1000 0.078s
im_proposals: 165/1000 0.078s
im_proposals: 166/1000 0.079s
im_proposals: 167/1000 0.079s
im_proposals: 168/1000 0.079s
im_proposals: 169/1000 0.079s
im_proposals: 170/1000 0.079s
im_proposals: 171/1000 0.079s
im_proposals: 172/1000 0.079s
im_proposals: 173/1000 0.079s
im_proposals: 174/1000 0.079s
im_proposals: 175/1000 0.079s
im_proposals: 176/1000 0.079s
im_proposals: 177/1000 0.079s
im_proposals: 178/1000 0.078s
im_proposals: 179/1000 0.078s
im_proposals: 180/1000 0.078s
im_proposals: 181/1000 0.078s
im_proposals: 182/1000 0.078s
im_proposals: 183/1000 0.078s
im_proposals: 184/1000 0.078s
im_proposals: 185/1000 0.078s
im_proposals: 186/1000 0.078s
im_proposals: 187/1000 0.078s
im_proposals: 188/1000 0.078s
im_proposals: 189/1000 0.078s
im_proposals: 190/1000 0.078s
im_proposals: 191/1000 0.078s
im_proposals: 192/1000 0.078s
im_proposals: 193/1000 0.078s
im_proposals: 194/1000 0.078s
im_proposals: 195/1000 0.078s
im_proposals: 196/1000 0.078s
im_proposals: 197/1000 0.078s
im_proposals: 198/1000 0.078s
im_proposals: 199/1000 0.078s
im_proposals: 200/1000 0.078s
im_proposals: 201/1000 0.078s
im_proposals: 202/1000 0.078s
im_proposals: 203/1000 0.078s
im_proposals: 204/1000 0.078s
im_proposals: 205/1000 0.078s
im_proposals: 206/1000 0.078s
im_proposals: 207/1000 0.078s
im_proposals: 208/1000 0.078s
im_proposals: 209/1000 0.078s
im_proposals: 210/1000 0.078s
im_proposals: 211/1000 0.078s
im_proposals: 212/1000 0.078s
im_proposals: 213/1000 0.078s
im_proposals: 214/1000 0.078s
im_proposals: 215/1000 0.078s
im_proposals: 216/1000 0.078s
im_proposals: 217/1000 0.078s
im_proposals: 218/1000 0.078s
im_proposals: 219/1000 0.078s
im_proposals: 220/1000 0.078s
im_proposals: 221/1000 0.078s
im_proposals: 222/1000 0.078s
im_proposals: 223/1000 0.078s
im_proposals: 224/1000 0.078s
im_proposals: 225/1000 0.078s
im_proposals: 226/1000 0.078s
im_proposals: 227/1000 0.078s
im_proposals: 228/1000 0.078s
im_proposals: 229/1000 0.078s
im_proposals: 230/1000 0.078s
im_proposals: 231/1000 0.078s
im_proposals: 232/1000 0.078s
im_proposals: 233/1000 0.078s
im_proposals: 234/1000 0.078s
im_proposals: 235/1000 0.078s
im_proposals: 236/1000 0.078s
im_proposals: 237/1000 0.078s
im_proposals: 238/1000 0.078s
im_proposals: 239/1000 0.078s
im_proposals: 240/1000 0.078s
im_proposals: 241/1000 0.078s
im_proposals: 242/1000 0.078s
im_proposals: 243/1000 0.078s
im_proposals: 244/1000 0.078s
im_proposals: 245/1000 0.078s
im_proposals: 246/1000 0.078s
im_proposals: 247/1000 0.078s
im_proposals: 248/1000 0.078s
im_proposals: 249/1000 0.078s
im_proposals: 250/1000 0.078s
im_proposals: 251/1000 0.078s
im_proposals: 252/1000 0.078s
im_proposals: 253/1000 0.078s
im_proposals: 254/1000 0.078s
im_proposals: 255/1000 0.078s
im_proposals: 256/1000 0.078s
im_proposals: 257/1000 0.078s
im_proposals: 258/1000 0.078s
im_proposals: 259/1000 0.078s
im_proposals: 260/1000 0.078s
im_proposals: 261/1000 0.078s
im_proposals: 262/1000 0.078s
im_proposals: 263/1000 0.078s
im_proposals: 264/1000 0.078s
im_proposals: 265/1000 0.078s
im_proposals: 266/1000 0.078s
im_proposals: 267/1000 0.078s
im_proposals: 268/1000 0.078s
im_proposals: 269/1000 0.078s
im_proposals: 270/1000 0.078s
im_proposals: 271/1000 0.078s
im_proposals: 272/1000 0.078s
im_proposals: 273/1000 0.078s
im_proposals: 274/1000 0.078s
im_proposals: 275/1000 0.078s
im_proposals: 276/1000 0.078s
im_proposals: 277/1000 0.078s
im_proposals: 278/1000 0.078s
im_proposals: 279/1000 0.078s
im_proposals: 280/1000 0.078s
im_proposals: 281/1000 0.078s
im_proposals: 282/1000 0.078s
im_proposals: 283/1000 0.078s
im_proposals: 284/1000 0.078s
im_proposals: 285/1000 0.078s
im_proposals: 286/1000 0.078s
im_proposals: 287/1000 0.078s
im_proposals: 288/1000 0.078s
im_proposals: 289/1000 0.078s
im_proposals: 290/1000 0.078s
im_proposals: 291/1000 0.078s
im_proposals: 292/1000 0.078s
im_proposals: 293/1000 0.078s
im_proposals: 294/1000 0.078s
im_proposals: 295/1000 0.078s
im_proposals: 296/1000 0.078s
im_proposals: 297/1000 0.078s
im_proposals: 298/1000 0.078s
im_proposals: 299/1000 0.078s
im_proposals: 300/1000 0.078s
im_proposals: 301/1000 0.078s
im_proposals: 302/1000 0.078s
im_proposals: 303/1000 0.078s
im_proposals: 304/1000 0.078s
im_proposals: 305/1000 0.078s
im_proposals: 306/1000 0.078s
im_proposals: 307/1000 0.078s
im_proposals: 308/1000 0.078s
im_proposals: 309/1000 0.078s
im_proposals: 310/1000 0.078s
im_proposals: 311/1000 0.078s
im_proposals: 312/1000 0.078s
im_proposals: 313/1000 0.078s
im_proposals: 314/1000 0.078s
im_proposals: 315/1000 0.078s
im_proposals: 316/1000 0.078s
im_proposals: 317/1000 0.078s
im_proposals: 318/1000 0.078s
im_proposals: 319/1000 0.078s
im_proposals: 320/1000 0.078s
im_proposals: 321/1000 0.078s
im_proposals: 322/1000 0.078s
im_proposals: 323/1000 0.078s
im_proposals: 324/1000 0.078s
im_proposals: 325/1000 0.078s
im_proposals: 326/1000 0.078s
im_proposals: 327/1000 0.078s
im_proposals: 328/1000 0.078s
im_proposals: 329/1000 0.078s
im_proposals: 330/1000 0.078s
im_proposals: 331/1000 0.078s
im_proposals: 332/1000 0.078s
im_proposals: 333/1000 0.078s
im_proposals: 334/1000 0.078s
im_proposals: 335/1000 0.078s
im_proposals: 336/1000 0.078s
im_proposals: 337/1000 0.078s
im_proposals: 338/1000 0.078s
im_proposals: 339/1000 0.078s
im_proposals: 340/1000 0.078s
im_proposals: 341/1000 0.078s
im_proposals: 342/1000 0.078s
im_proposals: 343/1000 0.078s
im_proposals: 344/1000 0.078s
im_proposals: 345/1000 0.078s
im_proposals: 346/1000 0.078s
im_proposals: 347/1000 0.078s
im_proposals: 348/1000 0.078s
im_proposals: 349/1000 0.078s
im_proposals: 350/1000 0.078s
im_proposals: 351/1000 0.078s
im_proposals: 352/1000 0.078s
im_proposals: 353/1000 0.078s
im_proposals: 354/1000 0.078s
im_proposals: 355/1000 0.078s
im_proposals: 356/1000 0.078s
im_proposals: 357/1000 0.078s
im_proposals: 358/1000 0.078s
im_proposals: 359/1000 0.078s
im_proposals: 360/1000 0.078s
im_proposals: 361/1000 0.078s
im_proposals: 362/1000 0.078s
im_proposals: 363/1000 0.078s
im_proposals: 364/1000 0.078s
im_proposals: 365/1000 0.078s
im_proposals: 366/1000 0.078s
im_proposals: 367/1000 0.078s
im_proposals: 368/1000 0.078s
im_proposals: 369/1000 0.078s
im_proposals: 370/1000 0.078s
im_proposals: 371/1000 0.078s
im_proposals: 372/1000 0.078s
im_proposals: 373/1000 0.078s
im_proposals: 374/1000 0.078s
im_proposals: 375/1000 0.078s
im_proposals: 376/1000 0.078s
im_proposals: 377/1000 0.078s
im_proposals: 378/1000 0.078s
im_proposals: 379/1000 0.078s
im_proposals: 380/1000 0.078s
im_proposals: 381/1000 0.078s
im_proposals: 382/1000 0.078s
im_proposals: 383/1000 0.078s
im_proposals: 384/1000 0.078s
im_proposals: 385/1000 0.078s
im_proposals: 386/1000 0.078s
im_proposals: 387/1000 0.078s
im_proposals: 388/1000 0.078s
im_proposals: 389/1000 0.078s
im_proposals: 390/1000 0.078s
im_proposals: 391/1000 0.078s
im_proposals: 392/1000 0.078s
im_proposals: 393/1000 0.078s
im_proposals: 394/1000 0.078s
im_proposals: 395/1000 0.078s
im_proposals: 396/1000 0.078s
im_proposals: 397/1000 0.078s
im_proposals: 398/1000 0.078s
im_proposals: 399/1000 0.078s
im_proposals: 400/1000 0.078s
im_proposals: 401/1000 0.078s
im_proposals: 402/1000 0.078s
im_proposals: 403/1000 0.078s
im_proposals: 404/1000 0.078s
im_proposals: 405/1000 0.078s
im_proposals: 406/1000 0.078s
im_proposals: 407/1000 0.078s
im_proposals: 408/1000 0.078s
im_proposals: 409/1000 0.078s
im_proposals: 410/1000 0.078s
im_proposals: 411/1000 0.078s
im_proposals: 412/1000 0.078s
im_proposals: 413/1000 0.078s
im_proposals: 414/1000 0.078s
im_proposals: 415/1000 0.078s
im_proposals: 416/1000 0.078s
im_proposals: 417/1000 0.078s
im_proposals: 418/1000 0.078s
im_proposals: 419/1000 0.078s
im_proposals: 420/1000 0.078s
im_proposals: 421/1000 0.078s
im_proposals: 422/1000 0.078s
im_proposals: 423/1000 0.078s
im_proposals: 424/1000 0.078s
im_proposals: 425/1000 0.078s
im_proposals: 426/1000 0.078s
im_proposals: 427/1000 0.078s
im_proposals: 428/1000 0.078s
im_proposals: 429/1000 0.078s
im_proposals: 430/1000 0.078s
im_proposals: 431/1000 0.078s
im_proposals: 432/1000 0.078s
im_proposals: 433/1000 0.078s
im_proposals: 434/1000 0.078s
im_proposals: 435/1000 0.078s
im_proposals: 436/1000 0.078s
im_proposals: 437/1000 0.078s
im_proposals: 438/1000 0.078s
im_proposals: 439/1000 0.078s
im_proposals: 440/1000 0.078s
im_proposals: 441/1000 0.078s
im_proposals: 442/1000 0.078s
im_proposals: 443/1000 0.078s
im_proposals: 444/1000 0.078s
im_proposals: 445/1000 0.078s
im_proposals: 446/1000 0.078s
im_proposals: 447/1000 0.078s
im_proposals: 448/1000 0.078s
im_proposals: 449/1000 0.078s
im_proposals: 450/1000 0.078s
im_proposals: 451/1000 0.078s
im_proposals: 452/1000 0.078s
im_proposals: 453/1000 0.078s
im_proposals: 454/1000 0.078s
im_proposals: 455/1000 0.078s
im_proposals: 456/1000 0.078s
im_proposals: 457/1000 0.078s
im_proposals: 458/1000 0.078s
im_proposals: 459/1000 0.078s
im_proposals: 460/1000 0.078s
im_proposals: 461/1000 0.078s
im_proposals: 462/1000 0.078s
im_proposals: 463/1000 0.078s
im_proposals: 464/1000 0.078s
im_proposals: 465/1000 0.078s
im_proposals: 466/1000 0.078s
im_proposals: 467/1000 0.078s
im_proposals: 468/1000 0.078s
im_proposals: 469/1000 0.078s
im_proposals: 470/1000 0.078s
im_proposals: 471/1000 0.078s
im_proposals: 472/1000 0.078s
im_proposals: 473/1000 0.078s
im_proposals: 474/1000 0.078s
im_proposals: 475/1000 0.078s
im_proposals: 476/1000 0.078s
im_proposals: 477/1000 0.078s
im_proposals: 478/1000 0.078s
im_proposals: 479/1000 0.078s
im_proposals: 480/1000 0.078s
im_proposals: 481/1000 0.078s
im_proposals: 482/1000 0.078s
im_proposals: 483/1000 0.078s
im_proposals: 484/1000 0.078s
im_proposals: 485/1000 0.078s
im_proposals: 486/1000 0.078s
im_proposals: 487/1000 0.078s
im_proposals: 488/1000 0.078s
im_proposals: 489/1000 0.078s
im_proposals: 490/1000 0.078s
im_proposals: 491/1000 0.078s
im_proposals: 492/1000 0.078s
im_proposals: 493/1000 0.078s
im_proposals: 494/1000 0.078s
im_proposals: 495/1000 0.078s
im_proposals: 496/1000 0.078s
im_proposals: 497/1000 0.078s
im_proposals: 498/1000 0.078s
im_proposals: 499/1000 0.078s
im_proposals: 500/1000 0.078s
im_proposals: 501/1000 0.078s
im_proposals: 502/1000 0.078s
im_proposals: 503/1000 0.078s
im_proposals: 504/1000 0.078s
im_proposals: 505/1000 0.078s
im_proposals: 506/1000 0.078s
im_proposals: 507/1000 0.078s
im_proposals: 508/1000 0.078s
im_proposals: 509/1000 0.078s
im_proposals: 510/1000 0.078s
im_proposals: 511/1000 0.078s
im_proposals: 512/1000 0.078s
im_proposals: 513/1000 0.078s
im_proposals: 514/1000 0.078s
im_proposals: 515/1000 0.078s
im_proposals: 516/1000 0.078s
im_proposals: 517/1000 0.078s
im_proposals: 518/1000 0.078s
im_proposals: 519/1000 0.078s
im_proposals: 520/1000 0.078s
im_proposals: 521/1000 0.078s
im_proposals: 522/1000 0.078s
im_proposals: 523/1000 0.078s
im_proposals: 524/1000 0.078s
im_proposals: 525/1000 0.078s
im_proposals: 526/1000 0.078s
im_proposals: 527/1000 0.078s
im_proposals: 528/1000 0.078s
im_proposals: 529/1000 0.078s
im_proposals: 530/1000 0.078s
im_proposals: 531/1000 0.078s
im_proposals: 532/1000 0.078s
im_proposals: 533/1000 0.078s
im_proposals: 534/1000 0.078s
im_proposals: 535/1000 0.078s
im_proposals: 536/1000 0.078s
im_proposals: 537/1000 0.078s
im_proposals: 538/1000 0.078s
im_proposals: 539/1000 0.078s
im_proposals: 540/1000 0.078s
im_proposals: 541/1000 0.078s
im_proposals: 542/1000 0.078s
im_proposals: 543/1000 0.078s
im_proposals: 544/1000 0.078s
im_proposals: 545/1000 0.078s
im_proposals: 546/1000 0.078s
im_proposals: 547/1000 0.078s
im_proposals: 548/1000 0.078s
im_proposals: 549/1000 0.078s
im_proposals: 550/1000 0.078s
im_proposals: 551/1000 0.078s
im_proposals: 552/1000 0.078s
im_proposals: 553/1000 0.078s
im_proposals: 554/1000 0.078s
im_proposals: 555/1000 0.078s
im_proposals: 556/1000 0.078s
im_proposals: 557/1000 0.078s
im_proposals: 558/1000 0.078s
im_proposals: 559/1000 0.078s
im_proposals: 560/1000 0.078s
im_proposals: 561/1000 0.078s
im_proposals: 562/1000 0.078s
im_proposals: 563/1000 0.078s
im_proposals: 564/1000 0.078s
im_proposals: 565/1000 0.078s
im_proposals: 566/1000 0.078s
im_proposals: 567/1000 0.078s
im_proposals: 568/1000 0.078s
im_proposals: 569/1000 0.078s
im_proposals: 570/1000 0.078s
im_proposals: 571/1000 0.078s
im_proposals: 572/1000 0.078s
im_proposals: 573/1000 0.078s
im_proposals: 574/1000 0.078s
im_proposals: 575/1000 0.078s
im_proposals: 576/1000 0.078s
im_proposals: 577/1000 0.078s
im_proposals: 578/1000 0.078s
im_proposals: 579/1000 0.078s
im_proposals: 580/1000 0.078s
im_proposals: 581/1000 0.078s
im_proposals: 582/1000 0.078s
im_proposals: 583/1000 0.078s
im_proposals: 584/1000 0.078s
im_proposals: 585/1000 0.078s
im_proposals: 586/1000 0.078s
im_proposals: 587/1000 0.078s
im_proposals: 588/1000 0.078s
im_proposals: 589/1000 0.078s
im_proposals: 590/1000 0.078s
im_proposals: 591/1000 0.078s
im_proposals: 592/1000 0.078s
im_proposals: 593/1000 0.078s
im_proposals: 594/1000 0.078s
im_proposals: 595/1000 0.078s
im_proposals: 596/1000 0.078s
im_proposals: 597/1000 0.078s
im_proposals: 598/1000 0.078s
im_proposals: 599/1000 0.078s
im_proposals: 600/1000 0.078s
im_proposals: 601/1000 0.078s
im_proposals: 602/1000 0.078s
im_proposals: 603/1000 0.078s
im_proposals: 604/1000 0.078s
im_proposals: 605/1000 0.078s
im_proposals: 606/1000 0.078s
im_proposals: 607/1000 0.078s
im_proposals: 608/1000 0.078s
im_proposals: 609/1000 0.078s
im_proposals: 610/1000 0.078s
im_proposals: 611/1000 0.078s
im_proposals: 612/1000 0.078s
im_proposals: 613/1000 0.078s
im_proposals: 614/1000 0.078s
im_proposals: 615/1000 0.078s
im_proposals: 616/1000 0.078s
im_proposals: 617/1000 0.078s
im_proposals: 618/1000 0.078s
im_proposals: 619/1000 0.078s
im_proposals: 620/1000 0.078s
im_proposals: 621/1000 0.078s
im_proposals: 622/1000 0.078s
im_proposals: 623/1000 0.078s
im_proposals: 624/1000 0.078s
im_proposals: 625/1000 0.078s
im_proposals: 626/1000 0.078s
im_proposals: 627/1000 0.078s
im_proposals: 628/1000 0.078s
im_proposals: 629/1000 0.078s
im_proposals: 630/1000 0.078s
im_proposals: 631/1000 0.078s
im_proposals: 632/1000 0.078s
im_proposals: 633/1000 0.078s
im_proposals: 634/1000 0.078s
im_proposals: 635/1000 0.078s
im_proposals: 636/1000 0.078s
im_proposals: 637/1000 0.078s
im_proposals: 638/1000 0.078s
im_proposals: 639/1000 0.078s
im_proposals: 640/1000 0.078s
im_proposals: 641/1000 0.078s
im_proposals: 642/1000 0.078s
im_proposals: 643/1000 0.078s
im_proposals: 644/1000 0.078s
im_proposals: 645/1000 0.078s
im_proposals: 646/1000 0.078s
im_proposals: 647/1000 0.078s
im_proposals: 648/1000 0.078s
im_proposals: 649/1000 0.078s
im_proposals: 650/1000 0.078s
im_proposals: 651/1000 0.078s
im_proposals: 652/1000 0.078s
im_proposals: 653/1000 0.078s
im_proposals: 654/1000 0.078s
im_proposals: 655/1000 0.078s
im_proposals: 656/1000 0.078s
im_proposals: 657/1000 0.078s
im_proposals: 658/1000 0.078s
im_proposals: 659/1000 0.078s
im_proposals: 660/1000 0.078s
im_proposals: 661/1000 0.078s
im_proposals: 662/1000 0.078s
im_proposals: 663/1000 0.078s
im_proposals: 664/1000 0.078s
im_proposals: 665/1000 0.078s
im_proposals: 666/1000 0.078s
im_proposals: 667/1000 0.078s
im_proposals: 668/1000 0.078s
im_proposals: 669/1000 0.078s
im_proposals: 670/1000 0.078s
im_proposals: 671/1000 0.078s
im_proposals: 672/1000 0.078s
im_proposals: 673/1000 0.078s
im_proposals: 674/1000 0.078s
im_proposals: 675/1000 0.078s
im_proposals: 676/1000 0.078s
im_proposals: 677/1000 0.078s
im_proposals: 678/1000 0.078s
im_proposals: 679/1000 0.078s
im_proposals: 680/1000 0.078s
im_proposals: 681/1000 0.078s
im_proposals: 682/1000 0.078s
im_proposals: 683/1000 0.078s
im_proposals: 684/1000 0.078s
im_proposals: 685/1000 0.078s
im_proposals: 686/1000 0.078s
im_proposals: 687/1000 0.078s
im_proposals: 688/1000 0.078s
im_proposals: 689/1000 0.078s
im_proposals: 690/1000 0.078s
im_proposals: 691/1000 0.078s
im_proposals: 692/1000 0.078s
im_proposals: 693/1000 0.078s
im_proposals: 694/1000 0.078s
im_proposals: 695/1000 0.078s
im_proposals: 696/1000 0.078s
im_proposals: 697/1000 0.078s
im_proposals: 698/1000 0.078s
im_proposals: 699/1000 0.078s
im_proposals: 700/1000 0.078s
im_proposals: 701/1000 0.078s
im_proposals: 702/1000 0.078s
im_proposals: 703/1000 0.078s
im_proposals: 704/1000 0.078s
im_proposals: 705/1000 0.078s
im_proposals: 706/1000 0.078s
im_proposals: 707/1000 0.078s
im_proposals: 708/1000 0.078s
im_proposals: 709/1000 0.078s
im_proposals: 710/1000 0.078s
im_proposals: 711/1000 0.078s
im_proposals: 712/1000 0.078s
im_proposals: 713/1000 0.078s
im_proposals: 714/1000 0.078s
im_proposals: 715/1000 0.078s
im_proposals: 716/1000 0.078s
im_proposals: 717/1000 0.078s
im_proposals: 718/1000 0.078s
im_proposals: 719/1000 0.078s
im_proposals: 720/1000 0.078s
im_proposals: 721/1000 0.078s
im_proposals: 722/1000 0.078s
im_proposals: 723/1000 0.078s
im_proposals: 724/1000 0.078s
im_proposals: 725/1000 0.078s
im_proposals: 726/1000 0.078s
im_proposals: 727/1000 0.078s
im_proposals: 728/1000 0.078s
im_proposals: 729/1000 0.078s
im_proposals: 730/1000 0.078s
im_proposals: 731/1000 0.078s
im_proposals: 732/1000 0.078s
im_proposals: 733/1000 0.078s
im_proposals: 734/1000 0.078s
im_proposals: 735/1000 0.078s
im_proposals: 736/1000 0.078s
im_proposals: 737/1000 0.078s
im_proposals: 738/1000 0.078s
im_proposals: 739/1000 0.078s
im_proposals: 740/1000 0.078s
im_proposals: 741/1000 0.078s
im_proposals: 742/1000 0.078s
im_proposals: 743/1000 0.078s
im_proposals: 744/1000 0.078s
im_proposals: 745/1000 0.078s
im_proposals: 746/1000 0.078s
im_proposals: 747/1000 0.078s
im_proposals: 748/1000 0.078s
im_proposals: 749/1000 0.078s
im_proposals: 750/1000 0.078s
im_proposals: 751/1000 0.078s
im_proposals: 752/1000 0.078s
im_proposals: 753/1000 0.078s
im_proposals: 754/1000 0.078s
im_proposals: 755/1000 0.078s
im_proposals: 756/1000 0.078s
im_proposals: 757/1000 0.078s
im_proposals: 758/1000 0.078s
im_proposals: 759/1000 0.078s
im_proposals: 760/1000 0.078s
im_proposals: 761/1000 0.078s
im_proposals: 762/1000 0.078s
im_proposals: 763/1000 0.078s
im_proposals: 764/1000 0.078s
im_proposals: 765/1000 0.078s
im_proposals: 766/1000 0.078s
im_proposals: 767/1000 0.078s
im_proposals: 768/1000 0.078s
im_proposals: 769/1000 0.078s
im_proposals: 770/1000 0.078s
im_proposals: 771/1000 0.078s
im_proposals: 772/1000 0.078s
im_proposals: 773/1000 0.078s
im_proposals: 774/1000 0.078s
im_proposals: 775/1000 0.078s
im_proposals: 776/1000 0.078s
im_proposals: 777/1000 0.078s
im_proposals: 778/1000 0.078s
im_proposals: 779/1000 0.078s
im_proposals: 780/1000 0.078s
im_proposals: 781/1000 0.078s
im_proposals: 782/1000 0.078s
im_proposals: 783/1000 0.078s
im_proposals: 784/1000 0.078s
im_proposals: 785/1000 0.078s
im_proposals: 786/1000 0.078s
im_proposals: 787/1000 0.078s
im_proposals: 788/1000 0.078s
im_proposals: 789/1000 0.078s
im_proposals: 790/1000 0.078s
im_proposals: 791/1000 0.078s
im_proposals: 792/1000 0.078s
im_proposals: 793/1000 0.078s
im_proposals: 794/1000 0.078s
im_proposals: 795/1000 0.078s
im_proposals: 796/1000 0.078s
im_proposals: 797/1000 0.078s
im_proposals: 798/1000 0.078s
im_proposals: 799/1000 0.078s
im_proposals: 800/1000 0.078s
im_proposals: 801/1000 0.078s
im_proposals: 802/1000 0.078s
im_proposals: 803/1000 0.078s
im_proposals: 804/1000 0.078s
im_proposals: 805/1000 0.078s
im_proposals: 806/1000 0.078s
im_proposals: 807/1000 0.078s
im_proposals: 808/1000 0.078s
im_proposals: 809/1000 0.078s
im_proposals: 810/1000 0.078s
im_proposals: 811/1000 0.078s
im_proposals: 812/1000 0.078s
im_proposals: 813/1000 0.078s
im_proposals: 814/1000 0.078s
im_proposals: 815/1000 0.078s
im_proposals: 816/1000 0.078s
im_proposals: 817/1000 0.078s
im_proposals: 818/1000 0.078s
im_proposals: 819/1000 0.078s
im_proposals: 820/1000 0.078s
im_proposals: 821/1000 0.078s
im_proposals: 822/1000 0.078s
im_proposals: 823/1000 0.078s
im_proposals: 824/1000 0.078s
im_proposals: 825/1000 0.078s
im_proposals: 826/1000 0.078s
im_proposals: 827/1000 0.078s
im_proposals: 828/1000 0.078s
im_proposals: 829/1000 0.078s
im_proposals: 830/1000 0.078s
im_proposals: 831/1000 0.078s
im_proposals: 832/1000 0.078s
im_proposals: 833/1000 0.078s
im_proposals: 834/1000 0.078s
im_proposals: 835/1000 0.078s
im_proposals: 836/1000 0.078s
im_proposals: 837/1000 0.078s
im_proposals: 838/1000 0.078s
im_proposals: 839/1000 0.078s
im_proposals: 840/1000 0.078s
im_proposals: 841/1000 0.078s
im_proposals: 842/1000 0.078s
im_proposals: 843/1000 0.078s
im_proposals: 844/1000 0.078s
im_proposals: 845/1000 0.078s
im_proposals: 846/1000 0.078s
im_proposals: 847/1000 0.078s
im_proposals: 848/1000 0.078s
im_proposals: 849/1000 0.078s
im_proposals: 850/1000 0.078s
im_proposals: 851/1000 0.078s
im_proposals: 852/1000 0.078s
im_proposals: 853/1000 0.078s
im_proposals: 854/1000 0.078s
im_proposals: 855/1000 0.078s
im_proposals: 856/1000 0.078s
im_proposals: 857/1000 0.078s
im_proposals: 858/1000 0.078s
im_proposals: 859/1000 0.078s
im_proposals: 860/1000 0.078s
im_proposals: 861/1000 0.078s
im_proposals: 862/1000 0.078s
im_proposals: 863/1000 0.078s
im_proposals: 864/1000 0.078s
im_proposals: 865/1000 0.078s
im_proposals: 866/1000 0.078s
im_proposals: 867/1000 0.078s
im_proposals: 868/1000 0.078s
im_proposals: 869/1000 0.078s
im_proposals: 870/1000 0.078s
im_proposals: 871/1000 0.078s
im_proposals: 872/1000 0.078s
im_proposals: 873/1000 0.078s
im_proposals: 874/1000 0.078s
im_proposals: 875/1000 0.078s
im_proposals: 876/1000 0.078s
im_proposals: 877/1000 0.078s
im_proposals: 878/1000 0.078s
im_proposals: 879/1000 0.078s
im_proposals: 880/1000 0.078s
im_proposals: 881/1000 0.078s
im_proposals: 882/1000 0.078s
im_proposals: 883/1000 0.078s
im_proposals: 884/1000 0.078s
im_proposals: 885/1000 0.078s
im_proposals: 886/1000 0.078s
im_proposals: 887/1000 0.078s
im_proposals: 888/1000 0.078s
im_proposals: 889/1000 0.078s
im_proposals: 890/1000 0.078s
im_proposals: 891/1000 0.078s
im_proposals: 892/1000 0.078s
im_proposals: 893/1000 0.078s
im_proposals: 894/1000 0.078s
im_proposals: 895/1000 0.078s
im_proposals: 896/1000 0.078s
im_proposals: 897/1000 0.078s
im_proposals: 898/1000 0.078s
im_proposals: 899/1000 0.078s
im_proposals: 900/1000 0.078s
im_proposals: 901/1000 0.078s
im_proposals: 902/1000 0.078s
im_proposals: 903/1000 0.078s
im_proposals: 904/1000 0.078s
im_proposals: 905/1000 0.078s
im_proposals: 906/1000 0.078s
im_proposals: 907/1000 0.078s
im_proposals: 908/1000 0.078s
im_proposals: 909/1000 0.078s
im_proposals: 910/1000 0.078s
im_proposals: 911/1000 0.078s
im_proposals: 912/1000 0.078s
im_proposals: 913/1000 0.078s
im_proposals: 914/1000 0.078s
im_proposals: 915/1000 0.078s
im_proposals: 916/1000 0.078s
im_proposals: 917/1000 0.078s
im_proposals: 918/1000 0.078s
im_proposals: 919/1000 0.078s
im_proposals: 920/1000 0.078s
im_proposals: 921/1000 0.078s
im_proposals: 922/1000 0.078s
im_proposals: 923/1000 0.078s
im_proposals: 924/1000 0.078s
im_proposals: 925/1000 0.078s
im_proposals: 926/1000 0.078s
im_proposals: 927/1000 0.078s
im_proposals: 928/1000 0.078s
im_proposals: 929/1000 0.078s
im_proposals: 930/1000 0.078s
im_proposals: 931/1000 0.078s
im_proposals: 932/1000 0.078s
im_proposals: 933/1000 0.078s
im_proposals: 934/1000 0.078s
im_proposals: 935/1000 0.078s
im_proposals: 936/1000 0.078s
im_proposals: 937/1000 0.078s
im_proposals: 938/1000 0.078s
im_proposals: 939/1000 0.078s
im_proposals: 940/1000 0.078s
im_proposals: 941/1000 0.078s
im_proposals: 942/1000 0.078s
im_proposals: 943/1000 0.078s
im_proposals: 944/1000 0.078s
im_proposals: 945/1000 0.078s
im_proposals: 946/1000 0.078s
im_proposals: 947/1000 0.078s
im_proposals: 948/1000 0.078s
im_proposals: 949/1000 0.078s
im_proposals: 950/1000 0.078s
im_proposals: 951/1000 0.078s
im_proposals: 952/1000 0.078s
im_proposals: 953/1000 0.078s
im_proposals: 954/1000 0.078s
im_proposals: 955/1000 0.078s
im_proposals: 956/1000 0.078s
im_proposals: 957/1000 0.078s
im_proposals: 958/1000 0.078s
im_proposals: 959/1000 0.078s
im_proposals: 960/1000 0.078s
im_proposals: 961/1000 0.078s
im_proposals: 962/1000 0.078s
im_proposals: 963/1000 0.078s
im_proposals: 964/1000 0.078s
im_proposals: 965/1000 0.078s
im_proposals: 966/1000 0.078s
im_proposals: 967/1000 0.078s
im_proposals: 968/1000 0.078s
im_proposals: 969/1000 0.078s
im_proposals: 970/1000 0.078s
im_proposals: 971/1000 0.078s
im_proposals: 972/1000 0.078s
im_proposals: 973/1000 0.078s
im_proposals: 974/1000 0.078s
im_proposals: 975/1000 0.078s
im_proposals: 976/1000 0.078s
im_proposals: 977/1000 0.078s
im_proposals: 978/1000 0.078s
im_proposals: 979/1000 0.078s
im_proposals: 980/1000 0.078s
im_proposals: 981/1000 0.078s
im_proposals: 982/1000 0.078s
im_proposals: 983/1000 0.078s
im_proposals: 984/1000 0.078s
im_proposals: 985/1000 0.078s
im_proposals: 986/1000 0.078s
im_proposals: 987/1000 0.078s
im_proposals: 988/1000 0.078s
im_proposals: 989/1000 0.078s
im_proposals: 990/1000 0.078s
im_proposals: 991/1000 0.078s
im_proposals: 992/1000 0.078s
im_proposals: 993/1000 0.078s
im_proposals: 994/1000 0.078s
im_proposals: 995/1000 0.078s
im_proposals: 996/1000 0.078s
im_proposals: 997/1000 0.078s
im_proposals: 998/1000 0.078s
im_proposals: 999/1000 0.078s
im_proposals: 1000/1000 0.078s
Wrote RPN proposals to /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage2_iter_1000_proposals.pkl
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 2 Fast R-CNN, init from stage 2 RPN R-CNN model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage2_iter_1000.caffemodel
RPN proposals: /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage2_iter_1000_proposals.pkl
Using config:
{'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.3,
           'BG_THRESH_LO': 0.01,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': False,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'rpn',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage2',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `voc_2007_trainval` for training
Set proposal method: rpn
Appending horizontally-flipped training examples...
done
Preparing training data...
voc_2007_trainval gt roidb loaded from /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/data/cache/voc_2007_trainval_gt_roidb.pkl
loading /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage2_iter_1000_proposals.pkl
done
Output will be saved to `/home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval`
Computing bounding-box regression targets...
bbox target means:
[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [ -5.16581779e-03  -4.67567066e-03   1.36134068e-01  -4.60968495e-02]
 [ -1.01932582e-02   6.69854749e-03   3.47605001e-02   4.63080317e-02]
 [ -5.66915556e-05   2.24096570e-03   3.27490042e-02   1.41217678e-02]
 [ -2.20052427e-03  -5.38900746e-03   4.53896777e-02  -2.62764056e-02]
 [  1.15703054e-03   9.11712161e-04  -6.32735250e-02   7.58336032e-02]
 [ -1.27451786e-04  -5.74419512e-04   3.91890363e-02  -4.84999975e-03]
 [  2.72932537e-03   1.65414468e-03   9.47516369e-02  -2.97254153e-02]
 [  2.88975429e-03   5.56196203e-03   9.50767477e-02   4.68520161e-02]
 [ -9.01858351e-04   3.71255102e-03   6.98123859e-03   2.93447502e-02]
 [ -1.44924691e-03   5.84999413e-03   5.89389483e-02   4.05887450e-02]
 [ -3.18108625e-03   1.66845198e-02   1.26314154e-01  -3.17852550e-04]
 [  1.54788339e-03   4.22858978e-03   6.97132052e-02   5.45004564e-02]
 [  3.83925411e-03   4.74399660e-03   3.85358081e-02   4.11813958e-02]
 [  1.16489287e-03   2.06280553e-03   8.26383107e-02   3.70860503e-02]
 [ -1.14832951e-03   7.21088646e-03  -3.09712965e-03   8.41853459e-02]
 [  2.02988596e-03   2.18640635e-03   1.93581768e-02   2.93863472e-02]
 [ -1.35812267e-02  -5.15455541e-03   9.23159842e-03  -2.18262429e-02]
 [  2.90998109e-03   7.95147455e-03   1.15765634e-01   2.02053446e-02]
 [  1.01082082e-03  -7.36917581e-04   7.11915570e-02   3.66247408e-03]
 [  1.21676812e-03   6.78846426e-03   6.47440107e-03   1.43558127e-02]]
[-0.00087549  0.00309782  0.05084115  0.02042597]
bbox target stdevs:
[[ 0.          0.          0.          0.        ]
 [ 0.14391491  0.1209679   0.23026119  0.23517535]
 [ 0.137237    0.12864962  0.25901861  0.23735877]
 [ 0.13339497  0.12804364  0.2499263   0.24590843]
 [ 0.13463865  0.12534426  0.24890948  0.2487431 ]
 [ 0.11396759  0.12795961  0.24342815  0.23386657]
 [ 0.13224527  0.13230244  0.24964664  0.25861297]
 [ 0.13594324  0.12425134  0.23285076  0.24343906]
 [ 0.14531868  0.13165804  0.2506102   0.23869754]
 [ 0.13396107  0.12905976  0.25185021  0.24464152]
 [ 0.13817015  0.13065201  0.24945694  0.23703817]
 [ 0.14868743  0.13240967  0.24558678  0.24594053]
 [ 0.13996229  0.13049488  0.25259197  0.2362224 ]
 [ 0.13614845  0.13410479  0.25448596  0.24460437]
 [ 0.13753905  0.13555499  0.24579384  0.25060343]
 [ 0.13197984  0.13423325  0.25585158  0.23528195]
 [ 0.12807299  0.12464554  0.24985113  0.24846539]
 [ 0.12520057  0.12254439  0.2449597   0.2247653 ]
 [ 0.14298862  0.13628531  0.24157818  0.24867656]
 [ 0.13364954  0.13408574  0.24556541  0.2587368 ]
 [ 0.13325313  0.1318389   0.25411568  0.24723948]]
[ 0.13531367  0.1297543   0.24781694  0.24320088]
Normalizing targets
done
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0512 06:33:06.173090  9614 solver.cpp:54] Initializing solver from parameters: 
train_net: "models/ZF/faster_rcnn_alt_opt/stage2_fast_rcnn_train.pt"
base_lr: 0.001
display: 20
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 30000
snapshot: 0
snapshot_prefix: "zf_fast_rcnn"
average_loss: 100
I0512 06:33:06.173149  9614 solver.cpp:86] Creating training net from train_net file: models/ZF/faster_rcnn_alt_opt/stage2_fast_rcnn_train.pt
I0512 06:33:06.173887  9614 net.cpp:50] Initializing net from parameters: 
name: "ZF"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "rois"
  top: "labels"
  top: "bbox_targets"
  top: "bbox_inside_weights"
  top: "bbox_outside_weights"
  python_param {
    module: "roi_data_layer.layer"
    layer: "RoIDataLayer"
    param_str: "\'num_classes\': 21"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "roi_pool_conv5"
  type: "ROIPooling"
  bottom: "conv5"
  bottom: "rois"
  top: "roi_pool_conv5"
  roi_pooling_param {
    pooled_h: 6
    pooled_w: 6
    spatial_scale: 0.0625
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "roi_pool_conv5"
  top: "fc6"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
    scale_train: false
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
    scale_train: false
  }
}
layer {
  name: "cls_score"
  type: "InnerProduct"
  bottom: "fc7"
  top: "cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 21
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bbox_pred"
  type: "InnerProduct"
  bottom: "fc7"
  top: "bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 84
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "cls_score"
  bottom: "labels"
  top: "cls_loss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalize: true
  }
}
layer {
  name: "loss_bbox"
  type: "SmoothL1Loss"
  bottom: "bbox_pred"
  bottom: "bbox_targets"
  bottom: "bbox_inside_weights"
  bottom: "bbox_outside_weights"
  top: "bbox_loss"
  loss_weight: 1
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "silence_rpn_cls_score"
  type: "Silence"
  bottom: "rpn_cls_score"
}
layer {
  name: "silence_rpn_bbox_pred"
  type: "Silence"
  bottom: "rpn_bbox_pred"
}
I0512 06:33:06.174021  9614 layer_factory.hpp:76] Creating layer data
I0512 06:33:06.174706  9614 net.cpp:110] Creating Layer data
I0512 06:33:06.174720  9614 net.cpp:433] data -> data
I0512 06:33:06.174736  9614 net.cpp:433] data -> rois
I0512 06:33:06.174741  9614 net.cpp:433] data -> labels
I0512 06:33:06.174746  9614 net.cpp:433] data -> bbox_targets
I0512 06:33:06.174751  9614 net.cpp:433] data -> bbox_inside_weights
I0512 06:33:06.174756  9614 net.cpp:433] data -> bbox_outside_weights
RoiDataLayer: name_to_top: {'bbox_inside_weights': 4, 'labels': 2, 'rois': 1, 'bbox_targets': 3, 'bbox_outside_weights': 5, 'data': 0}
I0512 06:33:06.175526  9614 net.cpp:155] Setting up data
I0512 06:33:06.175544  9614 net.cpp:163] Top shape: 2 3 600 1000 (3600000)
I0512 06:33:06.175556  9614 net.cpp:163] Top shape: 1 5 (5)
I0512 06:33:06.175560  9614 net.cpp:163] Top shape: 1 (1)
I0512 06:33:06.175564  9614 net.cpp:163] Top shape: 1 84 (84)
I0512 06:33:06.175566  9614 net.cpp:163] Top shape: 1 84 (84)
I0512 06:33:06.175570  9614 net.cpp:163] Top shape: 1 84 (84)
I0512 06:33:06.175572  9614 layer_factory.hpp:76] Creating layer conv1
I0512 06:33:06.175582  9614 net.cpp:110] Creating Layer conv1
I0512 06:33:06.175618  9614 net.cpp:477] conv1 <- data
I0512 06:33:06.175624  9614 net.cpp:433] conv1 -> conv1
I0512 06:33:06.191370  9614 net.cpp:155] Setting up conv1
I0512 06:33:06.191387  9614 net.cpp:163] Top shape: 2 96 300 500 (28800000)
I0512 06:33:06.191402  9614 layer_factory.hpp:76] Creating layer relu1
I0512 06:33:06.191408  9614 net.cpp:110] Creating Layer relu1
I0512 06:33:06.191412  9614 net.cpp:477] relu1 <- conv1
I0512 06:33:06.191416  9614 net.cpp:419] relu1 -> conv1 (in-place)
I0512 06:33:06.191424  9614 net.cpp:155] Setting up relu1
I0512 06:33:06.191428  9614 net.cpp:163] Top shape: 2 96 300 500 (28800000)
I0512 06:33:06.191431  9614 layer_factory.hpp:76] Creating layer norm1
I0512 06:33:06.191438  9614 net.cpp:110] Creating Layer norm1
I0512 06:33:06.191442  9614 net.cpp:477] norm1 <- conv1
I0512 06:33:06.191444  9614 net.cpp:433] norm1 -> norm1
I0512 06:33:06.191473  9614 net.cpp:155] Setting up norm1
I0512 06:33:06.191478  9614 net.cpp:163] Top shape: 2 96 300 500 (28800000)
I0512 06:33:06.191481  9614 layer_factory.hpp:76] Creating layer pool1
I0512 06:33:06.191485  9614 net.cpp:110] Creating Layer pool1
I0512 06:33:06.191488  9614 net.cpp:477] pool1 <- norm1
I0512 06:33:06.191491  9614 net.cpp:433] pool1 -> pool1
I0512 06:33:06.191498  9614 net.cpp:155] Setting up pool1
I0512 06:33:06.191503  9614 net.cpp:163] Top shape: 2 96 151 251 (7276992)
I0512 06:33:06.191504  9614 layer_factory.hpp:76] Creating layer conv2
I0512 06:33:06.191509  9614 net.cpp:110] Creating Layer conv2
I0512 06:33:06.191511  9614 net.cpp:477] conv2 <- pool1
I0512 06:33:06.191514  9614 net.cpp:433] conv2 -> conv2
I0512 06:33:06.193045  9614 net.cpp:155] Setting up conv2
I0512 06:33:06.193056  9614 net.cpp:163] Top shape: 2 256 76 126 (4902912)
I0512 06:33:06.193065  9614 layer_factory.hpp:76] Creating layer relu2
I0512 06:33:06.193069  9614 net.cpp:110] Creating Layer relu2
I0512 06:33:06.193073  9614 net.cpp:477] relu2 <- conv2
I0512 06:33:06.193076  9614 net.cpp:419] relu2 -> conv2 (in-place)
I0512 06:33:06.193080  9614 net.cpp:155] Setting up relu2
I0512 06:33:06.193084  9614 net.cpp:163] Top shape: 2 256 76 126 (4902912)
I0512 06:33:06.193086  9614 layer_factory.hpp:76] Creating layer norm2
I0512 06:33:06.193091  9614 net.cpp:110] Creating Layer norm2
I0512 06:33:06.193094  9614 net.cpp:477] norm2 <- conv2
I0512 06:33:06.193097  9614 net.cpp:433] norm2 -> norm2
I0512 06:33:06.193109  9614 net.cpp:155] Setting up norm2
I0512 06:33:06.193114  9614 net.cpp:163] Top shape: 2 256 76 126 (4902912)
I0512 06:33:06.193115  9614 layer_factory.hpp:76] Creating layer pool2
I0512 06:33:06.193120  9614 net.cpp:110] Creating Layer pool2
I0512 06:33:06.193123  9614 net.cpp:477] pool2 <- norm2
I0512 06:33:06.193126  9614 net.cpp:433] pool2 -> pool2
I0512 06:33:06.193131  9614 net.cpp:155] Setting up pool2
I0512 06:33:06.193135  9614 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:33:06.193138  9614 layer_factory.hpp:76] Creating layer conv3
I0512 06:33:06.193143  9614 net.cpp:110] Creating Layer conv3
I0512 06:33:06.193145  9614 net.cpp:477] conv3 <- pool2
I0512 06:33:06.193148  9614 net.cpp:433] conv3 -> conv3
I0512 06:33:06.196264  9614 net.cpp:155] Setting up conv3
I0512 06:33:06.196276  9614 net.cpp:163] Top shape: 2 384 39 64 (1916928)
I0512 06:33:06.196285  9614 layer_factory.hpp:76] Creating layer relu3
I0512 06:33:06.196290  9614 net.cpp:110] Creating Layer relu3
I0512 06:33:06.196292  9614 net.cpp:477] relu3 <- conv3
I0512 06:33:06.196296  9614 net.cpp:419] relu3 -> conv3 (in-place)
I0512 06:33:06.196301  9614 net.cpp:155] Setting up relu3
I0512 06:33:06.196305  9614 net.cpp:163] Top shape: 2 384 39 64 (1916928)
I0512 06:33:06.196307  9614 layer_factory.hpp:76] Creating layer conv4
I0512 06:33:06.196312  9614 net.cpp:110] Creating Layer conv4
I0512 06:33:06.196315  9614 net.cpp:477] conv4 <- conv3
I0512 06:33:06.196318  9614 net.cpp:433] conv4 -> conv4
I0512 06:33:06.199417  9614 net.cpp:155] Setting up conv4
I0512 06:33:06.199429  9614 net.cpp:163] Top shape: 2 384 39 64 (1916928)
I0512 06:33:06.199434  9614 layer_factory.hpp:76] Creating layer relu4
I0512 06:33:06.199440  9614 net.cpp:110] Creating Layer relu4
I0512 06:33:06.199442  9614 net.cpp:477] relu4 <- conv4
I0512 06:33:06.199447  9614 net.cpp:419] relu4 -> conv4 (in-place)
I0512 06:33:06.199451  9614 net.cpp:155] Setting up relu4
I0512 06:33:06.199455  9614 net.cpp:163] Top shape: 2 384 39 64 (1916928)
I0512 06:33:06.199458  9614 layer_factory.hpp:76] Creating layer conv5
I0512 06:33:06.199463  9614 net.cpp:110] Creating Layer conv5
I0512 06:33:06.199466  9614 net.cpp:477] conv5 <- conv4
I0512 06:33:06.199470  9614 net.cpp:433] conv5 -> conv5
I0512 06:33:06.201594  9614 net.cpp:155] Setting up conv5
I0512 06:33:06.201606  9614 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:33:06.201614  9614 layer_factory.hpp:76] Creating layer relu5
I0512 06:33:06.201619  9614 net.cpp:110] Creating Layer relu5
I0512 06:33:06.201622  9614 net.cpp:477] relu5 <- conv5
I0512 06:33:06.201627  9614 net.cpp:419] relu5 -> conv5 (in-place)
I0512 06:33:06.201632  9614 net.cpp:155] Setting up relu5
I0512 06:33:06.201635  9614 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:33:06.201637  9614 layer_factory.hpp:76] Creating layer conv5_relu5_0_split
I0512 06:33:06.201645  9614 net.cpp:110] Creating Layer conv5_relu5_0_split
I0512 06:33:06.201648  9614 net.cpp:477] conv5_relu5_0_split <- conv5
I0512 06:33:06.201653  9614 net.cpp:433] conv5_relu5_0_split -> conv5_relu5_0_split_0
I0512 06:33:06.201658  9614 net.cpp:433] conv5_relu5_0_split -> conv5_relu5_0_split_1
I0512 06:33:06.201664  9614 net.cpp:155] Setting up conv5_relu5_0_split
I0512 06:33:06.201668  9614 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:33:06.201670  9614 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:33:06.201673  9614 layer_factory.hpp:76] Creating layer roi_pool_conv5
I0512 06:33:06.201681  9614 net.cpp:110] Creating Layer roi_pool_conv5
I0512 06:33:06.201684  9614 net.cpp:477] roi_pool_conv5 <- conv5_relu5_0_split_0
I0512 06:33:06.201688  9614 net.cpp:477] roi_pool_conv5 <- rois
I0512 06:33:06.201691  9614 net.cpp:433] roi_pool_conv5 -> roi_pool_conv5
I0512 06:33:06.201697  9614 roi_pooling_layer.cpp:30] Spatial scale: 0.0625
I0512 06:33:06.201712  9614 net.cpp:155] Setting up roi_pool_conv5
I0512 06:33:06.201716  9614 net.cpp:163] Top shape: 1 256 6 6 (9216)
I0512 06:33:06.201719  9614 layer_factory.hpp:76] Creating layer fc6
I0512 06:33:06.201726  9614 net.cpp:110] Creating Layer fc6
I0512 06:33:06.201768  9614 net.cpp:477] fc6 <- roi_pool_conv5
I0512 06:33:06.201773  9614 net.cpp:433] fc6 -> fc6
I0512 06:33:06.283937  9614 net.cpp:155] Setting up fc6
I0512 06:33:06.283974  9614 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:33:06.283989  9614 layer_factory.hpp:76] Creating layer relu6
I0512 06:33:06.284000  9614 net.cpp:110] Creating Layer relu6
I0512 06:33:06.284005  9614 net.cpp:477] relu6 <- fc6
I0512 06:33:06.284013  9614 net.cpp:419] relu6 -> fc6 (in-place)
I0512 06:33:06.284021  9614 net.cpp:155] Setting up relu6
I0512 06:33:06.284024  9614 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:33:06.284027  9614 layer_factory.hpp:76] Creating layer drop6
I0512 06:33:06.284042  9614 net.cpp:110] Creating Layer drop6
I0512 06:33:06.284045  9614 net.cpp:477] drop6 <- fc6
I0512 06:33:06.284049  9614 net.cpp:419] drop6 -> fc6 (in-place)
I0512 06:33:06.284056  9614 net.cpp:155] Setting up drop6
I0512 06:33:06.284060  9614 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:33:06.284063  9614 layer_factory.hpp:76] Creating layer fc7
I0512 06:33:06.284070  9614 net.cpp:110] Creating Layer fc7
I0512 06:33:06.284072  9614 net.cpp:477] fc7 <- fc6
I0512 06:33:06.284076  9614 net.cpp:433] fc7 -> fc7
I0512 06:33:06.320798  9614 net.cpp:155] Setting up fc7
I0512 06:33:06.320833  9614 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:33:06.320843  9614 layer_factory.hpp:76] Creating layer relu7
I0512 06:33:06.320853  9614 net.cpp:110] Creating Layer relu7
I0512 06:33:06.320858  9614 net.cpp:477] relu7 <- fc7
I0512 06:33:06.320864  9614 net.cpp:419] relu7 -> fc7 (in-place)
I0512 06:33:06.320873  9614 net.cpp:155] Setting up relu7
I0512 06:33:06.320876  9614 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:33:06.320878  9614 layer_factory.hpp:76] Creating layer drop7
I0512 06:33:06.320886  9614 net.cpp:110] Creating Layer drop7
I0512 06:33:06.320888  9614 net.cpp:477] drop7 <- fc7
I0512 06:33:06.320893  9614 net.cpp:419] drop7 -> fc7 (in-place)
I0512 06:33:06.320899  9614 net.cpp:155] Setting up drop7
I0512 06:33:06.320901  9614 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:33:06.320904  9614 layer_factory.hpp:76] Creating layer fc7_drop7_0_split
I0512 06:33:06.320910  9614 net.cpp:110] Creating Layer fc7_drop7_0_split
I0512 06:33:06.320912  9614 net.cpp:477] fc7_drop7_0_split <- fc7
I0512 06:33:06.320916  9614 net.cpp:433] fc7_drop7_0_split -> fc7_drop7_0_split_0
I0512 06:33:06.320921  9614 net.cpp:433] fc7_drop7_0_split -> fc7_drop7_0_split_1
I0512 06:33:06.320927  9614 net.cpp:155] Setting up fc7_drop7_0_split
I0512 06:33:06.320930  9614 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:33:06.320933  9614 net.cpp:163] Top shape: 1 4096 (4096)
I0512 06:33:06.320936  9614 layer_factory.hpp:76] Creating layer cls_score
I0512 06:33:06.320943  9614 net.cpp:110] Creating Layer cls_score
I0512 06:33:06.320950  9614 net.cpp:477] cls_score <- fc7_drop7_0_split_0
I0512 06:33:06.320956  9614 net.cpp:433] cls_score -> cls_score
I0512 06:33:06.323326  9614 net.cpp:155] Setting up cls_score
I0512 06:33:06.323333  9614 net.cpp:163] Top shape: 1 21 (21)
I0512 06:33:06.323338  9614 layer_factory.hpp:76] Creating layer bbox_pred
I0512 06:33:06.323345  9614 net.cpp:110] Creating Layer bbox_pred
I0512 06:33:06.323348  9614 net.cpp:477] bbox_pred <- fc7_drop7_0_split_1
I0512 06:33:06.323354  9614 net.cpp:433] bbox_pred -> bbox_pred
I0512 06:33:06.333263  9614 net.cpp:155] Setting up bbox_pred
I0512 06:33:06.333273  9614 net.cpp:163] Top shape: 1 84 (84)
I0512 06:33:06.333286  9614 layer_factory.hpp:76] Creating layer loss_cls
I0512 06:33:06.333295  9614 net.cpp:110] Creating Layer loss_cls
I0512 06:33:06.333300  9614 net.cpp:477] loss_cls <- cls_score
I0512 06:33:06.333304  9614 net.cpp:477] loss_cls <- labels
I0512 06:33:06.333308  9614 net.cpp:433] loss_cls -> cls_loss
I0512 06:33:06.333318  9614 layer_factory.hpp:76] Creating layer loss_cls
I0512 06:33:06.333361  9614 net.cpp:155] Setting up loss_cls
I0512 06:33:06.333367  9614 net.cpp:163] Top shape: (1)
I0512 06:33:06.333369  9614 net.cpp:168]     with loss weight 1
I0512 06:33:06.333379  9614 layer_factory.hpp:76] Creating layer loss_bbox
I0512 06:33:06.333386  9614 net.cpp:110] Creating Layer loss_bbox
I0512 06:33:06.333389  9614 net.cpp:477] loss_bbox <- bbox_pred
I0512 06:33:06.333392  9614 net.cpp:477] loss_bbox <- bbox_targets
I0512 06:33:06.333395  9614 net.cpp:477] loss_bbox <- bbox_inside_weights
I0512 06:33:06.333398  9614 net.cpp:477] loss_bbox <- bbox_outside_weights
I0512 06:33:06.333402  9614 net.cpp:433] loss_bbox -> bbox_loss
I0512 06:33:06.333434  9614 net.cpp:155] Setting up loss_bbox
I0512 06:33:06.333447  9614 net.cpp:163] Top shape: (1)
I0512 06:33:06.333451  9614 net.cpp:168]     with loss weight 1
I0512 06:33:06.333454  9614 layer_factory.hpp:76] Creating layer rpn_conv1
I0512 06:33:06.333461  9614 net.cpp:110] Creating Layer rpn_conv1
I0512 06:33:06.333472  9614 net.cpp:477] rpn_conv1 <- conv5_relu5_0_split_1
I0512 06:33:06.333477  9614 net.cpp:433] rpn_conv1 -> rpn_conv1
I0512 06:33:06.350164  9614 net.cpp:155] Setting up rpn_conv1
I0512 06:33:06.350174  9614 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:33:06.350181  9614 layer_factory.hpp:76] Creating layer rpn_relu1
I0512 06:33:06.350186  9614 net.cpp:110] Creating Layer rpn_relu1
I0512 06:33:06.350189  9614 net.cpp:477] rpn_relu1 <- rpn_conv1
I0512 06:33:06.350194  9614 net.cpp:419] rpn_relu1 -> rpn_conv1 (in-place)
I0512 06:33:06.350199  9614 net.cpp:155] Setting up rpn_relu1
I0512 06:33:06.350203  9614 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:33:06.350206  9614 layer_factory.hpp:76] Creating layer rpn_conv1_rpn_relu1_0_split
I0512 06:33:06.350210  9614 net.cpp:110] Creating Layer rpn_conv1_rpn_relu1_0_split
I0512 06:33:06.350214  9614 net.cpp:477] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I0512 06:33:06.350217  9614 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I0512 06:33:06.350222  9614 net.cpp:433] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I0512 06:33:06.350226  9614 net.cpp:155] Setting up rpn_conv1_rpn_relu1_0_split
I0512 06:33:06.350230  9614 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:33:06.350234  9614 net.cpp:163] Top shape: 2 256 39 64 (1277952)
I0512 06:33:06.350235  9614 layer_factory.hpp:76] Creating layer rpn_cls_score
I0512 06:33:06.350242  9614 net.cpp:110] Creating Layer rpn_cls_score
I0512 06:33:06.350244  9614 net.cpp:477] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I0512 06:33:06.350250  9614 net.cpp:433] rpn_cls_score -> rpn_cls_score
I0512 06:33:06.350415  9614 net.cpp:155] Setting up rpn_cls_score
I0512 06:33:06.350422  9614 net.cpp:163] Top shape: 2 18 39 64 (89856)
I0512 06:33:06.350427  9614 layer_factory.hpp:76] Creating layer rpn_bbox_pred
I0512 06:33:06.350438  9614 net.cpp:110] Creating Layer rpn_bbox_pred
I0512 06:33:06.350453  9614 net.cpp:477] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I0512 06:33:06.350458  9614 net.cpp:433] rpn_bbox_pred -> rpn_bbox_pred
I0512 06:33:06.350747  9614 net.cpp:155] Setting up rpn_bbox_pred
I0512 06:33:06.350754  9614 net.cpp:163] Top shape: 2 36 39 64 (179712)
I0512 06:33:06.350759  9614 layer_factory.hpp:76] Creating layer silence_rpn_cls_score
I0512 06:33:06.350766  9614 net.cpp:110] Creating Layer silence_rpn_cls_score
I0512 06:33:06.350769  9614 net.cpp:477] silence_rpn_cls_score <- rpn_cls_score
I0512 06:33:06.350775  9614 net.cpp:155] Setting up silence_rpn_cls_score
I0512 06:33:06.350776  9614 layer_factory.hpp:76] Creating layer silence_rpn_bbox_pred
I0512 06:33:06.350780  9614 net.cpp:110] Creating Layer silence_rpn_bbox_pred
I0512 06:33:06.350783  9614 net.cpp:477] silence_rpn_bbox_pred <- rpn_bbox_pred
I0512 06:33:06.350786  9614 net.cpp:155] Setting up silence_rpn_bbox_pred
I0512 06:33:06.350788  9614 net.cpp:240] silence_rpn_bbox_pred does not need backward computation.
I0512 06:33:06.350791  9614 net.cpp:240] silence_rpn_cls_score does not need backward computation.
I0512 06:33:06.350793  9614 net.cpp:240] rpn_bbox_pred does not need backward computation.
I0512 06:33:06.350796  9614 net.cpp:240] rpn_cls_score does not need backward computation.
I0512 06:33:06.350798  9614 net.cpp:240] rpn_conv1_rpn_relu1_0_split does not need backward computation.
I0512 06:33:06.350801  9614 net.cpp:240] rpn_relu1 does not need backward computation.
I0512 06:33:06.350803  9614 net.cpp:240] rpn_conv1 does not need backward computation.
I0512 06:33:06.350806  9614 net.cpp:236] loss_bbox needs backward computation.
I0512 06:33:06.350811  9614 net.cpp:236] loss_cls needs backward computation.
I0512 06:33:06.350812  9614 net.cpp:236] bbox_pred needs backward computation.
I0512 06:33:06.350816  9614 net.cpp:236] cls_score needs backward computation.
I0512 06:33:06.350818  9614 net.cpp:236] fc7_drop7_0_split needs backward computation.
I0512 06:33:06.350821  9614 net.cpp:236] drop7 needs backward computation.
I0512 06:33:06.350822  9614 net.cpp:236] relu7 needs backward computation.
I0512 06:33:06.350824  9614 net.cpp:236] fc7 needs backward computation.
I0512 06:33:06.350827  9614 net.cpp:236] drop6 needs backward computation.
I0512 06:33:06.350829  9614 net.cpp:236] relu6 needs backward computation.
I0512 06:33:06.350831  9614 net.cpp:236] fc6 needs backward computation.
I0512 06:33:06.350834  9614 net.cpp:240] roi_pool_conv5 does not need backward computation.
I0512 06:33:06.350838  9614 net.cpp:240] conv5_relu5_0_split does not need backward computation.
I0512 06:33:06.350841  9614 net.cpp:240] relu5 does not need backward computation.
I0512 06:33:06.350844  9614 net.cpp:240] conv5 does not need backward computation.
I0512 06:33:06.350847  9614 net.cpp:240] relu4 does not need backward computation.
I0512 06:33:06.350849  9614 net.cpp:240] conv4 does not need backward computation.
I0512 06:33:06.350852  9614 net.cpp:240] relu3 does not need backward computation.
I0512 06:33:06.350855  9614 net.cpp:240] conv3 does not need backward computation.
I0512 06:33:06.350858  9614 net.cpp:240] pool2 does not need backward computation.
I0512 06:33:06.350862  9614 net.cpp:240] norm2 does not need backward computation.
I0512 06:33:06.350864  9614 net.cpp:240] relu2 does not need backward computation.
I0512 06:33:06.350867  9614 net.cpp:240] conv2 does not need backward computation.
I0512 06:33:06.350869  9614 net.cpp:240] pool1 does not need backward computation.
I0512 06:33:06.350872  9614 net.cpp:240] norm1 does not need backward computation.
I0512 06:33:06.350875  9614 net.cpp:240] relu1 does not need backward computation.
I0512 06:33:06.350878  9614 net.cpp:240] conv1 does not need backward computation.
I0512 06:33:06.350883  9614 net.cpp:240] data does not need backward computation.
I0512 06:33:06.350885  9614 net.cpp:283] This network produces output bbox_loss
I0512 06:33:06.350888  9614 net.cpp:283] This network produces output cls_loss
I0512 06:33:06.350908  9614 net.cpp:297] Network initialization done.
I0512 06:33:06.350911  9614 net.cpp:298] Memory required for data: 525867700
I0512 06:33:06.351040  9614 solver.cpp:65] Solver scaffolding done.
Loading pretrained model weights from /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_rpn_stage2_iter_1000.caffemodel
Solving...
I0512 06:33:06.896961  9614 solver.cpp:242] Iteration 0, loss = 1.62344
I0512 06:33:06.897011  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.380483 (* 1 = 0.380483 loss)
I0512 06:33:06.897017  9614 solver.cpp:258]     Train net output #1: cls_loss = 1.24295 (* 1 = 1.24295 loss)
I0512 06:33:06.897030  9614 solver.cpp:571] Iteration 0, lr = 0.001
I0512 06:33:08.090374  9614 solver.cpp:242] Iteration 20, loss = 1.15029
I0512 06:33:08.090417  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.469981 (* 1 = 0.469981 loss)
I0512 06:33:08.090425  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.680306 (* 1 = 0.680306 loss)
I0512 06:33:08.090430  9614 solver.cpp:571] Iteration 20, lr = 0.001
I0512 06:33:09.291030  9614 solver.cpp:242] Iteration 40, loss = 0.985942
I0512 06:33:09.291077  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.34608 (* 1 = 0.34608 loss)
I0512 06:33:09.291085  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.639862 (* 1 = 0.639862 loss)
I0512 06:33:09.291090  9614 solver.cpp:571] Iteration 40, lr = 0.001
I0512 06:33:10.487603  9614 solver.cpp:242] Iteration 60, loss = 1.07009
I0512 06:33:10.487649  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.290077 (* 1 = 0.290077 loss)
I0512 06:33:10.487655  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.780017 (* 1 = 0.780017 loss)
I0512 06:33:10.487661  9614 solver.cpp:571] Iteration 60, lr = 0.001
I0512 06:33:11.706712  9614 solver.cpp:242] Iteration 80, loss = 1.08114
I0512 06:33:11.706758  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.470525 (* 1 = 0.470525 loss)
I0512 06:33:11.706764  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.610619 (* 1 = 0.610619 loss)
I0512 06:33:11.706770  9614 solver.cpp:571] Iteration 80, lr = 0.001
I0512 06:33:12.917801  9614 solver.cpp:242] Iteration 100, loss = 0.973027
I0512 06:33:12.917853  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.424508 (* 1 = 0.424508 loss)
I0512 06:33:12.917860  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.548518 (* 1 = 0.548518 loss)
I0512 06:33:12.917865  9614 solver.cpp:571] Iteration 100, lr = 0.001
I0512 06:33:14.098656  9614 solver.cpp:242] Iteration 120, loss = 1.1146
I0512 06:33:14.098698  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.500491 (* 1 = 0.500491 loss)
I0512 06:33:14.098706  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.614105 (* 1 = 0.614105 loss)
I0512 06:33:14.098711  9614 solver.cpp:571] Iteration 120, lr = 0.001
I0512 06:33:15.310991  9614 solver.cpp:242] Iteration 140, loss = 0.760473
I0512 06:33:15.311034  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.366287 (* 1 = 0.366287 loss)
I0512 06:33:15.311041  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.394186 (* 1 = 0.394186 loss)
I0512 06:33:15.311046  9614 solver.cpp:571] Iteration 140, lr = 0.001
I0512 06:33:16.514359  9614 solver.cpp:242] Iteration 160, loss = 0.830197
I0512 06:33:16.514400  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.452808 (* 1 = 0.452808 loss)
I0512 06:33:16.514406  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.377388 (* 1 = 0.377388 loss)
I0512 06:33:16.514412  9614 solver.cpp:571] Iteration 160, lr = 0.001
I0512 06:33:17.724366  9614 solver.cpp:242] Iteration 180, loss = 0.884015
I0512 06:33:17.724412  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.373034 (* 1 = 0.373034 loss)
I0512 06:33:17.724419  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.510981 (* 1 = 0.510981 loss)
I0512 06:33:17.724426  9614 solver.cpp:571] Iteration 180, lr = 0.001
speed: 0.060s / iter
I0512 06:33:18.928221  9614 solver.cpp:242] Iteration 200, loss = 0.852526
I0512 06:33:18.928267  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.32857 (* 1 = 0.32857 loss)
I0512 06:33:18.928273  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.523956 (* 1 = 0.523956 loss)
I0512 06:33:18.928278  9614 solver.cpp:571] Iteration 200, lr = 0.001
I0512 06:33:20.118532  9614 solver.cpp:242] Iteration 220, loss = 0.789008
I0512 06:33:20.118573  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.371082 (* 1 = 0.371082 loss)
I0512 06:33:20.118580  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.417926 (* 1 = 0.417926 loss)
I0512 06:33:20.118585  9614 solver.cpp:571] Iteration 220, lr = 0.001
I0512 06:33:21.297392  9614 solver.cpp:242] Iteration 240, loss = 0.801351
I0512 06:33:21.297433  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.407804 (* 1 = 0.407804 loss)
I0512 06:33:21.297441  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.393547 (* 1 = 0.393547 loss)
I0512 06:33:21.297446  9614 solver.cpp:571] Iteration 240, lr = 0.001
I0512 06:33:22.499748  9614 solver.cpp:242] Iteration 260, loss = 1.03805
I0512 06:33:22.499791  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.470321 (* 1 = 0.470321 loss)
I0512 06:33:22.499799  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.567725 (* 1 = 0.567725 loss)
I0512 06:33:22.499804  9614 solver.cpp:571] Iteration 260, lr = 0.001
I0512 06:33:23.721772  9614 solver.cpp:242] Iteration 280, loss = 0.671424
I0512 06:33:23.721814  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.280108 (* 1 = 0.280108 loss)
I0512 06:33:23.721822  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.391316 (* 1 = 0.391316 loss)
I0512 06:33:23.721827  9614 solver.cpp:571] Iteration 280, lr = 0.001
I0512 06:33:24.932816  9614 solver.cpp:242] Iteration 300, loss = 0.860072
I0512 06:33:24.932857  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.348955 (* 1 = 0.348955 loss)
I0512 06:33:24.932863  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.511116 (* 1 = 0.511116 loss)
I0512 06:33:24.932870  9614 solver.cpp:571] Iteration 300, lr = 0.001
I0512 06:33:26.128530  9614 solver.cpp:242] Iteration 320, loss = 0.705534
I0512 06:33:26.128574  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.339027 (* 1 = 0.339027 loss)
I0512 06:33:26.128582  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.366506 (* 1 = 0.366506 loss)
I0512 06:33:26.128587  9614 solver.cpp:571] Iteration 320, lr = 0.001
I0512 06:33:27.336369  9614 solver.cpp:242] Iteration 340, loss = 0.917444
I0512 06:33:27.336410  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.357366 (* 1 = 0.357366 loss)
I0512 06:33:27.336416  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.560078 (* 1 = 0.560078 loss)
I0512 06:33:27.336422  9614 solver.cpp:571] Iteration 340, lr = 0.001
I0512 06:33:28.512707  9614 solver.cpp:242] Iteration 360, loss = 1.03334
I0512 06:33:28.512748  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.45698 (* 1 = 0.45698 loss)
I0512 06:33:28.512768  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.576358 (* 1 = 0.576358 loss)
I0512 06:33:28.512773  9614 solver.cpp:571] Iteration 360, lr = 0.001
I0512 06:33:29.708734  9614 solver.cpp:242] Iteration 380, loss = 0.745601
I0512 06:33:29.708775  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.318521 (* 1 = 0.318521 loss)
I0512 06:33:29.708782  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.427079 (* 1 = 0.427079 loss)
I0512 06:33:29.708788  9614 solver.cpp:571] Iteration 380, lr = 0.001
speed: 0.060s / iter
I0512 06:33:30.913240  9614 solver.cpp:242] Iteration 400, loss = 1.00309
I0512 06:33:30.913280  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.399103 (* 1 = 0.399103 loss)
I0512 06:33:30.913287  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.603986 (* 1 = 0.603986 loss)
I0512 06:33:30.913293  9614 solver.cpp:571] Iteration 400, lr = 0.001
I0512 06:33:32.097452  9614 solver.cpp:242] Iteration 420, loss = 0.734505
I0512 06:33:32.097492  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.289452 (* 1 = 0.289452 loss)
I0512 06:33:32.097498  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.445053 (* 1 = 0.445053 loss)
I0512 06:33:32.097504  9614 solver.cpp:571] Iteration 420, lr = 0.001
I0512 06:33:33.303956  9614 solver.cpp:242] Iteration 440, loss = 0.61642
I0512 06:33:33.304002  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.317061 (* 1 = 0.317061 loss)
I0512 06:33:33.304008  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.29936 (* 1 = 0.29936 loss)
I0512 06:33:33.304014  9614 solver.cpp:571] Iteration 440, lr = 0.001
I0512 06:33:34.491660  9614 solver.cpp:242] Iteration 460, loss = 0.93353
I0512 06:33:34.491705  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.384129 (* 1 = 0.384129 loss)
I0512 06:33:34.491713  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.549401 (* 1 = 0.549401 loss)
I0512 06:33:34.491719  9614 solver.cpp:571] Iteration 460, lr = 0.001
I0512 06:33:35.681999  9614 solver.cpp:242] Iteration 480, loss = 0.599252
I0512 06:33:35.682042  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.44285 (* 1 = 0.44285 loss)
I0512 06:33:35.682049  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.156402 (* 1 = 0.156402 loss)
I0512 06:33:35.682055  9614 solver.cpp:571] Iteration 480, lr = 0.001
I0512 06:33:36.874572  9614 solver.cpp:242] Iteration 500, loss = 0.843557
I0512 06:33:36.874615  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.414908 (* 1 = 0.414908 loss)
I0512 06:33:36.874622  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.428649 (* 1 = 0.428649 loss)
I0512 06:33:36.874627  9614 solver.cpp:571] Iteration 500, lr = 0.001
I0512 06:33:38.053141  9614 solver.cpp:242] Iteration 520, loss = 0.515568
I0512 06:33:38.053182  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.240239 (* 1 = 0.240239 loss)
I0512 06:33:38.053189  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.27533 (* 1 = 0.27533 loss)
I0512 06:33:38.053195  9614 solver.cpp:571] Iteration 520, lr = 0.001
I0512 06:33:39.244726  9614 solver.cpp:242] Iteration 540, loss = 0.694199
I0512 06:33:39.244770  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.371973 (* 1 = 0.371973 loss)
I0512 06:33:39.244776  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.322226 (* 1 = 0.322226 loss)
I0512 06:33:39.244781  9614 solver.cpp:571] Iteration 540, lr = 0.001
I0512 06:33:40.450881  9614 solver.cpp:242] Iteration 560, loss = 0.888354
I0512 06:33:40.450928  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.347067 (* 1 = 0.347067 loss)
I0512 06:33:40.450935  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.541286 (* 1 = 0.541286 loss)
I0512 06:33:40.450942  9614 solver.cpp:571] Iteration 560, lr = 0.001
I0512 06:33:41.656997  9614 solver.cpp:242] Iteration 580, loss = 0.777021
I0512 06:33:41.657039  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.440629 (* 1 = 0.440629 loss)
I0512 06:33:41.657047  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.336393 (* 1 = 0.336393 loss)
I0512 06:33:41.657052  9614 solver.cpp:571] Iteration 580, lr = 0.001
speed: 0.060s / iter
I0512 06:33:42.854795  9614 solver.cpp:242] Iteration 600, loss = 0.690381
I0512 06:33:42.854837  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.395618 (* 1 = 0.395618 loss)
I0512 06:33:42.854845  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.294763 (* 1 = 0.294763 loss)
I0512 06:33:42.854849  9614 solver.cpp:571] Iteration 600, lr = 0.001
I0512 06:33:44.051661  9614 solver.cpp:242] Iteration 620, loss = 1.08575
I0512 06:33:44.051702  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.365449 (* 1 = 0.365449 loss)
I0512 06:33:44.051709  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.720297 (* 1 = 0.720297 loss)
I0512 06:33:44.051715  9614 solver.cpp:571] Iteration 620, lr = 0.001
I0512 06:33:45.253777  9614 solver.cpp:242] Iteration 640, loss = 0.494842
I0512 06:33:45.253818  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.211171 (* 1 = 0.211171 loss)
I0512 06:33:45.253824  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.28367 (* 1 = 0.28367 loss)
I0512 06:33:45.253830  9614 solver.cpp:571] Iteration 640, lr = 0.001
I0512 06:33:46.478340  9614 solver.cpp:242] Iteration 660, loss = 0.768616
I0512 06:33:46.478384  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.379313 (* 1 = 0.379313 loss)
I0512 06:33:46.478390  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.389303 (* 1 = 0.389303 loss)
I0512 06:33:46.478396  9614 solver.cpp:571] Iteration 660, lr = 0.001
I0512 06:33:47.728148  9614 solver.cpp:242] Iteration 680, loss = 1.09531
I0512 06:33:47.728189  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.403497 (* 1 = 0.403497 loss)
I0512 06:33:47.728196  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.691811 (* 1 = 0.691811 loss)
I0512 06:33:47.728202  9614 solver.cpp:571] Iteration 680, lr = 0.001
I0512 06:33:48.990734  9614 solver.cpp:242] Iteration 700, loss = 0.821142
I0512 06:33:48.990780  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.297393 (* 1 = 0.297393 loss)
I0512 06:33:48.990787  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.523749 (* 1 = 0.523749 loss)
I0512 06:33:48.990792  9614 solver.cpp:571] Iteration 700, lr = 0.001
I0512 06:33:50.251027  9614 solver.cpp:242] Iteration 720, loss = 0.578521
I0512 06:33:50.251067  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.357905 (* 1 = 0.357905 loss)
I0512 06:33:50.251075  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.220616 (* 1 = 0.220616 loss)
I0512 06:33:50.251080  9614 solver.cpp:571] Iteration 720, lr = 0.001
I0512 06:33:51.503021  9614 solver.cpp:242] Iteration 740, loss = 0.469742
I0512 06:33:51.503064  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.340821 (* 1 = 0.340821 loss)
I0512 06:33:51.503072  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.128921 (* 1 = 0.128921 loss)
I0512 06:33:51.503077  9614 solver.cpp:571] Iteration 740, lr = 0.001
I0512 06:33:52.766201  9614 solver.cpp:242] Iteration 760, loss = 0.812953
I0512 06:33:52.766243  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.374431 (* 1 = 0.374431 loss)
I0512 06:33:52.766250  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.438523 (* 1 = 0.438523 loss)
I0512 06:33:52.766255  9614 solver.cpp:571] Iteration 760, lr = 0.001
I0512 06:33:54.043237  9614 solver.cpp:242] Iteration 780, loss = 0.914647
I0512 06:33:54.043278  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.329026 (* 1 = 0.329026 loss)
I0512 06:33:54.043285  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.585621 (* 1 = 0.585621 loss)
I0512 06:33:54.043290  9614 solver.cpp:571] Iteration 780, lr = 0.001
speed: 0.061s / iter
I0512 06:33:55.301188  9614 solver.cpp:242] Iteration 800, loss = 0.788894
I0512 06:33:55.301232  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.354246 (* 1 = 0.354246 loss)
I0512 06:33:55.301239  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.434648 (* 1 = 0.434648 loss)
I0512 06:33:55.301244  9614 solver.cpp:571] Iteration 800, lr = 0.001
I0512 06:33:56.543166  9614 solver.cpp:242] Iteration 820, loss = 0.701168
I0512 06:33:56.543207  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.311485 (* 1 = 0.311485 loss)
I0512 06:33:56.543215  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.389682 (* 1 = 0.389682 loss)
I0512 06:33:56.543220  9614 solver.cpp:571] Iteration 820, lr = 0.001
I0512 06:33:57.807451  9614 solver.cpp:242] Iteration 840, loss = 0.682249
I0512 06:33:57.807494  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.385326 (* 1 = 0.385326 loss)
I0512 06:33:57.807502  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.296923 (* 1 = 0.296923 loss)
I0512 06:33:57.807507  9614 solver.cpp:571] Iteration 840, lr = 0.001
I0512 06:33:59.071322  9614 solver.cpp:242] Iteration 860, loss = 0.927448
I0512 06:33:59.071362  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.36857 (* 1 = 0.36857 loss)
I0512 06:33:59.071369  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.558879 (* 1 = 0.558879 loss)
I0512 06:33:59.071374  9614 solver.cpp:571] Iteration 860, lr = 0.001
I0512 06:34:00.307945  9614 solver.cpp:242] Iteration 880, loss = 0.696881
I0512 06:34:00.307993  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.420408 (* 1 = 0.420408 loss)
I0512 06:34:00.308001  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.276472 (* 1 = 0.276472 loss)
I0512 06:34:00.308006  9614 solver.cpp:571] Iteration 880, lr = 0.001
I0512 06:34:01.546936  9614 solver.cpp:242] Iteration 900, loss = 0.616657
I0512 06:34:01.546977  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.372985 (* 1 = 0.372985 loss)
I0512 06:34:01.546984  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.243672 (* 1 = 0.243672 loss)
I0512 06:34:01.546993  9614 solver.cpp:571] Iteration 900, lr = 0.001
I0512 06:34:02.786551  9614 solver.cpp:242] Iteration 920, loss = 0.713511
I0512 06:34:02.786592  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.394725 (* 1 = 0.394725 loss)
I0512 06:34:02.786599  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.318786 (* 1 = 0.318786 loss)
I0512 06:34:02.786605  9614 solver.cpp:571] Iteration 920, lr = 0.001
I0512 06:34:04.024267  9614 solver.cpp:242] Iteration 940, loss = 0.661825
I0512 06:34:04.024309  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.407126 (* 1 = 0.407126 loss)
I0512 06:34:04.024317  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.254699 (* 1 = 0.254699 loss)
I0512 06:34:04.024322  9614 solver.cpp:571] Iteration 940, lr = 0.001
I0512 06:34:05.253895  9614 solver.cpp:242] Iteration 960, loss = 0.785159
I0512 06:34:05.253940  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.365822 (* 1 = 0.365822 loss)
I0512 06:34:05.253947  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.419337 (* 1 = 0.419337 loss)
I0512 06:34:05.253953  9614 solver.cpp:571] Iteration 960, lr = 0.001
I0512 06:34:06.474943  9614 solver.cpp:242] Iteration 980, loss = 0.77995
I0512 06:34:06.474984  9614 solver.cpp:258]     Train net output #0: bbox_loss = 0.357435 (* 1 = 0.357435 loss)
I0512 06:34:06.474995  9614 solver.cpp:258]     Train net output #1: cls_loss = 0.422515 (* 1 = 0.422515 loss)
I0512 06:34:06.475002  9614 solver.cpp:571] Iteration 980, lr = 0.001
speed: 0.061s / iter
Wrote snapshot to: /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_fast_rcnn_stage2_iter_1000.caffemodel
done solving
cp /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/zf_fast_rcnn_stage2_iter_1000.caffemodel -> /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/ZF_faster_rcnn_final.caffemodel
Final model: /home/xiaogao/job/faster-rcnn/py-faster-rcnn-master/output/faster_rcnn_alt_opt/voc_2007_trainval/ZF_faster_rcnn_final.caffemodel

real	7m34.387s
user	6m10.147s
sys	1m24.926s
